{
 "cells": [
  {
   "cell_type": "code",
   "id": "4a384230-c4f0-4c7c-8ec9-697198b7b192",
   "metadata": {},
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import emoji\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from googleapiclient import discovery"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aceebfd2-94b6-4219-9d06-6be1eee117e3",
   "metadata": {},
   "source": [
    "# file paths of each needed dataset\n",
    "female_comments_path = \"/home/haters/Downloads/loaded_data/Combined_data_29Apr/combined_female_comments.csv\"\n",
    "male_comments_path = \"/home/haters/Downloads/loaded_data/Combined_data_29Apr/combined_male_comments.csv\"\n",
    "female_submissions_path = \"/home/haters/Downloads/loaded_data/Combined_data_29Apr/combined_female_submissions.csv\"\n",
    "male_submissions_path = \"/home/haters/Downloads/loaded_data/Combined_data_29Apr/combined_male_submissions.csv\"\n",
    "\n",
    "# load dataset from the file paths\n",
    "female_com_df = pd.read_csv(female_comments_path)\n",
    "male_com_df = pd.read_csv(male_comments_path)\n",
    "female_sub_df = pd.read_csv(female_submissions_path)\n",
    "male_sub_df = pd.read_csv(male_submissions_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "10a20fa7-5ea2-49cd-a537-e6543e21dc9e",
   "metadata": {},
   "source": [
    "# initial a list of the datasets to be processed\n",
    "dfs = [female_com_df, male_com_df, female_sub_df, male_sub_df]\n",
    "\n",
    "# drop 'temp_id' column if it exists\n",
    "for df in dfs:\n",
    "    if 'temp_id' in df.columns:\n",
    "        df.drop(columns=['temp_id'], inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3facf23b-aca0-4c43-b8de-cd19a3391c9d",
   "metadata": {},
   "source": [
    "# split DataFrame into n parts\n",
    "def split_dataframe(df, n=3):\n",
    "    return [df.iloc[i::n, :].reset_index(drop=True) for i in range(n)]\n",
    "\n",
    "# split DataFrame into 2 parts, then each of those parts into 3 smaller parts\n",
    "def split_and_subsplit_dataframe(df, split_n=2, subsplit_n=3):\n",
    "    main_parts = split_dataframe(df, split_n)\n",
    "    sub_parts = []\n",
    "    for part in main_parts:\n",
    "        sub_parts.append(split_dataframe(part, subsplit_n))\n",
    "    return sub_parts\n",
    "\n",
    "female_sub_parts = split_dataframe(female_sub_df)\n",
    "male_com_parts = split_dataframe(male_com_df)\n",
    "male_sub_parts = split_dataframe(male_sub_df)\n",
    "female_com_parts = split_and_subsplit_dataframe(female_com_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7dedb544-29e2-402a-adac-e48ff063e9e2",
   "metadata": {},
   "source": [
    "# list of API keys to use for Perspective API (get your own keys at https://console.developers.google.com/)\n",
    "API_KEYS = [\n",
    "    'AIzaSyA_uZndSn69dCshlHBt01IZRmmL6GV00eM',\n",
    "    'AIzaSyCOjCPE66GcfVJHyXkUF72P1ibU6XKz6e4',\n",
    "    'AIzaSyAU9TrCiUwaSbDkF1CSR9cpKJ-4FJAy-4s', \n",
    "    'AIzaSyDPieGPiGBulfG7xcbRSIaj-vG6o_sq0h0', \n",
    "    'AIzaSyBHTLT8p7Rp5dkIpQ3zZwwo7NUhc8-DkmY', \n",
    "    'AIzaSyC_dlgL-RP6T5dsHe_qSjPQbqtrH_0R6Xc'\n",
    "]\n",
    "\n",
    "# create a list of clients using the API keys\n",
    "clients = [\n",
    "    discovery.build(\n",
    "        \"commentanalyzer\",\n",
    "        \"v1alpha1\",\n",
    "        developerKey=api_key,\n",
    "        discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "        static_discovery=False,\n",
    "    ) for api_key in API_KEYS\n",
    "]\n",
    "\n",
    "# Basic text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove special characters\n",
    "    text = emoji.demojize(text)  # Convert emojis to text\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    return text.strip()\n",
    "\n",
    "# Function to analyze text using Perspective API with timeout handling\n",
    "def get_toxicity_and_sexually_explicit_scores(client, text, sleep_time=1, retry_limit=3):\n",
    "    preprocessed_text = preprocess_text(text)\n",
    "    analyze_request = {\n",
    "        'comment': {'text': preprocessed_text},\n",
    "        'requestedAttributes': {'TOXICITY': {}, 'SEXUALLY_EXPLICIT': {}}\n",
    "    }\n",
    "    retries = 0\n",
    "    while retries < retry_limit:\n",
    "        try:\n",
    "            response = client.comments().analyze(body=analyze_request).execute()\n",
    "            toxicity_score = response['attributeScores']['TOXICITY']['summaryScore']['value']\n",
    "            sexually_explicit_score = response['attributeScores']['SEXUALLY_EXPLICIT']['summaryScore']['value']\n",
    "            return toxicity_score, sexually_explicit_score\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing text: {e}. Retrying ({retries + 1}/{retry_limit})...\")\n",
    "            retries += 1\n",
    "            time.sleep(sleep_time)\n",
    "    print(f\"Skipping text after {retry_limit} retries: {preprocessed_text[:30]}...\")  # Log the problematic text\n",
    "    return None, None\n",
    "\n",
    "# Function to process DataFrame with multiple API keys\n",
    "def process_dataframe(df, clients):\n",
    "    toxicity_scores = []\n",
    "    sexually_explicit_scores = []\n",
    "    client_count = len(clients)\n",
    "    for i, text in enumerate(tqdm(df['body'], desc=\"Processing rows\")):\n",
    "        client = clients[i % client_count]  # Rotate clients\n",
    "        toxicity_score, sexually_explicit_score = get_toxicity_and_sexually_explicit_scores(client, text)\n",
    "        toxicity_scores.append(toxicity_score)\n",
    "        sexually_explicit_scores.append(sexually_explicit_score)\n",
    "        if (i + 1) % (50 * client_count) == 0:  # Adjust the batch size as needed\n",
    "            time.sleep(60)  # Sleep for 60 seconds after every batch\n",
    "    df['toxicity_score'] = toxicity_scores\n",
    "    df['sexually_explicit_score'] = sexually_explicit_scores\n",
    "    return df\n",
    "\n",
    "# Ensure the output directory exists\n",
    "output_dir = \"/home/haters/Downloads/Toxicity_Detection/output_perspective/output_score/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process each part and save intermediate results\n",
    "def process_and_save_parts(parts, filename_prefix, clients):\n",
    "    processed_parts = []\n",
    "    for i, part in enumerate(parts):\n",
    "        processed_part = process_dataframe(part, clients)\n",
    "        processed_part.to_csv(f\"{output_dir}{filename_prefix}_part_{i+1}.csv\", index=False)\n",
    "        processed_parts.append(processed_part)\n",
    "    return processed_parts"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "female_sub_preprocessed_parts = process_and_save_parts(female_sub_parts, 'female_submissions_outcome', clients)\n",
    "final_df = pd.concat(female_sub_preprocessed_parts).reset_index(drop=True)\n",
    "final_df.to_csv('home/haters/Downloads/Toxicity_Detection/output_perspective/output_score/female_submissions_outcome.csv', index=False)"
   ],
   "id": "e8adc2c2edb5d9b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "male_sub_preprocessed_parts = process_and_save_parts(male_sub_parts, 'male_submissions_outcome', clients)\n",
    "final_df = pd.concat(male_sub_preprocessed_parts).reset_index(drop=True)\n",
    "final_df.to_csv('home/haters/Downloads/Toxicity_Detection/output_perspective/output_score/male_submissions_outcome.csv', index=False)"
   ],
   "id": "3707066b9ce10477"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "be25d3dacdae45ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "745f06bb978e42f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e559554163c9c51b"
  },
  {
   "cell_type": "code",
   "id": "cdc581ef-7897-4ea0-b650-64a90a0b20ca",
   "metadata": {},
   "source": [
    "female_com_preprocessed_parts = process_and_save_parts(female_com_parts, 'female_comments_outcome', clients)\n",
    "final_df = pd.concat(female_com_preprocessed_parts).reset_index(drop=True)\n",
    "final_df.to_csv('home/haters/Downloads/Toxicity_Detection/output_perspective/output_score/female_comments_outcome.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f9842833-0eaf-472b-a070-a1706e2768d5",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
