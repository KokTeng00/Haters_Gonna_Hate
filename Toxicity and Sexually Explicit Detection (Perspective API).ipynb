{
 "cells": [
  {
   "cell_type": "code",
   "id": "4a384230-c4f0-4c7c-8ec9-697198b7b192",
   "metadata": {},
   "source": [
    "# loaded necessary libraries and modules for the script to run successfully\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import emoji\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from googleapiclient import discovery"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aceebfd2-94b6-4219-9d06-6be1eee117e3",
   "metadata": {},
   "source": [
    "# file paths of each needed dataset\n",
    "female_comments_path = \"/home/haters/Downloads/loaded_data/Combined_data_29Apr/combined_female_comments.csv\"\n",
    "male_comments_path = \"/home/haters/Downloads/loaded_data/Combined_data_29Apr/combined_male_comments.csv\"\n",
    "female_submissions_path = \"/home/haters/Downloads/loaded_data/Combined_data_29Apr/combined_female_submissions.csv\"\n",
    "male_submissions_path = \"/home/haters/Downloads/loaded_data/Combined_data_29Apr/combined_male_submissions.csv\"\n",
    "\n",
    "# load dataset from the file paths\n",
    "female_com_df = pd.read_csv(female_comments_path)\n",
    "male_com_df = pd.read_csv(male_comments_path)\n",
    "female_sub_df = pd.read_csv(female_submissions_path)\n",
    "male_sub_df = pd.read_csv(male_submissions_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "10a20fa7-5ea2-49cd-a537-e6543e21dc9e",
   "metadata": {},
   "source": [
    "# initial a list of the datasets to be processed\n",
    "dfs = [female_com_df, male_com_df, female_sub_df, male_sub_df]\n",
    "\n",
    "# drop 'temp_id' column if it exists\n",
    "for df in dfs:\n",
    "    if 'temp_id' in df.columns:\n",
    "        df.drop(columns=['temp_id'], inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3facf23b-aca0-4c43-b8de-cd19a3391c9d",
   "metadata": {},
   "source": [
    "# split DataFrame into n parts\n",
    "def split_dataframe(df, n=3):\n",
    "    return [df.iloc[i::n, :].reset_index(drop=True) for i in range(n)]\n",
    "\n",
    "# split DataFrame into 2 parts, then each of those parts into 3 smaller parts\n",
    "def split_and_subsplit_dataframe(df, split_n=2, subsplit_n=3):\n",
    "    main_parts = split_dataframe(df, split_n)\n",
    "    sub_parts = []\n",
    "    for part in main_parts:\n",
    "        sub_parts.append(split_dataframe(part, subsplit_n))\n",
    "    return sub_parts\n",
    "\n",
    "female_sub_parts = split_dataframe(female_sub_df)\n",
    "male_com_parts = split_dataframe(male_com_df)\n",
    "male_sub_parts = split_dataframe(male_sub_df)\n",
    "female_com_parts = split_and_subsplit_dataframe(female_com_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Toxicity and Sexually Explicit Detection using Perspective API (Submissions dataset)",
   "id": "b266dd9a634d329c"
  },
  {
   "cell_type": "code",
   "id": "7dedb544-29e2-402a-adac-e48ff063e9e2",
   "metadata": {},
   "source": [
    "# list of API keys to use for Perspective API (get your own keys at https://console.developers.google.com/)\n",
    "API_KEYS = [\n",
    "    'AIzaSyA_uZndSn69dCshlHBt01IZRmmL6GV00eM',\n",
    "    'AIzaSyCOjCPE66GcfVJHyXkUF72P1ibU6XKz6e4',\n",
    "    'AIzaSyAU9TrCiUwaSbDkF1CSR9cpKJ-4FJAy-4s', \n",
    "    'AIzaSyDPieGPiGBulfG7xcbRSIaj-vG6o_sq0h0', \n",
    "    'AIzaSyBHTLT8p7Rp5dkIpQ3zZwwo7NUhc8-DkmY', \n",
    "    'AIzaSyC_dlgL-RP6T5dsHe_qSjPQbqtrH_0R6Xc'\n",
    "]\n",
    "\n",
    "# create a list of clients using the API keys\n",
    "clients = [\n",
    "    discovery.build(\n",
    "        \"commentanalyzer\",\n",
    "        \"v1alpha1\",\n",
    "        developerKey=api_key,\n",
    "        discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "        static_discovery=False,\n",
    "    ) for api_key in API_KEYS\n",
    "]\n",
    "\n",
    "# basic text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove special characters\n",
    "    text = emoji.demojize(text)  # Convert emojis to text\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    return text.strip()\n",
    "\n",
    "# function to analyze text using Perspective API with timeout handling\n",
    "def get_toxicity_and_sexually_explicit_scores(client, text, sleep_time=1, retry_limit=3):\n",
    "    preprocessed_text = preprocess_text(text)\n",
    "    analyze_request = {\n",
    "        'comment': {'text': preprocessed_text},\n",
    "        'requestedAttributes': {'TOXICITY': {}, 'SEXUALLY_EXPLICIT': {}}\n",
    "    }\n",
    "    retries = 0\n",
    "    while retries < retry_limit:\n",
    "        try:\n",
    "            response = client.comments().analyze(body=analyze_request).execute()\n",
    "            toxicity_score = response['attributeScores']['TOXICITY']['summaryScore']['value']\n",
    "            sexually_explicit_score = response['attributeScores']['SEXUALLY_EXPLICIT']['summaryScore']['value']\n",
    "            return toxicity_score, sexually_explicit_score\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing text: {e}. Retrying ({retries + 1}/{retry_limit})...\")\n",
    "            retries += 1\n",
    "            time.sleep(sleep_time)\n",
    "    print(f\"Skipping text after {retry_limit} retries: {preprocessed_text[:30]}...\")  # Log the problematic text\n",
    "    return None, None\n",
    "\n",
    "# function to process DataFrame with multiple API keys\n",
    "def process_dataframe(df, clients):\n",
    "    toxicity_scores = []\n",
    "    sexually_explicit_scores = []\n",
    "    client_count = len(clients)\n",
    "    for i, text in enumerate(tqdm(df['body'], desc=\"Processing rows\")):\n",
    "        client = clients[i % client_count]  # Rotate clients\n",
    "        toxicity_score, sexually_explicit_score = get_toxicity_and_sexually_explicit_scores(client, text)\n",
    "        toxicity_scores.append(toxicity_score)\n",
    "        sexually_explicit_scores.append(sexually_explicit_score)\n",
    "        if (i + 1) % (50 * client_count) == 0:  # Adjust the batch size as needed\n",
    "            time.sleep(60)  # Sleep for 60 seconds after every batch\n",
    "    df['toxicity_score'] = toxicity_scores\n",
    "    df['sexually_explicit_score'] = sexually_explicit_scores\n",
    "    return df\n",
    "\n",
    "# ensure the output directory exists\n",
    "output_dir = \"/home/haters/Downloads/Toxicity_Detection/output_perspective/output_score/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# process each part and save intermediate results\n",
    "def process_and_save_parts(parts, filename_prefix, clients):\n",
    "    processed_parts = []\n",
    "    for i, part in enumerate(parts):\n",
    "        processed_part = process_dataframe(part, clients)\n",
    "        processed_part.to_csv(f\"{output_dir}{filename_prefix}_part_{i+1}.csv\", index=False)\n",
    "        processed_parts.append(processed_part)\n",
    "    return processed_parts"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# process and save the parts of toxic and sexually explicit scores for female submissions dataset\n",
    "female_sub_preprocessed_parts = process_and_save_parts(female_sub_parts, 'female_submissions_outcome', clients)\n",
    "final_df = pd.concat(female_sub_preprocessed_parts).reset_index(drop=True)\n",
    "final_df.to_csv('home/haters/Downloads/Toxicity_Detection/output_perspective/output_score/female_submissions_outcome.csv', index=False)"
   ],
   "id": "e8adc2c2edb5d9b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# process and save the parts of toxic and sexually explicit scores for male submissions dataset\n",
    "male_sub_preprocessed_parts = process_and_save_parts(male_sub_parts, 'male_submissions_outcome', clients)\n",
    "final_df = pd.concat(male_sub_preprocessed_parts).reset_index(drop=True)\n",
    "final_df.to_csv('home/haters/Downloads/Toxicity_Detection/output_perspective/output_score/male_submissions_outcome.csv', index=False)"
   ],
   "id": "3707066b9ce10477",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Toxicity and Sexually Explicit Detection using Perspective API (Comments dataset)",
   "id": "a23962edce951e91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# define the function for process and save the parts of toxic and sexually explicit scores for male comments dataset\n",
    "def process_and_save_parts(parts, filename_prefix, clients, start_part=1):\n",
    "    for i, part in enumerate(parts, start=1):\n",
    "        if i >= start_part:\n",
    "            processed_part = process_dataframe(part, clients)\n",
    "            processed_part.to_csv(f\"{output_dir}{filename_prefix}_part_{i}.csv\", index=False)\n",
    "            print(f\"Processed and saved {filename_prefix}_part_{i}.csv\")"
   ],
   "id": "be25d3dacdae45ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "process_and_save_parts(male_com_parts, 'male_com', clients, start_part=1)",
   "id": "745f06bb978e42f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# define the function for process and save the parts of toxic and sexually explicit for female comments dataset\n",
    "def process_and_save_parts(parts, filename_prefix, clients, main_part=1, start_subpart=3):\n",
    "    for i, part in enumerate(parts[main_part - 1], start=1):\n",
    "        if i >= start_subpart:\n",
    "            processed_part = process_dataframe(part, clients)\n",
    "            processed_part.to_csv(f\"{output_dir}{filename_prefix}_mainpart_{main_part}_subpart_{i}.csv\", index=False)\n",
    "            print(f\"Processed and saved {filename_prefix}_mainpart_{main_part}_subpart_{i}.csv\")"
   ],
   "id": "e559554163c9c51b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cdc581ef-7897-4ea0-b650-64a90a0b20ca",
   "metadata": {},
   "source": "process_and_save_parts(female_com_parts, 'female_com_parts', clients, main_part=1)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "process_and_save_parts(female_com_parts, 'female_com_parts', clients, main_part=2)",
   "id": "4e9c7b03d515e88b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Process Missing Parts",
   "id": "25708b3363ba6480"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "11bca2a69ebfd865"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "66786887b417b2d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e0e43afdb9e13e72"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
