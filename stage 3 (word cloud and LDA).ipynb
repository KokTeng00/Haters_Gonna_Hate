{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# pip install urllib3==1.26.15\n",
    "# pip install scipy==1.12"
   ],
   "id": "9530d4e45b188a08",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)"
   ],
   "id": "fc460cfae43beebe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "id": "a119cf6f5ceb656a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "import emoji\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# Define Text Preprocessor class\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.tokenizer = WordPunctTokenizer()\n",
    "        self.singer_names = [\n",
    "            \"Bad Bunny\", \"El Conejo Malo\", \"The Weeknd\", \"Abel\", \"Abel Tesfaye\",\n",
    "            \"Morgan Wallen\", \"Ed Sheeran\", \"Ginger Jesus\", \"Ed\", \"Drake\", \n",
    "            \"Drizzy\", \"Champagne Papi\", \"Aubrey\", \"Harry Styles\", \"Feid\", \n",
    "            \"Imagine Dragons\", \"Dan Reynolds\", \"Ben McKee\", \"Daniel Wayne Sermon\", \n",
    "            \"Daniel Platz Platzman\", \"Post Malone\", \"Posty\", \"BTS\", \"Bangtan\", \n",
    "            \"Bangtan Sonyeondan\", \"Tannies\", \"RM\", \"Jin\", \"Suga\", \"J-Hope\", \n",
    "            \"Jimin\", \"V\", \"Jungkook\", \"harry\", \"justin bieber\", \"bieber\", \"justin\", \"namjoon\", \n",
    "            \"Taylor Swift\" , \"T-Swift\" , \"TayTay\" , \"Taylor\" , \"Miss Americana\", \"SZA\", \"Solana Imani Row\", \n",
    "            \"Miley Cyrus\", \"Miley\", \"Hannah Montana\", \"New Jeans\", \"Minji\", \"Hanni\", \"Danielle New Jeans\", \"Haerin\", \"Hyein\", \n",
    "            \"Dua Lipa\", \"Dua\", \"Dula Peep\", \"Olivia Rodrigo\", \"Liv\", \"Ariana Grande\", \"Ari\", \"Ariana\", \"Ms Grande\", \n",
    "            \"Billie Eilish\", \"Billie\", \"Rihanna\", \"RiRi\", \"Badgalriri\", \"Adele\", \"swiftie\", \"Oliia\", \"newjean\", \n",
    "            \"rodrigo\", \"rodrigos\"\n",
    "        ]\n",
    "        self.singer_names = [name.lower() for name in self.singer_names]\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = self.remove_singer_names(text)\n",
    "        text = emoji.demojize(text)\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        tokens = [word for word in tokens if word not in self.stop_words and word.isalnum()]\n",
    "        tokens = [self.lemmatize_token(word) for word in tokens]\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    def remove_singer_names(self, text):\n",
    "        for name in self.singer_names:\n",
    "            text = text.replace(name, '')\n",
    "        \n",
    "        text = text.replace('austin', 'album')\n",
    "        text = text.replace('wts', 'want to sell')\n",
    "        text = text.replace('merch', 'merchandise')\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def lemmatize_token(self, token):\n",
    "        tag = self.get_wordnet_pos(nltk.pos_tag([token])[0][1])\n",
    "        return self.lemmatizer.lemmatize(token, pos=tag) if tag else token\n",
    "\n",
    "    @staticmethod\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        # Converts treebank tag to wordnet tag\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return None"
   ],
   "id": "82b560727c7f5542",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "male_submissions = pd.read_csv(\"/home/haters/Downloads/Toxicity_Detection/output_perspective/output_score/male_submissions_outcome_final.csv\")\n",
    "female_submissions = pd.read_csv(\"/home/haters/Downloads/Toxicity_Detection/output_perspective/output_score/female_submissions_outcome_final.csv\")\n",
    "\n",
    "# Combine 'title' and 'selftext' into a new column 'combined_text'\n",
    "male_submissions['combined_text'] = male_submissions['title'] + \" \" + male_submissions['selftext']\n",
    "female_submissions['combined_text'] = female_submissions['title'] + \" \" + female_submissions['selftext']"
   ],
   "id": "bc71ecb5799394ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def remove_hyperlinks(text):\n",
    "    return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags = re.MULTILINE)\n",
    "\n",
    "male_submissions[\"combined_text\"] = male_submissions[\"combined_text\"].apply(remove_hyperlinks)\n",
    "female_submissions[\"combined_text\"] = female_submissions[\"combined_text\"].apply(remove_hyperlinks)"
   ],
   "id": "6ebbdd1be459d2c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def preprocess(df, data_type):\n",
    "    if data_type == 'comments':\n",
    "        text = 'body'\n",
    "    elif data_type == 'submissions':\n",
    "        # text = 'selftext'\n",
    "        text = 'combined_text'\n",
    "    col_index = df.columns.get_loc(text) + 1\n",
    "    preprocessed_text = df[text].apply(preprocessor.preprocess_text)\n",
    "    df.insert(col_index, 'preprocessed_txt', preprocessed_text)\n",
    "    return df"
   ],
   "id": "35a77f4d875351e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "preprocessor = TextPreprocessor()\n",
    "combined_male_submissions = preprocess(male_submissions, 'submissions')"
   ],
   "id": "e86e5d335db97c18",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "preprocessor = TextPreprocessor()\n",
    "combined_female_submissions = preprocess(female_submissions, 'submissions')"
   ],
   "id": "9d744294f92d2bc4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!pip install git+https://github.com/huggingface/transformers.git -q -U # transformers version:  4.37.0\n",
    "!pip install git+https://github.com/huggingface/accelerate.git -q -U # accelerate version:  0.27.0\n",
    "!pip install bitsandbytes # bitsandbytes version:  0.42.0\n",
    "!pip install git+https://github.com/huggingface/peft.git -q -U # peft version: 0.7.2\n",
    "!pip install einops\n",
    "!pip install xformers\n",
    "!pip install torchvision"
   ],
   "id": "615c59d5d3bead83",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from huggingface_hub import login\n",
    "login(token = \"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\")"
   ],
   "id": "1a478941f0537a23",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch import cuda\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'"
   ],
   "id": "26e66c1d30902f19",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch import bfloat16\n",
    "import transformers\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Enable 4-bit quantization\n",
    "    bnb_4bit_quant_type='nf4',  # Normalized float 4\n",
    "    bnb_4bit_use_double_quant=True,  # Enable second quantization after the first\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16  # Computation type\n",
    ")"
   ],
   "id": "f138e54483fa8712",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"daryl149/llama-2-7b-chat-hf\")\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"daryl149/llama-2-7b-chat-hf\", trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto', force_download=True)\n",
    "model.eval()"
   ],
   "id": "694552f3b2e701af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "generator = transformers.pipeline(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    task='text-generation',\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=500,\n",
    "    repetition_penalty=1.1\n",
    ")"
   ],
   "id": "3174d5a782c21fa2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# System prompt describes information given to all conversations\n",
    "system_prompt = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant for labeling topics.\n",
    "<</SYS>>\n",
    "\"\"\"\n",
    "\n",
    "# Example prompt demonstrating the output we are looking for\n",
    "example_prompt = \"\"\"\n",
    "I have a topic that contains the following documents:\n",
    "- I've been checking Ticketmaster every day for Coldplay tickets but they're sold out everywhere. Does anyone have any suggestions on where else I might find them?\n",
    "- Hello all! If anyone is looking to buy Arctic Monkeys tickets for the show in Paris on September 10th, I have two extra tickets available. We can arrange for a safe and secure transaction!\n",
    "- I have general admission tickets for the upcoming Foo Fighters concert and I'm wondering how early I should get to the venue to get a good spot. Any tips from past attendees?\n",
    "- Just scored VIP passes for Lollapalooza this year! I'm super excited to see Kendrick Lamar, Billie Eilish, and The Weeknd. Who else is going? Any advice on the best spots to watch the performances from?\n",
    "- Does anyone know if there's a way to get refunds for canceled concerts? I had tickets for the Red Hot Chili Peppers show next month, but it got canceled and now I'm trying to figure out my options.\n",
    "\n",
    "The topic is described by the following keywords: 'concert, tour, ticket, attend, stage, stadium, wembley, arena, floor, meet, ticketmaster, ticket, stubhub, presale, sale, sell, buy, fee, concert, paypal'.\n",
    "\n",
    "To create a short label for this topic, follow these steps:\n",
    "\n",
    "1. **Identify the main themes**: Analyze the documents and keywords to identify the main themes discussed.\n",
    "2. **Summarize the core message**: Summarize the core message of the topic based on the identified themes.\n",
    "3. **Craft a concise label**: Create a short, descriptive label that encapsulates the core message.\n",
    "\n",
    "Think through these steps carefully and return only the final label.\n",
    "\n",
    "[START OF LABEL]\n",
    "Concert and ticket\n",
    "[END OF LABEL]\n",
    "\"\"\"\n",
    "\n",
    "# Our main prompt with documents ([DOCUMENTS]) and keywords ([KEYWORDS]) tags\n",
    "main_prompt = \"\"\"\n",
    "[INST]\n",
    "I have a topic that contains the following documents:\n",
    "[DOCUMENTS]\n",
    "\n",
    "The topic is described by the following keywords: '[KEYWORDS]'.\n",
    "\n",
    "To create a short label for this topic, follow these steps:\n",
    "\n",
    "1. **Identify the main themes**: Analyze the documents and keywords to identify the main themes discussed.\n",
    "2. **Summarize the core message**: Summarize the core message of the topic based on the identified themes.\n",
    "3. **Craft a concise label**: Create a short, descriptive label that encapsulates the core message.\n",
    "\n",
    "Think through these steps carefully and return only the final label.\n",
    "\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "prompt = system_prompt + example_prompt + main_prompt"
   ],
   "id": "5c8744c7d43dbedd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['OMP_NUM_THREADS'] = \"4\"\n",
    "# os.environ['OMP_NESTED'] = 'TRUE'\n",
    "os.environ['OMP_MAX_ACTIVE_LEVELS'] = \"2\""
   ],
   "id": "9bfbc3efbd956b0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "selftext = combined_male_submissions[\"preprocessed_txt\"]",
   "id": "8880891f1375cd2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = embedding_model.encode(selftext, show_progress_bar = True)"
   ],
   "id": "832726cfbfa029ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "umap_model = UMAP(n_neighbors=80, n_components=3, min_dist=0.1, metric='cosine', random_state=42)\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=150, metric='manhattan', cluster_selection_method='leaf', prediction_data=True)\n",
    "\n",
    "# umap_model = UMAP(n_neighbors=5, n_components=2, min_dist=0.0, metric='euclidean', random_state=42)\n",
    "# hdbscan_model = HDBSCAN(min_cluster_size=120, metric='euclidean', cluster_selection_method='eom', prediction_data=True)"
   ],
   "id": "c58646a33485e46c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Pre-reduce embeddings for visualization purposes\n",
    "reduced_embeddings = UMAP(n_neighbors=80, n_components=3, min_dist=0.1, metric='cosine', random_state=42).fit_transform(embeddings)\n",
    "# reduced_embeddings = UMAP(n_neighbors=5, n_components=2, min_dist=0.0, metric='euclidean', random_state=42).fit_transform(embeddings)"
   ],
   "id": "5d5d057d76fc8020",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, TextGeneration\n",
    "\n",
    "keybert = KeyBERTInspired()\n",
    "\n",
    "mmr = MaximalMarginalRelevance(diversity = 0.3)\n",
    "\n",
    "llama2 = TextGeneration(generator, prompt = prompt)\n",
    "\n",
    "representation_model = {\n",
    "    \"KeyBERT\": keybert,\n",
    "    \"Llama2\" : llama2,\n",
    "    \"MMR\" : mmr,\n",
    "}"
   ],
   "id": "295c3b5a2cc1be83",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from bertopic.vectorizers import OnlineCountVectorizer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True, bm25_weighting=True)\n",
    "vectorizer_model = OnlineCountVectorizer(stop_words=\"english\")"
   ],
   "id": "e60e396f81bb8b00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "def evaluate_model(umap_params, hdbscan_params, selftext, embeddings):\n",
    "    umap_model = UMAP(**umap_params)\n",
    "    hdbscan_model = HDBSCAN(**hdbscan_params)\n",
    "\n",
    "    # Fit UMAP and HDBSCAN models\n",
    "    umap_embeddings = umap_model.fit_transform(embeddings)\n",
    "    hdbscan_labels = hdbscan_model.fit_predict(umap_embeddings)\n",
    "\n",
    "    # Check the number of unique topics\n",
    "    num_topics = len(set(hdbscan_labels)) - (1 if -1 in hdbscan_labels else 0)  # Exclude noise points labeled as -1\n",
    "    print(f\"Number of clusters identified by HDBSCAN: {num_topics}\")\n",
    "    \"\"\"\n",
    "    if num_topics > 10:\n",
    "        del embeddings\n",
    "        del umap_model\n",
    "        del hdbscan_model\n",
    "        del umap_embeddings\n",
    "        del hdbscan_labels\n",
    "        gc.collect()\n",
    "        return -np.inf  # Return a low score if the number of topics exceeds 10\n",
    "\"\"\"\n",
    "    # Proceed with BERTopic model if the number of topics is within the limit\n",
    "    \n",
    "    topic_model = BERTopic(\n",
    "            embedding_model=embedding_model,\n",
    "            umap_model=umap_model,\n",
    "            hdbscan_model=hdbscan_model,\n",
    "            representation_model=representation_model,\n",
    "            vectorizer_model=vectorizer_model,\n",
    "            ctfidf_model=ctfidf_model,\n",
    "            top_n_words=10,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "    topics, probs = topic_model.fit_transform(selftext, embeddings)\n",
    "    silhouette_avg = silhouette_score(embeddings, topics)\n",
    "\n",
    "        # Clean up BERTopic model from memory\n",
    "    del topic_model\n",
    "    del embeddings\n",
    "    del umap_model\n",
    "    del hdbscan_model\n",
    "    del umap_embeddings\n",
    "    del hdbscan_labels\n",
    "    gc.collect()\n",
    "    return silhouette_avg\n",
    "\n",
    "# Expanded UMAP parameter grid\n",
    "umap_param_grid = [\n",
    "    {'n_neighbors': 15, 'n_components': 2, 'min_dist': 0.0, 'metric': 'cosine'},\n",
    "    {'n_neighbors': 25, 'n_components': 5, 'min_dist': 0.1, 'metric': 'euclidean'},\n",
    "    {'n_neighbors': 10, 'n_components': 3, 'min_dist': 0.2, 'metric': 'manhattan'},\n",
    "    {'n_neighbors': 20, 'n_components': 4, 'min_dist': 0.3, 'metric': 'manhattan'},\n",
    "    {'n_neighbors': 30, 'n_components': 2, 'min_dist': 0.4, 'metric': 'manhattan'},\n",
    "    {'n_neighbors': 35, 'n_components': 5, 'min_dist': 0.5, 'metric': 'cosine'},\n",
    "    {'n_neighbors': 40, 'n_components': 3, 'min_dist': 0.6, 'metric': 'euclidean'},\n",
    "    {'n_neighbors': 50, 'n_components': 4, 'min_dist': 0.7, 'metric': 'manhattan'},\n",
    "    {'n_neighbors': 60, 'n_components': 2, 'min_dist': 0.8, 'metric': 'euclidean'},\n",
    "    {'n_neighbors': 70, 'n_components': 5, 'min_dist': 0.9, 'metric': 'cosine'},\n",
    "    {'n_neighbors': 80, 'n_components': 3, 'min_dist': 0.1, 'metric': 'cosine'},\n",
    "    {'n_neighbors': 90, 'n_components': 4, 'min_dist': 0.2, 'metric': 'euclidean'},\n",
    "    {'n_neighbors': 100, 'n_components': 5, 'min_dist': 0.3, 'metric': 'manhattan'}\n",
    "]\n",
    "\n",
    "# Expanded HDBSCAN parameter grid without 'cosine' metric\n",
    "hdbscan_param_grid = [\n",
    "    {'min_cluster_size': 50, 'metric': 'euclidean', 'cluster_selection_method': 'eom'},\n",
    "    {'min_cluster_size': 100, 'metric': 'manhattan', 'cluster_selection_method': 'leaf'},\n",
    "    {'min_cluster_size': 30, 'metric': 'manhattan', 'cluster_selection_method': 'eom'},\n",
    "    {'min_cluster_size': 80, 'metric': 'euclidean', 'cluster_selection_method': 'leaf'},\n",
    "    {'min_cluster_size': 90, 'metric': 'manhattan', 'cluster_selection_method': 'eom'},\n",
    "    {'min_cluster_size': 110, 'metric': 'manhattan', 'cluster_selection_method': 'leaf'},\n",
    "    {'min_cluster_size': 140, 'metric': 'euclidean', 'cluster_selection_method': 'eom'},\n",
    "    {'min_cluster_size': 150, 'metric': 'manhattan', 'cluster_selection_method': 'leaf'}\n",
    "]\n",
    "\n",
    "# Create a list of parameter combinations\n",
    "param_grid = list(ParameterGrid({\n",
    "    'umap': umap_param_grid,\n",
    "    'hdbscan': hdbscan_param_grid\n",
    "}))\n",
    "\n",
    "with open('parameter_performance.txt', 'w') as file:\n",
    "    # Evaluate each combination\n",
    "    best_score = -np.inf\n",
    "    best_params = None\n",
    "\n",
    "    for params in param_grid:\n",
    "        umap_params = params['umap']\n",
    "        hdbscan_params = params['hdbscan']\n",
    "        score = evaluate_model(umap_params, hdbscan_params, selftext, embeddings)\n",
    "\n",
    "        # Record the performance of each parameter combination\n",
    "        file.write(f\"UMAP Params: {umap_params}, HDBSCAN Params: {hdbscan_params}, Score: {score}\\n\")\n",
    "        file.flush()  # Ensure the data is written to the file immediately\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = params\n",
    "\n",
    "print(f\"Best Score: {best_score}\")\n",
    "print(f\"Best Parameters: {best_params}\")"
   ],
   "id": "66e46beb71d658cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "#best_umap_params = best_params['umap']\n",
    "#best_hdbscan_params = best_params['hdbscan']\n",
    "\n",
    "#best_umap_model = UMAP(**best_umap_params)\n",
    "#best_hdbscan_model = HDBSCAN(**best_hdbscan_params)\n",
    "\n",
    "topic_model = BERTopic(\n",
    "\n",
    "  # Sub-models\n",
    "  embedding_model=embedding_model,\n",
    "  umap_model=umap_model,\n",
    "  hdbscan_model=hdbscan_model,\n",
    "  representation_model=representation_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    ctfidf_model=ctfidf_model,\n",
    "\n",
    "  # Hyperparameters\n",
    "  top_n_words=10,\n",
    "  verbose=True\n",
    ")\n",
    "\n",
    "# Train model\n",
    "topics, probs = topic_model.fit_transform(selftext, embeddings)"
   ],
   "id": "74eb7eaa99357077",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "output_df = topic_model.get_topic_info()\n",
    "output_df"
   ],
   "id": "9d52d5531117f169",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#topic_info = topic_model.get_topic_info()\n",
    "topic_labels = {row['Topic']: row['Name'] for _, row in topic_info.iterrows()}\n",
    "\n",
    "combined_male_submissions['topic_id'] = topics\n",
    "combined_male_submissions['topic_label'] = combined_male_submissions['topic_id'].map(topic_labels)"
   ],
   "id": "b071082f40f5c169",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "male_comments = pd.read_csv(\"/home/haters/Downloads/loaded_data/Combined_data_29Apr/combined_male_comments.csv\")\n",
    "female_comments = pd.read_csv(\"/home/haters/Downloads/loaded_data/Combined_data_29Apr/combined_female_comments.csv\")"
   ],
   "id": "b56e14f0d6500a96",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "merged_df = male_comments.merge(combined_male_submissions[['name', 'topic_id', 'topic_label']], \n",
    "                                left_on='link_id', \n",
    "                                right_on='name', \n",
    "                                how='left')\n",
    "\n",
    "# Drop the 'name' column\n",
    "merged_df.drop(columns=['name'], inplace=True)"
   ],
   "id": "d19e43e4aaf8f2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "missing_topic = merged_df[merged_df['topic_id'].isna() | merged_df['topic_label'].isna()]",
   "id": "98d57d7ceb665984",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "missing_topic.reset_index(drop = True, inplace = True)",
   "id": "5e980833ef734821",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "missing_topic = preprocess(missing_topic, 'comments')",
   "id": "251c9920286e847b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "comment_topics, comment_probs = topic_model.transform(missing_topic['preprocessed_txt'])",
   "id": "5ff6b717126bd19d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "missing_topic['topic_id'] = comment_topics\n",
    "missing_topic['topic_prob'] = comment_probs"
   ],
   "id": "43034a9dc624fa68",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Update merged_df with missing topics\n",
    "merged_df.update(missing_topic)\n",
    "merged_df.fillna(-99, inplace=True)"
   ],
   "id": "576dc00da821aaf5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "merged_topics = {\n",
    "    -1: 1,  # Concert and Ticket Issues\n",
    "    2: 1,   # Concert and Ticket Issues (Resale)\n",
    "    3: 1,   # Concert and Ticket Issues (General)\n",
    "    5: 1,   # Concert and Ticket Issues (Shipping)\n",
    "    12: 1,  # Concert and Ticket Issues (Discussion)\n",
    "    14: 1,  # Concert and Ticket Discussion (General)\n",
    "    15: 1,  # Concert and Tour Discussion (Tickets)\n",
    "    20: 1,  # Concert and Ticket (General)\n",
    "    7: 2,   # Concert and Fashion\n",
    "    8: 2,   # Concert and Fashion (Celebration and Conversation)\n",
    "    9: 2,   # Concert Posters and Wallpapers (Fashion-Related)\n",
    "    10: 2,  # Concert and Fashion (Outfits)\n",
    "    18: 2,  # Concert and Fashion (Crewnecks)\n",
    "    4: 3,   # Merchandise Wanted\n",
    "    11: 4,  # Concert and Friendship Bracelets\n",
    "    19: 5,  # Covers and Remixes\n",
    "    1: 6,   # Art and Creativity (Vinyl, CD, Art)\n",
    "    13: 6,  # Art and Creativity (Tattoo and Art)\n",
    "    17: 6,  # Art and Creativity (General)\n",
    "    21: 7,  # Leak and Music\n",
    "    0: 8,   # Others undefineable topics (random)\n",
    "    6: 8,   # Others undefineable topics\n",
    "    16: 8   # Others undefineable topics (Bias Poll)\n",
    "}\n",
    "\n",
    "topic_labels = {\n",
    "    1: 'Concert and Ticket Issues: Discussions about buying, selling, and handling concert tickets, including resale, shipping, and general availability.',\n",
    "    2: 'Concert and Fashion: Discussions about fashion choices for concerts, including outfits, jackets, and general concert-related fashion.',\n",
    "    3: 'Merchandise Wanted: Conversations around buying and selling artist-related merchandise like hoodies, shirts, and other items.',\n",
    "    4: 'Concert and Friendship Bracelets: Topics discussing the creation and exchange of friendship bracelets related to concerts.',\n",
    "    5: 'Covers and Remixes: Discussions about cover versions of songs and remixes in the music community.',\n",
    "    6: 'Art and Creativity: Conversations focused on tattoos, artwork, and other creative expressions related to music and concerts.',\n",
    "    7: 'Leak and Music: Discussions and speculations around leaked music content, including upcoming releases and snippets.',\n",
    "    8: 'Others undefineable topics'\n",
    "}\n",
    "\n",
    "combined_female_submissions['topic_id'] = combined_female_submissions['topic_id'].replace(merged_topics)\n",
    "\n",
    "# Map new topic IDs to their labels\n",
    "combined_female_submissions['topic_label'] = combined_female_submissions['topic_id'].map(topic_labels)"
   ],
   "id": "ff4476e5d89b29d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## For male\n",
    "\n",
    "merged_topics = {\n",
    "    -1: 1,\n",
    "    5: 1,\n",
    "    7: 1,\n",
    "    0: 2,\n",
    "    1: 3,\n",
    "    2: 4,\n",
    "    8: 4,\n",
    "    3: 5,\n",
    "    4: 6,\n",
    "    6: 7,\n",
    "    9: 8,\n",
    "    -99: 8\n",
    "}\n",
    "\n",
    "# Update topic labels\n",
    "topic_labels = {\n",
    "    1: 'General discussions about music, including song recommendations and rap music.',\n",
    "    2: 'Discussions around buying and selling photocards.',\n",
    "    3: 'Personal stories and relationships.',\n",
    "    4: 'Discussions about buying and selling event tickets, and concert experiences.',\n",
    "    5: 'Polls and opinions on various topics.',\n",
    "    6: 'Discussions about merchandise like shirts and hoodies.',\n",
    "    7: 'Tips and discussions about avoiding scams.',\n",
    "    8: 'Others undefineable topics'\n",
    "}\n",
    "\n",
    "combined_female_submissions['topic_id'] = combined_female_submissions['topic_id'].replace(merged_topics)\n",
    "\n",
    "# Map new topic IDs to their labels\n",
    "combined_female_submissions['topic_label'] = combined_female_submissions['topic_id'].map(topic_labels)"
   ],
   "id": "c4edfee4e14b1934",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "combined_male_submissions[\"topic_label\"].value_counts()",
   "id": "bfffe0952663dea6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "combined_male_submissions[\"topic_id\"].value_counts()",
   "id": "c78a5905d21e32b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "merged_topics = {\n",
    "    -1: 1,  # Concert and Ticket Issues\n",
    "    2: 1,   # Concert and Ticket Issues (Resale)\n",
    "    3: 1,   # Concert and Ticket Issues (General)\n",
    "    5: 1,   # Concert and Ticket Issues (Shipping)\n",
    "    12: 1,  # Concert and Ticket Issues (Discussion)\n",
    "    14: 1,  # Concert and Ticket Discussion (General)\n",
    "    15: 1,  # Concert and Tour Discussion (Tickets)\n",
    "    20: 1,  # Concert and Ticket (General)\n",
    "    7: 2,   # Concert and Fashion\n",
    "    8: 2,   # Concert and Fashion (Celebration and Conversation)\n",
    "    9: 2,   # Concert Posters and Wallpapers (Fashion-Related)\n",
    "    10: 2,  # Concert and Fashion (Outfits)\n",
    "    18: 2,  # Concert and Fashion (Crewnecks)\n",
    "    4: 3,   # Merchandise Wanted\n",
    "    11: 4,  # Concert and Friendship Bracelets\n",
    "    19: 5,  # Covers and Remixes\n",
    "    1: 6,   # Art and Creativity (Vinyl, CD, Art)\n",
    "    13: 6,  # Art and Creativity (Tattoo and Art)\n",
    "    17: 6,  # Art and Creativity (General)\n",
    "    21: 7,  # Leak and Music\n",
    "    0: 8,   # Others undefineable topics (random)\n",
    "    6: 8,   # Others undefineable topics\n",
    "    16: 8   # Others undefineable topics (Bias Poll)\n",
    "}\n",
    "\n",
    "topic_labels = {\n",
    "    1: 'Concert and Ticket Issues: Discussions about buying, selling, and handling concert tickets, including resale, shipping, and general availability.',\n",
    "    2: 'Concert and Fashion: Discussions about fashion choices for concerts, including outfits, jackets, and general concert-related fashion.',\n",
    "    3: 'Merchandise Wanted: Conversations around buying and selling artist-related merchandise like hoodies, shirts, and other items.',\n",
    "    4: 'Concert and Friendship Bracelets: Topics discussing the creation and exchange of friendship bracelets related to concerts.',\n",
    "    5: 'Covers and Remixes: Discussions about cover versions of songs and remixes in the music community.',\n",
    "    6: 'Art and Creativity: Conversations focused on tattoos, artwork, and other creative expressions related to music and concerts.',\n",
    "    7: 'Leak and Music: Discussions and speculations around leaked music content, including upcoming releases and snippets.',\n",
    "    8: 'Others undefineable topics'\n",
    "}\n",
    "\n",
    "merged_df['topic_id'] = merged_df['topic_id'].replace(merged_topics)\n",
    "\n",
    "# Map new topic IDs to their labels\n",
    "merged_df['topic_label'] = merged_df['topic_id'].map(topic_labels)"
   ],
   "id": "82a94fe1a4a27594",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## for male\n",
    "\n",
    "merged_topics = {\n",
    "    -1: 1,\n",
    "    5: 1,\n",
    "    7: 1,\n",
    "    0: 2,\n",
    "    1: 3,\n",
    "    2: 4,\n",
    "    8: 4,\n",
    "    3: 5,\n",
    "    4: 6,\n",
    "    6: 7,\n",
    "    9: 8,\n",
    "    -99: 8\n",
    "}\n",
    "\n",
    "# Update topic labels\n",
    "topic_labels = {\n",
    "    1: 'General discussions about music, including song recommendations and rap music.',\n",
    "    2: 'Discussions around buying and selling photocards.',\n",
    "    3: 'Personal stories and relationships.',\n",
    "    4: 'Discussions about buying and selling event tickets, and concert experiences.',\n",
    "    5: 'Polls and opinions on various topics.',\n",
    "    6: 'Discussions about merchandise like shirts and hoodies.',\n",
    "    7: 'Tips and discussions about avoiding scams.',\n",
    "    8: 'Others undefineable topics'\n",
    "}\n",
    "\n",
    "merged_df['topic_id'] = merged_df['topic_id'].replace(merged_topics)\n",
    "\n",
    "# Map new topic IDs to their labels\n",
    "merged_df['topic_label'] = merged_df['topic_id'].map(topic_labels)"
   ],
   "id": "9e4059e017114d82",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "merged_df[\"topic_label\"].value_counts()",
   "id": "8757dc3bdd90990c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "combined_male_submissions.to_csv(\"combined_female_submission(after merging topics).csv\", index = False)",
   "id": "d091934db6b59b7a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "merged_df.to_csv(\"combined_male_comments(after merging topics).csv\", index = False)",
   "id": "3ecc4ebb802e6e94",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "topic_model.visualize_documents(selftext, reduced_embeddings=reduced_embeddings, hide_annotations=True, hide_document_hover=False, custom_labels=True)",
   "id": "724366bd4a55b73b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "12bbb9732d1e47b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5137a7e49fb4d6af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "937010337281a8f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1e11e3571c5d1ae0",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
