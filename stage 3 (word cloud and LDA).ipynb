{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install git+https://github.com/huggingface/transformers.git -q -U # transformers version:  4.37.0\n",
    "!pip install git+https://github.com/huggingface/accelerate.git -q -U # accelerate version:  0.27.0\n",
    "!pip install bitsandbytes # bitsandbytes version:  0.42.0\n",
    "!pip install git+https://github.com/huggingface/peft.git -q -U # peft version: 0.7.2\n",
    "!pip install einops\n",
    "!pip install xformers\n",
    "!pip install torchvision\n",
    "!pip install urllib3==1.26.15\n",
    "!pip install scipy==1.12"
   ],
   "id": "8b10a79e751c77fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import necessary libraries and modules for data processing and analysis\n",
    "import re\n",
    "import gc\n",
    "import os\n",
    "import nltk\n",
    "import torch\n",
    "import emoji\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from umap import UMAP\n",
    "from torch import cuda\n",
    "from torch import bfloat16\n",
    "from hdbscan import HDBSCAN\n",
    "from bertopic import BERTopic\n",
    "from huggingface_hub import login\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from sklearn.metrics import silhouette_score\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic.vectorizers import OnlineCountVectorizer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, TextGeneration\n",
    "\n",
    "# download necessary NLTK resources and models\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# set random seed for reproducibility across runs and devices\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    \n",
    "# set device to GPU if available, otherwise use CPU for processing\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'"
   ],
   "id": "fc460cfae43beebe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Text Preprocessing",
   "id": "d328fd336dc4ec9c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# define Text Preprocessor class\n",
    "class TextPreprocessor:\n",
    "    # initialize the TextPreprocessor class with necessary attributes and methods for text preprocessing tasks\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.tokenizer = WordPunctTokenizer()\n",
    "        # define a list of singer names to remove from text data for better topic modeling results (case-insensitive)\n",
    "        self.singer_names = [\n",
    "            \"Bad Bunny\", \"El Conejo Malo\", \"The Weeknd\", \"Abel\", \"Abel Tesfaye\",\n",
    "            \"Morgan Wallen\", \"Ed Sheeran\", \"Ginger Jesus\", \"Ed\", \"Drake\", \n",
    "            \"Drizzy\", \"Champagne Papi\", \"Aubrey\", \"Harry Styles\", \"Feid\", \n",
    "            \"Imagine Dragons\", \"Dan Reynolds\", \"Ben McKee\", \"Daniel Wayne Sermon\", \n",
    "            \"Daniel Platz Platzman\", \"Post Malone\", \"Posty\", \"BTS\", \"Bangtan\", \n",
    "            \"Bangtan Sonyeondan\", \"Tannies\", \"RM\", \"Jin\", \"Suga\", \"J-Hope\", \n",
    "            \"Jimin\", \"V\", \"Jungkook\", \"harry\", \"justin bieber\", \"bieber\", \"justin\", \"namjoon\", \n",
    "            \"Taylor Swift\" , \"T-Swift\" , \"TayTay\" , \"Taylor\" , \"Miss Americana\", \"SZA\", \"Solana Imani Row\", \n",
    "            \"Miley Cyrus\", \"Miley\", \"Hannah Montana\", \"New Jeans\", \"Minji\", \"Hanni\", \"Danielle New Jeans\", \"Haerin\", \"Hyein\", \n",
    "            \"Dua Lipa\", \"Dua\", \"Dula Peep\", \"Olivia Rodrigo\", \"Liv\", \"Ariana Grande\", \"Ari\", \"Ariana\", \"Ms Grande\", \n",
    "            \"Billie Eilish\", \"Billie\", \"Rihanna\", \"RiRi\", \"Badgalriri\", \"Adele\", \"swiftie\", \"Oliia\", \"newjean\", \n",
    "            \"rodrigo\", \"rodrigos\"\n",
    "        ]\n",
    "        # convert singer names to lowercase for case-insensitive matching during text preprocessing tasks\n",
    "        self.singer_names = [name.lower() for name in self.singer_names]\n",
    "\n",
    "    # preprocess text data by removing singer names, emojis, special characters, numbers, and lemmatizing tokens for topic modeling tasks\n",
    "    def preprocess_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = self.remove_singer_names(text)\n",
    "        text = emoji.demojize(text)\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        tokens = [word for word in tokens if word not in self.stop_words and word.isalnum()]\n",
    "        tokens = [self.lemmatize_token(word) for word in tokens]\n",
    "        # return preprocessed text data as a single string with tokens separated by whitespace\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    # remove singer names from text data for better topic modeling results and to avoid bias in topic assignments\n",
    "    def remove_singer_names(self, text):\n",
    "        for name in self.singer_names:\n",
    "            text = text.replace(name, '')\n",
    "        # replace common abbreviations and acronyms with full words for better topic modeling results\n",
    "        text = text.replace('austin', 'album')\n",
    "        text = text.replace('wts', 'want to sell')\n",
    "        text = text.replace('merch', 'merchandise')\n",
    "        # return text data with singer names removed for further preprocessing\n",
    "        return text\n",
    "\n",
    "    # lemmatize tokens in text data to reduce inflectional forms and sometimes derivationally related forms of words to a common base form\n",
    "    def lemmatize_token(self, token):\n",
    "        # get the part of speech tag for each token to lemmatize based on the WordNet lexical database and NLTK library functions\n",
    "        tag = self.get_wordnet_pos(nltk.pos_tag([token])[0][1])\n",
    "        # return lemmatized token based on the part of speech tag if available, otherwise return the original token\n",
    "        return self.lemmatizer.lemmatize(token, pos=tag) if tag else token\n",
    "\n",
    "    # get the WordNet part of speech tag for each token to lemmatize based on the Penn Treebank tag set and WordNet lexical database in NLTK\n",
    "    @staticmethod\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        # converts treebank tag to wordnet tag\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return None"
   ],
   "id": "a119cf6f5ceb656a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# define a function to remove hyperlinks from text data for better topic modeling results\n",
    "def remove_hyperlinks(text):\n",
    "    return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags = re.MULTILINE)"
   ],
   "id": "3de125d74ff0be39"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# define a function to preprocess text data for topic modeling tasks using the TextPreprocessor class and methods\n",
    "def preprocess(df, data_type):\n",
    "    # initialize the TextPreprocessor class for text preprocessing tasks in the dataset based on the data type (comments or submissions)\n",
    "    if data_type == 'comments':\n",
    "        text = 'body'\n",
    "    elif data_type == 'submissions':\n",
    "        text = 'combined_text'\n",
    "    col_index = df.columns.get_loc(text) + 1\n",
    "    # preprocess text data in the dataset using the TextPreprocessor class and methods\n",
    "    preprocessed_text = df[text].apply(preprocessor.preprocess_text)\n",
    "    df.insert(col_index, 'preprocessed_txt', preprocessed_text)\n",
    "    return df"
   ],
   "id": "3ce23f12f7710262"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import female and male submission dataset files for text analysis and topic modeling tasks\n",
    "male_submissions = pd.read_csv(\"/home/haters/Downloads/Toxicity_Detection/output_perspective/output_score/male_submissions_outcome_final.csv\")\n",
    "female_submissions = pd.read_csv(\"/home/haters/Downloads/Toxicity_Detection/output_perspective/output_score/female_submissions_outcome_final.csv\")\n",
    "\n",
    "# combine title and selftext columns into a single column for text analysis and topic modeling tasks in the dataset\n",
    "male_submissions['combined_text'] = male_submissions['title'] + \" \" + male_submissions['selftext']\n",
    "female_submissions['combined_text'] = female_submissions['title'] + \" \" + female_submissions['selftext']\n",
    "\n",
    "# remove hyperlinks from text data in the combined_text column for better topic modeling results\n",
    "male_submissions[\"combined_text\"] = male_submissions[\"combined_text\"].apply(remove_hyperlinks)\n",
    "female_submissions[\"combined_text\"] = female_submissions[\"combined_text\"].apply(remove_hyperlinks)"
   ],
   "id": "bc71ecb5799394ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# initialize the TextPreprocessor class for text preprocessing tasks in the dataset\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# preprocess the male and female submission datasets by using the preprocess function with the TextPreprocessor class and methods\n",
    "male_submissions = preprocess(male_submissions, 'submissions')\n",
    "female_submissions = preprocess(female_submissions, 'submissions')"
   ],
   "id": "6d4c83ac2b2a9930"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Load model and tokenizer from Hugging Face model hub for text generation tasks",
   "id": "c8543d3542ebf91b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# login to the Hugging Face model hub to access and download pre-trained models\n",
    "login(token = \"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\")"
   ],
   "id": "9c1777fabd028c1b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# define the BitsAndBytesConfig class for 4-bit quantization and bfloat16 computation in the model configuration settings for faster inference\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Enable 4-bit quantization\n",
    "    bnb_4bit_quant_type='nf4',  # Normalized float 4\n",
    "    bnb_4bit_use_double_quant=True,  # Enable second quantization after the first\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16  # Computation type\n",
    ")"
   ],
   "id": "452b858752b04ffc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load the pre-trained llama2 7b chat model and tokenizer for text generation tasks using the Hugging Face model hub\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"daryl149/llama-2-7b-chat-hf\")\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"daryl149/llama-2-7b-chat-hf\", trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto', force_download=True)\n",
    "# set the model to evaluation mode for inference tasks\n",
    "model.eval()"
   ],
   "id": "e86e5d335db97c18",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# define the llama2 text generation pipeline for text generation tasks using the pre-trained model and tokenizer from the Hugging Face model hub with specific parameters for text generation tasks\n",
    "generator = transformers.pipeline(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    task='text-generation',\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=500,\n",
    "    repetition_penalty=1.1\n",
    ")"
   ],
   "id": "615c59d5d3bead83",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# system prompt describes information given to all conversations\n",
    "system_prompt = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant for labeling topics.\n",
    "<</SYS>>\n",
    "\"\"\"\n",
    "\n",
    "# example prompt demonstrating the output we are looking for (with using chain-of-thought method) for labeling topics based on the given documents and keywords in the prompt\n",
    "example_prompt = \"\"\"\n",
    "I have a topic that contains the following documents:\n",
    "- I've been checking Ticketmaster every day for Coldplay tickets but they're sold out everywhere. Does anyone have any suggestions on where else I might find them?\n",
    "- Hello all! If anyone is looking to buy Arctic Monkeys tickets for the show in Paris on September 10th, I have two extra tickets available. We can arrange for a safe and secure transaction!\n",
    "- I have general admission tickets for the upcoming Foo Fighters concert and I'm wondering how early I should get to the venue to get a good spot. Any tips from past attendees?\n",
    "- Just scored VIP passes for Lollapalooza this year! I'm super excited to see Kendrick Lamar, Billie Eilish, and The Weeknd. Who else is going? Any advice on the best spots to watch the performances from?\n",
    "- Does anyone know if there's a way to get refunds for canceled concerts? I had tickets for the Red Hot Chili Peppers show next month, but it got canceled and now I'm trying to figure out my options.\n",
    "\n",
    "The topic is described by the following keywords: 'concert, tour, ticket, attend, stage, stadium, wembley, arena, floor, meet, ticketmaster, ticket, stubhub, presale, sale, sell, buy, fee, concert, paypal'.\n",
    "\n",
    "To create a short label for this topic, follow these steps:\n",
    "\n",
    "1. **Identify the main themes**: Analyze the documents and keywords to identify the main themes discussed.\n",
    "2. **Summarize the core message**: Summarize the core message of the topic based on the identified themes.\n",
    "3. **Craft a concise label**: Create a short, descriptive label that encapsulates the core message.\n",
    "\n",
    "Think through these steps carefully and return only the final label.\n",
    "\n",
    "[START OF LABEL]\n",
    "Concert and ticket\n",
    "[END OF LABEL]\n",
    "\"\"\"\n",
    "\n",
    "# our main prompt with documents ([DOCUMENTS]) and keywords ([KEYWORDS]) tags\n",
    "main_prompt = \"\"\"\n",
    "[INST]\n",
    "I have a topic that contains the following documents:\n",
    "[DOCUMENTS]\n",
    "\n",
    "The topic is described by the following keywords: '[KEYWORDS]'.\n",
    "\n",
    "To create a short label for this topic, follow these steps:\n",
    "\n",
    "1. **Identify the main themes**: Analyze the documents and keywords to identify the main themes discussed.\n",
    "2. **Summarize the core message**: Summarize the core message of the topic based on the identified themes.\n",
    "3. **Craft a concise label**: Create a short, descriptive label that encapsulates the core message.\n",
    "\n",
    "Think through these steps carefully and return only the final label.\n",
    "\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "# combine the system prompt, example prompt, and main prompt for labeling topics based on the given documents and keywords\n",
    "prompt = system_prompt + example_prompt + main_prompt"
   ],
   "id": "1a478941f0537a23",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# define the environment variables for the Hugging Face model hub and transformers library to enable parallelism and set the number of OpenMP threads for faster processing\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['OMP_NUM_THREADS'] = \"4\"\n",
    "os.environ['OMP_MAX_ACTIVE_LEVELS'] = \"2\""
   ],
   "id": "26e66c1d30902f19",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Define Evaluation Model Function for Topic Modeling Tasks with Hyperparameter Tuning",
   "id": "b967dee8d38b65bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# define the evaluate model function to evaluate the performance of the UMAP and HDBSCAN models with specific parameters for topic modeling tasks (this function mainly being used for hyperparameter tuning process)\n",
    "def evaluate_model(umap_params, hdbscan_params, selftext, embeddings):\n",
    "    # define the UMAP and HDBSCAN models with specific parameters for dimensionality reduction and clustering tasks in the dataset for topic modeling\n",
    "    umap_model = UMAP(**umap_params)\n",
    "    hdbscan_model = HDBSCAN(**hdbscan_params)\n",
    "\n",
    "    # fit UMAP and HDBSCAN models\n",
    "    umap_embeddings = umap_model.fit_transform(embeddings)\n",
    "    hdbscan_labels = hdbscan_model.fit_predict(umap_embeddings)\n",
    "\n",
    "    # check the number of unique topics\n",
    "    num_topics = len(set(hdbscan_labels)) - (1 if -1 in hdbscan_labels else 0)  # exclude noise points labeled as -1\n",
    "    print(f\"Number of clusters identified by HDBSCAN: {num_topics}\")\n",
    "    \n",
    "    # fit those models into the BERTopic model for topic modeling tasks\n",
    "    topic_model = BERTopic(\n",
    "            embedding_model=embedding_model,\n",
    "            umap_model=umap_model,\n",
    "            hdbscan_model=hdbscan_model,\n",
    "            representation_model=representation_model,\n",
    "            vectorizer_model=vectorizer_model,\n",
    "            ctfidf_model=ctfidf_model,\n",
    "            top_n_words=10,\n",
    "            verbose=True\n",
    "        )\n",
    "    # get topics and probabilities from the BERTopic model for topic modeling tasks\n",
    "    topics, probs = topic_model.fit_transform(selftext, embeddings)\n",
    "    # calculate the silhouette score for the BERTopic model to evaluate the performance of the model\n",
    "    silhouette_avg = silhouette_score(embeddings, topics)\n",
    "\n",
    "    # clean up BERTopic model from memory to avoid memory leaks and bottleneck issues\n",
    "    del topic_model\n",
    "    del embeddings\n",
    "    del umap_model\n",
    "    del hdbscan_model\n",
    "    del umap_embeddings\n",
    "    del hdbscan_labels\n",
    "    # collect garbage to free up memory and resources\n",
    "    gc.collect()\n",
    "    # return the silhouette score for the BERTopic model to evaluate the performance of the model\n",
    "    return silhouette_avg\n",
    "\n",
    "# define the parameter grid for hyperparameter tuning of the UMAP and HDBSCAN models with specific parameters for topic modeling tasks\n",
    "umap_param_grid = [\n",
    "    {'n_neighbors': 15, 'n_components': 2, 'min_dist': 0.0, 'metric': 'cosine'},\n",
    "    {'n_neighbors': 25, 'n_components': 5, 'min_dist': 0.1, 'metric': 'euclidean'},\n",
    "    {'n_neighbors': 10, 'n_components': 3, 'min_dist': 0.2, 'metric': 'manhattan'},\n",
    "    {'n_neighbors': 20, 'n_components': 4, 'min_dist': 0.3, 'metric': 'manhattan'},\n",
    "    {'n_neighbors': 30, 'n_components': 2, 'min_dist': 0.4, 'metric': 'manhattan'},\n",
    "    {'n_neighbors': 35, 'n_components': 5, 'min_dist': 0.5, 'metric': 'cosine'},\n",
    "    {'n_neighbors': 40, 'n_components': 3, 'min_dist': 0.6, 'metric': 'euclidean'},\n",
    "    {'n_neighbors': 50, 'n_components': 4, 'min_dist': 0.7, 'metric': 'manhattan'},\n",
    "    {'n_neighbors': 60, 'n_components': 2, 'min_dist': 0.8, 'metric': 'euclidean'},\n",
    "    {'n_neighbors': 70, 'n_components': 5, 'min_dist': 0.9, 'metric': 'cosine'},\n",
    "    {'n_neighbors': 80, 'n_components': 3, 'min_dist': 0.1, 'metric': 'cosine'},\n",
    "    {'n_neighbors': 90, 'n_components': 4, 'min_dist': 0.2, 'metric': 'euclidean'},\n",
    "    {'n_neighbors': 100, 'n_components': 5, 'min_dist': 0.3, 'metric': 'manhattan'}\n",
    "]\n",
    "\n",
    "hdbscan_param_grid = [\n",
    "    {'min_cluster_size': 50, 'metric': 'euclidean', 'cluster_selection_method': 'eom'},\n",
    "    {'min_cluster_size': 100, 'metric': 'manhattan', 'cluster_selection_method': 'leaf'},\n",
    "    {'min_cluster_size': 30, 'metric': 'manhattan', 'cluster_selection_method': 'eom'},\n",
    "    {'min_cluster_size': 80, 'metric': 'euclidean', 'cluster_selection_method': 'leaf'},\n",
    "    {'min_cluster_size': 90, 'metric': 'manhattan', 'cluster_selection_method': 'eom'},\n",
    "    {'min_cluster_size': 110, 'metric': 'manhattan', 'cluster_selection_method': 'leaf'},\n",
    "    {'min_cluster_size': 140, 'metric': 'euclidean', 'cluster_selection_method': 'eom'},\n",
    "    {'min_cluster_size': 150, 'metric': 'manhattan', 'cluster_selection_method': 'leaf'}\n",
    "]\n",
    "\n",
    "# create a list of parameter combinations\n",
    "param_grid = list(ParameterGrid({\n",
    "    'umap': umap_param_grid,\n",
    "    'hdbscan': hdbscan_param_grid\n",
    "}))"
   ],
   "id": "713afee68664023c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Topic Modeling for male dataset",
   "id": "9fff0324502180a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# extract the preprocessed text data from the combined_male_submissions dataset\n",
    "male_submissions_preprocessed_txt = male_submissions[\"preprocessed_txt\"]\n",
    "\n",
    "# load the pre-trained MiniLM model for sentence embeddings using the SentenceTransformer library and Hugging Face model hub\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# generate sentence embeddings for the preprocessed text data using the MiniLM model for topic modeling tasks with the SentenceTransformer library\n",
    "embeddings = embedding_model.encode(male_submissions_preprocessed_txt, show_progress_bar = True)"
   ],
   "id": "f138e54483fa8712",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# define the KeyBERT and MMR models with specific parameters for keyword extraction and text summarization tasks in the dataset for topic modeling\n",
    "keybert = KeyBERTInspired()\n",
    "mmr = MaximalMarginalRelevance(diversity = 0.3)\n",
    "# define the Llama2 model for text generation tasks using the pre-trained model and tokenizer from the Hugging Face model hub with specific parameters for text generation tasks\n",
    "llama2 = TextGeneration(generator, prompt = prompt)\n",
    "\n",
    "# define the representation model with the KeyBERT, Llama2, and MMR models for text representation and summarization tasks in the dataset for topic modeling\n",
    "representation_model = {\n",
    "    \"KeyBERT\": keybert,\n",
    "    \"Llama2\" : llama2,\n",
    "    \"MMR\" : mmr,\n",
    "}"
   ],
   "id": "3174d5a782c21fa2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# define the ClassTfidfTransformer and OnlineCountVectorizer models with specific parameters for text vectorization and transformation tasks in the dataset for topic modeling\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True, bm25_weighting=True)\n",
    "vectorizer_model = OnlineCountVectorizer(stop_words=\"english\")"
   ],
   "id": "5c8744c7d43dbedd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# evaluate the performance of the UMAP and HDBSCAN models with specific parameters for topic modeling tasks in the dataset based on the hyperparameter tuning process\n",
    "with open('male_parameter_performance.txt', 'w') as file:\n",
    "    # evaluate each combination\n",
    "    best_score = -np.inf\n",
    "    best_params = None\n",
    "\n",
    "    for params in param_grid:\n",
    "        umap_params = params['umap']\n",
    "        hdbscan_params = params['hdbscan']\n",
    "        score = evaluate_model(umap_params, hdbscan_params, male_submissions_preprocessed_txt, embeddings)\n",
    "\n",
    "        # record the performance of each parameter combination\n",
    "        file.write(f\"UMAP Params: {umap_params}, HDBSCAN Params: {hdbscan_params}, Score: {score}\\n\")\n",
    "        file.flush()  # ensure the data is written to the file immediately\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = params\n",
    "\n",
    "print(f\"Best Score: {best_score}\")\n",
    "print(f\"Best Parameters: {best_params}\")"
   ],
   "id": "8880891f1375cd2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# define the best UMAP and HDBSCAN parameters for topic modeling tasks based on the hyperparameter tuning results (uncomment it for running the code with the best parameters after the hyperparameter tuning process) with will take few days to run the hyperparameter tuning process by a powerful GPU (not the lab GPU)\n",
    "\n",
    "#best_umap_params = best_params['umap']\n",
    "#best_hdbscan_params = best_params['hdbscan']\n",
    "#best_umap_model = UMAP(**best_umap_params)\n",
    "#best_hdbscan_model = HDBSCAN(**best_hdbscan_params)"
   ],
   "id": "832726cfbfa029ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# define the UMAP and HDBSCAN models with specific parameters for dimensionality reduction and clustering tasks in the dataset for topic modeling (the best hyperparameters that we tuned and we manually set them) with the best parameters, may check out the best parameters from the hyperparameter tuning process text file\n",
    "# if using the above code then comment the below code\n",
    "umap_model = UMAP(n_neighbors=80, n_components=3, min_dist=0.1, metric='cosine', random_state=42)\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=150, metric='manhattan', cluster_selection_method='leaf', prediction_data=True)\n",
    "# fit the embeddings into the UMAP model for dimensionality reduction tasks to visualize the data in lower dimensions\n",
    "reduced_embeddings = UMAP(n_neighbors=80, n_components=3, min_dist=0.1, metric='cosine', random_state=42).fit_transform(embeddings)"
   ],
   "id": "b5683e04ecfccdd0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# define the BERTopic model with tuned hyperparameters for topic modeling tasks in the dataset\n",
    "topic_model = BERTopic(\n",
    "  # sub-models\n",
    "  embedding_model=embedding_model,\n",
    "  umap_model=umap_model,\n",
    "  hdbscan_model=hdbscan_model,\n",
    "  representation_model=representation_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    ctfidf_model=ctfidf_model,\n",
    "  top_n_words=10,\n",
    "  verbose=True\n",
    ")\n",
    "\n",
    "# train model on the preprocessed text data and embeddings for topic modeling tasks\n",
    "topics, probs = topic_model.fit_transform(male_submissions_preprocessed_txt, embeddings)"
   ],
   "id": "74eb7eaa99357077",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# get the topic information from the BERTopic model for topic modeling tasks in the dataset to analyze the topics and their distributions in the text data for better understanding\n",
    "topic_info = topic_model.get_topic_info()\n",
    "\n",
    "# define the topic labels and their corresponding topics based on the topic information from the BERTopic model for topic modeling tasks in the dataset\n",
    "topic_labels = {row['Topic']: row['Name'] for _, row in topic_info.iterrows()}\n",
    "\n",
    "# map the topic IDs to their corresponding labels for better understanding and analysis of the topics in the text data for topic modeling tasks\n",
    "male_submissions['topic_id'] = topics\n",
    "male_submissions['topic_label'] = male_submissions['topic_id'].map(topic_labels)"
   ],
   "id": "9d52d5531117f169",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# load the male comments dataset file for text analysis and topic modeling tasks\n",
    "male_comments = pd.read_csv(\"/home/haters/Downloads/loaded_data/Combined_data_29Apr/combined_male_comments.csv\")\n",
    "\n",
    "# merge the comments and submission dataset together based on the 'link_id' column to analyze the topics and their distributions in the text data for better understanding\n",
    "merge_male_comments_submissions = male_comments.merge(male_submissions[['name', 'topic_id', 'topic_label']], \n",
    "                                left_on='link_id', \n",
    "                                right_on='name', \n",
    "                                how='left')\n",
    "\n",
    "# drop the 'name' column\n",
    "merge_male_comments_submissions.drop(columns=['name'], inplace=True)"
   ],
   "id": "dc06d252e76ee3aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# extract the null values from the merged dataset for further analysis and preprocessing tasks\n",
    "missing_topic = merge_male_comments_submissions[merge_male_comments_submissions['topic_id'].isna() | merge_male_comments_submissions['topic_label'].isna()]\n",
    "\n",
    "# reset the index of the missing topic dataset for better indexing and analysis\n",
    "missing_topic.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# preprocess the missing topic dataset by using the preprocess function with the TextPreprocessor class and methods for text preprocessing tasks\n",
    "missing_topic = preprocess(missing_topic, 'comments')\n",
    "\n",
    "# get the topics and probabilities from the BERTopic model for topic modeling tasks in the missing topic dataset\n",
    "comment_topics, comment_probs = topic_model.transform(missing_topic['preprocessed_txt'])\n",
    "\n",
    "# update the missing topic dataset with the topics and probabilities from the BERTopic model for topic modeling tasks\n",
    "missing_topic['topic_id'] = comment_topics\n",
    "missing_topic['topic_prob'] = comment_probs\n",
    "merge_male_comments_submissions.update(missing_topic)\n",
    "merge_male_comments_submissions.fillna(-99, inplace=True)"
   ],
   "id": "431456830fefc3b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# define the merged topics and their corresponding topics based on the topic information from the BERTopic model for topic modeling tasks in the dataset\n",
    "merged_topics = {\n",
    "    -1: 1,\n",
    "    5: 1,\n",
    "    7: 1,\n",
    "    0: 2,\n",
    "    1: 3,\n",
    "    2: 4,\n",
    "    8: 4,\n",
    "    3: 5,\n",
    "    4: 6,\n",
    "    6: 7,\n",
    "    9: 8,\n",
    "    -99: 8\n",
    "}\n",
    "\n",
    "# update topic labels\n",
    "topic_labels = {\n",
    "    1: 'General discussions about music, including song recommendations and rap music.',\n",
    "    2: 'Discussions around buying and selling photocards.',\n",
    "    3: 'Personal stories and relationships.',\n",
    "    4: 'Discussions about buying and selling event tickets, and concert experiences.',\n",
    "    5: 'Polls and opinions on various topics.',\n",
    "    6: 'Discussions about merchandise like shirts and hoodies.',\n",
    "    7: 'Tips and discussions about avoiding scams.',\n",
    "    8: 'Others undefineable topics'\n",
    "}\n",
    "\n",
    "# map the new topic IDs to their labels for better understanding and analysis of the topics in the text data for topic modeling tasks\n",
    "male_submissions['topic_id'] = male_submissions['topic_id'].replace(merged_topics)\n",
    "male_submissions['topic_label'] = male_submissions['topic_id'].map(topic_labels)\n",
    "\n",
    "# map the new topic IDs to their labels for better understanding and analysis of the topics in the text data for topic modeling tasks\n",
    "merge_male_comments_submissions['topic_id'] = merge_male_comments_submissions['topic_id'].replace(merged_topics)\n",
    "merge_male_comments_submissions['topic_label'] = merge_male_comments_submissions['topic_id'].map(topic_labels)"
   ],
   "id": "fa81962a56807541"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# export the datasets with the topic labels and IDs to CSV files for further analysis and visualization tasks\n",
    "merge_male_comments_submissions.to_csv(\"merge_female_comments_submissions.csv\", index = False)\n",
    "\n",
    "# visualize the documents in lower-dimensional space using the UMAP model for dimensionality reduction tasks with the BERTopic model for topic modeling tasks\n",
    "topic_model.visualize_documents(male_submissions, reduced_embeddings=reduced_embeddings, hide_annotations=True, hide_document_hover=False, custom_labels=True)"
   ],
   "id": "d4460c174818f766"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Topic Modeling for female dataset",
   "id": "a59221733bfd8b88"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# extract the preprocessed text data from the combined_female_submissions dataset\n",
    "female_submissions_preprocessed_txt = female_submissions[\"preprocessed_txt\"]\n",
    "\n",
    "# generate sentence embeddings for the preprocessed text data using the MiniLM model for topic modeling tasks with the SentenceTransformer library\n",
    "embeddings = embedding_model.encode(female_submissions_preprocessed_txt, show_progress_bar = True)"
   ],
   "id": "5b15d7439ca4001d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# define the KeyBERT and MMR models with specific parameters for keyword extraction and text summarization tasks in the dataset for topic modeling\n",
    "keybert = KeyBERTInspired()\n",
    "mmr = MaximalMarginalRelevance(diversity = 0.3)\n",
    "# define the Llama2 model for text generation tasks using the pre-trained model and tokenizer from the Hugging Face model hub with specific parameters for text generation tasks\n",
    "llama2 = TextGeneration(generator, prompt = prompt)\n",
    "\n",
    "# define the representation model with the KeyBERT, Llama2, and MMR models for text representation and summarization tasks in the dataset for topic modeling\n",
    "representation_model = {\n",
    "    \"KeyBERT\": keybert,\n",
    "    \"Llama2\" : llama2,\n",
    "    \"MMR\" : mmr,\n",
    "}"
   ],
   "id": "6f2a3bfbc8d6b5ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# define the ClassTfidfTransformer and OnlineCountVectorizer models with specific parameters for text vectorization and transformation tasks in the dataset for topic modeling\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True, bm25_weighting=True)\n",
    "vectorizer_model = OnlineCountVectorizer(stop_words=\"english\")"
   ],
   "id": "be8a491f6bb62f9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# evaluate the performance of the UMAP and HDBSCAN models with specific parameters for topic modeling tasks in the dataset based on the hyperparameter tuning process\n",
    "with open('female_parameter_performance.txt', 'w') as file:\n",
    "    # evaluate each combination\n",
    "    best_score = -np.inf\n",
    "    best_params = None\n",
    "\n",
    "    for params in param_grid:\n",
    "        umap_params = params['umap']\n",
    "        hdbscan_params = params['hdbscan']\n",
    "        score = evaluate_model(umap_params, hdbscan_params, female_submissions_preprocessed_txt, embeddings)\n",
    "\n",
    "        # record the performance of each parameter combination\n",
    "        file.write(f\"UMAP Params: {umap_params}, HDBSCAN Params: {hdbscan_params}, Score: {score}\\n\")\n",
    "        file.flush()  # ensure the data is written to the file immediately\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = params\n",
    "\n",
    "print(f\"Best Score: {best_score}\")\n",
    "print(f\"Best Parameters: {best_params}\")"
   ],
   "id": "ce1edcf5ed555bb0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# define the best UMAP and HDBSCAN parameters for topic modeling tasks based on the hyperparameter tuning results (uncomment it for running the code with the best parameters after the hyperparameter tuning process) with will take few days to run the hyperparameter tuning process by a powerful GPU (not the lab GPU)\n",
    "\n",
    "#best_umap_params = best_params['umap']\n",
    "#best_hdbscan_params = best_params['hdbscan']\n",
    "#best_umap_model = UMAP(**best_umap_params)\n",
    "#best_hdbscan_model = HDBSCAN(**best_hdbscan_params)"
   ],
   "id": "e9bd889bdd131d7d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# define the UMAP and HDBSCAN models with specific parameters for dimensionality reduction and clustering tasks in the dataset for topic modeling (the best hyperparameters that we tuned and we manually set them) with the best parameters, may check out the best parameters from the hyperparameter tuning process text file\n",
    "# if using the above code then comment the below code\n",
    "umap_model = UMAP(n_neighbors=5, n_components=2, min_dist=0.0, metric='euclidean', random_state=42)\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=120, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "# fit the embeddings into the UMAP model for dimensionality reduction tasks to visualize the data in lower dimensions\n",
    "reduced_embeddings = UMAP(n_neighbors=5, n_components=2, min_dist=0.0, metric='euclidean', random_state=42).fit_transform(embeddings)\n",
    "\n",
    "# define the BERTopic model with tuned hyperparameters for topic modeling tasks in the dataset\n",
    "topic_model = BERTopic(\n",
    "  # sub-models\n",
    "  embedding_model=embedding_model,\n",
    "  umap_model=umap_model,\n",
    "  hdbscan_model=hdbscan_model,\n",
    "  representation_model=representation_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    ctfidf_model=ctfidf_model,\n",
    "  top_n_words=10,\n",
    "  verbose=True\n",
    ")\n",
    "\n",
    "# train model on the preprocessed text data and embeddings for topic modeling tasks\n",
    "topics, probs = topic_model.fit_transform(female_submissions_preprocessed_txt, embeddings)"
   ],
   "id": "c18f6ec9c9536afb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# get the topic information from the BERTopic model for topic modeling tasks in the dataset to analyze the topics and their distributions in the text data for better understanding\n",
    "topic_info = topic_model.get_topic_info()\n",
    "\n",
    "# define the topic labels and their corresponding topics based on the topic information from the BERTopic model for topic modeling tasks in the dataset\n",
    "topic_labels = {row['Topic']: row['Name'] for _, row in topic_info.iterrows()}\n",
    "\n",
    "# map the topic IDs to their corresponding labels for better understanding and analysis of the topics in the text data for topic modeling tasks\n",
    "female_submissions['topic_id'] = topics\n",
    "female_submissions['topic_label'] = female_submissions['topic_id'].map(topic_labels)"
   ],
   "id": "19afe7a56a8d8548"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# load the female comments dataset file for text analysis and topic modeling tasks\n",
    "female_comments = pd.read_csv(\"/home/haters/Downloads/loaded_data/Combined_data_29Apr/combined_female_comments.csv\")\n",
    "\n",
    "# merge the comments and submission dataset together based on the 'link_id' column to analyze the topics and their distributions in the text data for better understanding\n",
    "merge_female_comments_submissions = female_comments.merge(female_submissions[['name', 'topic_id', 'topic_label']], \n",
    "                                left_on='link_id', \n",
    "                                right_on='name', \n",
    "                                how='left')\n",
    "\n",
    "# drop the 'name' column\n",
    "merge_female_comments_submissions.drop(columns=['name'], inplace=True)"
   ],
   "id": "e4d7cd8b487bf7e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# extract the null values from the merged dataset for further analysis and preprocessing tasks\n",
    "missing_topic = merge_female_comments_submissions[merge_female_comments_submissions['topic_id'].isna() | merge_female_comments_submissions['topic_label'].isna()]\n",
    "\n",
    "# reset the index of the missing topic dataset for better indexing and analysis\n",
    "missing_topic.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# preprocess the missing topic dataset by using the preprocess function with the TextPreprocessor class and methods for text preprocessing tasks\n",
    "missing_topic = preprocess(missing_topic, 'comments')\n",
    "\n",
    "# get the topics and probabilities from the BERTopic model for topic modeling tasks in the missing topic dataset\n",
    "comment_topics, comment_probs = topic_model.transform(missing_topic['preprocessed_txt'])\n",
    "\n",
    "# update the missing topic dataset with the topics and probabilities from the BERTopic model for topic modeling tasks\n",
    "missing_topic['topic_id'] = comment_topics\n",
    "missing_topic['topic_prob'] = comment_probs\n",
    "merge_female_comments_submissions.update(missing_topic)\n",
    "merge_female_comments_submissions.fillna(-99, inplace=True)"
   ],
   "id": "1c857fcd68e4002b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# define the merged topics and their corresponding topics based on the topic information from the BERTopic model for topic modeling tasks in the dataset\n",
    "merged_topics = {\n",
    "    -1: 1,  # Concert and Ticket Issues\n",
    "    2: 1,   # Concert and Ticket Issues (Resale)\n",
    "    3: 1,   # Concert and Ticket Issues (General)\n",
    "    5: 1,   # Concert and Ticket Issues (Shipping)\n",
    "    12: 1,  # Concert and Ticket Issues (Discussion)\n",
    "    14: 1,  # Concert and Ticket Discussion (General)\n",
    "    15: 1,  # Concert and Tour Discussion (Tickets)\n",
    "    20: 1,  # Concert and Ticket (General)\n",
    "    7: 2,   # Concert and Fashion\n",
    "    8: 2,   # Concert and Fashion (Celebration and Conversation)\n",
    "    9: 2,   # Concert Posters and Wallpapers (Fashion-Related)\n",
    "    10: 2,  # Concert and Fashion (Outfits)\n",
    "    18: 2,  # Concert and Fashion (Crewnecks)\n",
    "    4: 3,   # Merchandise Wanted\n",
    "    11: 4,  # Concert and Friendship Bracelets\n",
    "    19: 5,  # Covers and Remixes\n",
    "    1: 6,   # Art and Creativity (Vinyl, CD, Art)\n",
    "    13: 6,  # Art and Creativity (Tattoo and Art)\n",
    "    17: 6,  # Art and Creativity (General)\n",
    "    21: 7,  # Leak and Music\n",
    "    0: 8,   # Others undefineable topics (random)\n",
    "    6: 8,   # Others undefineable topics\n",
    "    16: 8,   # Others undefineable topics (Bias Poll)\n",
    "    -99: 8  # Others undefineable topics\n",
    "}\n",
    "\n",
    "topic_labels = {\n",
    "    1: 'Concert and Ticket Issues: Discussions about buying, selling, and handling concert tickets, including resale, shipping, and general availability.',\n",
    "    2: 'Concert and Fashion: Discussions about fashion choices for concerts, including outfits, jackets, and general concert-related fashion.',\n",
    "    3: 'Merchandise Wanted: Conversations around buying and selling artist-related merchandise like hoodies, shirts, and other items.',\n",
    "    4: 'Concert and Friendship Bracelets: Topics discussing the creation and exchange of friendship bracelets related to concerts.',\n",
    "    5: 'Covers and Remixes: Discussions about cover versions of songs and remixes in the music community.',\n",
    "    6: 'Art and Creativity: Conversations focused on tattoos, artwork, and other creative expressions related to music and concerts.',\n",
    "    7: 'Leak and Music: Discussions and speculations around leaked music content, including upcoming releases and snippets.',\n",
    "    8: 'Others undefineable topics'\n",
    "}\n",
    "\n",
    "female_submissions['topic_id'] = female_submissions['topic_id'].replace(merged_topics)\n",
    "female_submissions['topic_label'] = female_submissions['topic_id'].map(topic_labels)\n",
    "\n",
    "merge_female_comments_submissions['topic_id'] = merge_female_comments_submissions['topic_id'].replace(merged_topics)\n",
    "merge_female_comments_submissions['topic_label'] = merge_female_comments_submissions['topic_id'].map(topic_labels)"
   ],
   "id": "6e0d431ed60357b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# export the datasets with the topic labels and IDs to CSV files for further analysis and visualization tasks\n",
    "merge_female_comments_submissions.to_csv(\"merge_female_comments_submissions.csv\", index = False)"
   ],
   "id": "3a12be52c2cdecc8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# visualize the documents in lower-dimensional space using the UMAP model for dimensionality reduction tasks with the BERTopic model for topic modeling tasks\n",
    "topic_model.visualize_documents(female_submissions, reduced_embeddings=reduced_embeddings, hide_annotations=True, hide_document_hover=False, custom_labels=True)"
   ],
   "id": "b071082f40f5c169",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
