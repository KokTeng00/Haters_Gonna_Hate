{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b2b2ef2-bd4f-452b-a9dc-28ba2d30b71c",
   "metadata": {},
   "source": [
    "##### Required to use command line to run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e47df39-5b4e-4b3a-b754-d43b53075bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataset's sources coming from this torrents\n",
    "# https://academictorrents.com/details/56aa49f9653ba545f48df2e33679f014d2829c10\n",
    "\n",
    "# decompress the zst data\n",
    "# zstd -d --memory=2048MB RC_2010-04.zst (normally, we will replace the \"RC_2010-04\" into * for decompressing every zst file within the folder)\n",
    "\n",
    "# use this code in command line for spliting the decompressed data into several small file then only load into python (need to apply to teenager and hiphophead decompressed data due to memoy problem while loading into python code)\n",
    "# split -b 1048576000 teenagers_comments data_part_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27aeeb4-285b-43c1-813f-622283b8a7b3",
   "metadata": {},
   "source": [
    "# Loading the decompressed data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45603c44-7119-4b04-93d8-6a866d1c9eee",
   "metadata": {},
   "source": [
    "##### Function for loading the data (almost the clean code) into partition datasets and filtering them with important variables and timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7dfdf3-3a6d-4470-8158-ab631cf38ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the decompressed data (comments dataset) into python code\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bbe163-3d93-4b54-8635-6e98f588f15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collection_comments(decompressed_data):\n",
    "    base_path = \"/home/haters/Downloads/temp_data\"\n",
    "    dir_path = os.path.join(base_path, decompressed_data, \"Comments\")\n",
    "    output_dir = os.path.join(base_path, \"loaded_data\", decompressed_data, \"comments\")\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    file_name_comments = os.listdir(dir_path)\n",
    "    comments_columns = [\"body\", \"subreddit\", \"link_id\", \"retrieved_on\"]\n",
    "    start_date = pd.Timestamp(\"2023-01-01\")\n",
    "    end_date = pd.Timestamp(\"2023-12-31\")\n",
    "\n",
    "    chunksize = 1000000  # Define a suitable chunk size\n",
    "    files_per_iteration = len(file_name_comments) // 5\n",
    "\n",
    "    for iteration in range(1, 6):\n",
    "        data_dicts_comments = []\n",
    "        start_index = files_per_iteration * (iteration - 1)\n",
    "        end_index = start_index + files_per_iteration if iteration < 5 else len(file_name_comments)\n",
    "        \n",
    "        for file_name in file_name_comments[start_index:end_index]:\n",
    "            if 'Teenagers' in file_name or 'Music' in file_name or 'Hiphopheads' in file_name:\n",
    "                print(f\"Skipping file: {file_name}\")\n",
    "                continue\n",
    "                \n",
    "            file_path = os.path.join(dir_path, file_name)\n",
    "            \n",
    "            with open(file_path, 'r') as file:\n",
    "                chunk_lines = []\n",
    "    \n",
    "                for line_number, line in enumerate(file, 1):\n",
    "                    chunk_lines.append(line)\n",
    "                    if line_number % chunksize == 0:\n",
    "                        data_dicts_comments.extend(process_chunk(chunk_lines, comments_columns))\n",
    "                        chunk_lines = []  # Reset for next chunk\n",
    "    \n",
    "                if chunk_lines:  # Process any remaining lines\n",
    "                    data_dicts_comments.extend(process_chunk(chunk_lines, comments_columns))\n",
    "                    \n",
    "                print(f'completed file name::: {file_name}')\n",
    "        save_data(data_dicts_comments, decompressed_data, output_dir, comments_columns, start_date, end_date, iteration, file_name)\n",
    "\n",
    "def process_chunk(lines, columns):\n",
    "    data_dicts = []\n",
    "    for line in lines:\n",
    "        try:\n",
    "            # Load the line as a JSON object\n",
    "            data_dict = json.loads(line)\n",
    "            \n",
    "            filtered_dict = {col: data_dict.get(col) for col in columns}  # Filter dict based on columns\n",
    "            data_dicts.append(filtered_dict)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error parsing line: {line}\")\n",
    "    return data_dicts\n",
    "\n",
    "def save_data(data_dicts, decompressed_data, output_dir, columns, start_date, end_date, iteration, file_name):\n",
    "    loaded_dataset = pd.DataFrame(data_dicts)\n",
    "    loaded_dataset = loaded_dataset[columns]\n",
    "    loaded_dataset['retrieved_on'] = pd.to_datetime(loaded_dataset['retrieved_on'], unit='s')\n",
    "    filtered_chunk = loaded_dataset[(loaded_dataset['retrieved_on'] >= start_date) & (loaded_dataset['retrieved_on'] <= end_date)]\n",
    "    csv_path = os.path.join(output_dir, f\"{decompressed_data}_test_{iteration}.csv\")\n",
    "    filtered_chunk.to_csv(csv_path, index=False)\n",
    "    \n",
    "    # Debugging output to monitor rows\n",
    "    print(filtered_chunk.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d271e6d5-6a45-4a20-90a4-70d59980f92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def data_collection_submissions(decompressed_data):\n",
    "    # Set up base and output directories\n",
    "    base_path = \"/home/haters/Downloads/temp_data\"\n",
    "    dir_path = os.path.join(base_path, decompressed_data, \"Submissions\")\n",
    "    output_dir = os.path.join(base_path, \"loaded_data\", decompressed_data, \"submissions\")\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # List all files in the submissions directory\n",
    "    file_name_submissions = os.listdir(dir_path)\n",
    "    submissions_columns = [\"selftext\", \"url\", \"title\", \"subreddit\", \"name\", \"permalink\", \"created_utc\"]\n",
    "    start_date = pd.Timestamp(\"2023-01-01\")\n",
    "    end_date = pd.Timestamp(\"2023-12-31\")\n",
    "\n",
    "    chunksize = 1000000  # Define a suitable chunk size\n",
    "    files_per_iteration = len(file_name_submissions) // 5 # Split the files into 5 iterations\n",
    "\n",
    "    for iteration in range(1, 6):\n",
    "        data_dicts_submissions = []\n",
    "        start_index = files_per_iteration * (iteration - 1)\n",
    "        end_index = start_index + files_per_iteration if iteration < 5 else len(file_name_submissions)\n",
    "\n",
    "        for file_name in file_name_submissions[start_index:end_index]:\n",
    "            # Skip processing for specific subreddits\n",
    "            if 'Teenagers' in file_name or 'Music' in file_name or 'Hiphopheads' in file_name:\n",
    "                print(f\"Skipping file: {file_name}\")\n",
    "                continue\n",
    "            \n",
    "            # Open and read the file line by line\n",
    "            file_path = os.path.join(dir_path, file_name)\n",
    "            with open(file_path, 'r') as file:\n",
    "                chunk_lines = []\n",
    "\n",
    "                for line_number, line in enumerate(file, 1):\n",
    "                    chunk_lines.append(line)\n",
    "                    if line_number % chunksize == 0:\n",
    "                        data_dicts_submissions.extend(process_chunk(chunk_lines, submissions_columns))\n",
    "                        save_data(data_dicts_submissions, decompressed_data, output_dir, submissions_columns, start_date, end_date, iteration, file_name)\n",
    "                        data_dicts_submissions = []  # Reset for next chunk\n",
    "                        \n",
    "                # Process and save any remaining lines\n",
    "                if chunk_lines: \n",
    "                    data_dicts_submissions.extend(process_chunk(chunk_lines, submissions_columns))\n",
    "                    save_data(data_dicts_submissions, decompressed_data, output_dir, submissions_columns, start_date, end_date, iteration, file_name)\n",
    "\n",
    "def process_chunk(lines, columns):\n",
    "    data_dicts = []\n",
    "    for line in lines:\n",
    "        try:\n",
    "            # Convert each line to a JSON object\n",
    "            data_dict = json.loads(line)\n",
    "            \n",
    "            # Filter the JSON object based on required columns\n",
    "            filtered_dict = {col: data_dict.get(col) for col in columns}\n",
    "            data_dicts.append(filtered_dict)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error parsing line: {line}\")\n",
    "    return data_dicts\n",
    "\n",
    "def save_data(data_dicts, decompressed_data, output_dir, columns, start_date, end_date, iteration, file_name):\n",
    "    try:\n",
    "        # Convert list of dict to a dataframe\n",
    "        loaded_dataset = pd.DataFrame(data_dicts)\n",
    "        loaded_dataset = loaded_dataset[columns]\n",
    "        \n",
    "        # Convert 'created_utc' to a datetime object to filtering\n",
    "        loaded_dataset['created_utc'] = pd.to_datetime(loaded_dataset['created_utc'], unit='s')\n",
    "        filtered_chunk = loaded_dataset[(loaded_dataset['created_utc'] >= start_date) & (loaded_dataset['created_utc'] <= end_date)]\n",
    "        \n",
    "        # Save the filtered data to a CSV file\n",
    "        csv_path = os.path.join(output_dir, f\"{decompressed_data}_submissions_{iteration}.csv\")\n",
    "        filtered_chunk.to_csv(csv_path, index=False)\n",
    "\n",
    "        csv_path = os.path.join(output_dir, f\"{decompressed_data}_submissions_sample_{iteration}.csv\")\n",
    "        filtered_chunk.to_csv(csv_path, index=False)\n",
    "    \n",
    "        # Debugging output to monitor rows\n",
    "        print(filtered_chunk.info())\n",
    "    except Exception as e: \n",
    "        print(f\"Failed to process {file_name}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315a94e9-9bb6-4b9c-af20-d32f626b171c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load those dataset from  into the python code\n",
    "\n",
    "data_collection_comments(\"Male_Decompressed_Data\")\n",
    "data_collection_comments(\"Related_Decompressed_Data\")\n",
    "data_collection_comments(\"Female_Decompressed_Data\")\n",
    "data_collection_submissions(\"Male_Decompressed_Data\")\n",
    "data_collection_submissions(\"Female_Decompressed_Data\")\n",
    "data_collection_submissions(\"Related_Decompressed_Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159c4315-cd05-421a-ab31-32bb83e26cd3",
   "metadata": {},
   "source": [
    "## Merging the CSV datasets into a single file\n",
    "Due to the memory issues, we split the loaded files into 4 or 5 parts and then mreged them into a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223e9b3f-db0d-489d-ad22-a8eafe5c271d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_datasets(decompressed_data, data_type=\"comments\"):\n",
    "    \n",
    "    base_path = \"/home/haters/Downloads\"\n",
    "    dir_path = os.path.join(base_path, \"loaded_data\", decompressed_data, data_type)\n",
    "    \n",
    "    files = [os.path.join(dir_path, f) for f in os.listdir(dir_path) if f.endswith('.csv')]\n",
    "\n",
    "    dataframes = []\n",
    "    \n",
    "    # Read each file and append the DataFrame to the list\n",
    "    for file in files:\n",
    "        df = pd.read_csv(file)\n",
    "        dataframes.append(df)\n",
    "\n",
    "    # Save gthe merged DataFrame to a new csv file\n",
    "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "    csv_path = os.path.join(dir_path, f\"combined_{decompressed_data}.csv\")\n",
    "    \n",
    "    print(f'csv path::: {csv_path}')\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3884e06-bedd-4be8-afec-c7caeb285dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_co_female = merge_datasets('Female_Decompressed_Data', 'comments')\n",
    "merge_co_male = merge_datasets('Male_Decompressed_Data', 'comments')\n",
    "merge_co_related = merge_datasets('Related_Decompressed_Data', 'comments')\n",
    "merge_sub_female = merge_datasets('Female_Decompressed_Data', 'submissions')\n",
    "merge_sub_male = merge_datasets('Male_Decompressed_Data', 'submissions')\n",
    "merge_sub_related = merge_datasets('Related_Decompressed_Data', 'submissions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd92d8b8-31b8-4a5c-8452-e6c6e2102f93",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be2c59a-1630-4e66-9c37-a7dfa03014e7",
   "metadata": {},
   "source": [
    "### Filtering data using the singer's name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b93c18e-2bd6-43e3-8037-d3b1c85c9246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Dictionary with artistname and their nicknames\n",
    "artist_keywords = {\n",
    "    (\"Taylor Swift\", \"T-Swift\", \"TayTay\", \"Taylor\", \"Miss Americana\"): \"Taylor Swift\",(\"SZA\", \"Solana Imani Row\"): \"SZA\",\n",
    "    (\"Miley Cyrus\", \"Miley\", \"Hannah Montana\"): \"Miley Cyrus\",\n",
    "    (\"New Jeans\", \"Minji\", \"Hanni\", \"Danielle New Jeans\", \"Haerin\", \"Hyein\", \"maknaes\"): \"New Jeans\",\n",
    "    (\"Dua Lipa\", \"Dua\", \"Dula Peep\"): \"Dua Lipa\",\n",
    "    (\"Olivia Rodrigo\", \"Liv\"): \"Olivia Rodrigo\",\n",
    "    (\"Ariana Grande\", \"Ari\", \"Ariana\", \"Ms Grande\"): \"Ariana Grande\",\n",
    "    (\"Billie Eilish\", \"Billie\"): \"Billie Eilish\",\n",
    "    (\"Rihanna\", \"RiRi\", \"Badgalriri\"): \"Rihanna\",\n",
    "    (\"Adele\",): \"Adele\",\n",
    "    (\"Bad Bunny\", \"El Conejo Malo\"): \"Bad Bunny\",\n",
    "    (\"The Weeknd\", \"Abel\", \"Abel Tesfaye\"): \"The Weeknd\",\n",
    "    (\"Morgan Wallen\",): \"Morgan Wallen\",\n",
    "    (\"Ed Sheeran\", \"Ginger Jesus\", \"Ed\"): \"Ed Sheeran\",\n",
    "    (\"Drake\", \"Drizzy\", \"Champagne Papi\", \"Aubrey\"): \"Drake\",\n",
    "    (\"Harry Styles\",): \"Harry Styles\",\n",
    "    (\"Feid\",): \"Feid\",\n",
    "    (\"Imagine Dragons\", \"Dan Reynolds\", \"Ben McKee\", \"Daniel Wayne Sermon\", \"Daniel Platzman\"): \"Imagine Dragons\",\n",
    "    (\"Post Malone\", \"Posty\"): \"Post Malone\",\n",
    "    (\"BTS\", \"Bangtan\", \"Bangtan Sonyeondan\", \"Tannies\", \"RM\", \"Jin\", \"Suga\", \"J-Hope\", \"Jimin\", \"V\", \"Jungkook\"): \"BTS\"\n",
    "}\n",
    "\n",
    "# Function for checking each title against the dictionary of artist names\n",
    "def find_artist(title):\n",
    "    for keywords, artist_name in artist_keywords.items():\n",
    "        for keyword in keywords:\n",
    "            # Verwende Regex, um nur ganze WÃ¶rter zu matchen\n",
    "            if re.search(r'\\b' + re.escape(keyword) + r'\\b', title, re.IGNORECASE):\n",
    "                return artist_name\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78b14dd-ef97-4320-9b6c-0c8a133b8202",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preprocess for female's related subreddits submission\n",
    "related_submission_female = pd.read_csv('/home/haters/Downloads/temp_data/Related_Submissions_Decompressed_Data_female.csv')\n",
    "related_submission_female_clean = related_submission_female.drop_duplicates(subset='permalink', keep='first')\n",
    "\n",
    "# preprocess for male's related subreddits submission\n",
    "related_submission_male = pd.read_csv('/home/haters/Downloads/temp_data/Related_Submissions_Decompressed_Data_male.csv')\n",
    "related_submission_male_clean = related_submission_male.drop_duplicates(subset='permalink', keep='first')\n",
    "\n",
    "print(related_submission_female_clean.head(10))\n",
    "print(related_submission_male_clean.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e79f4c-b521-47a6-9481-3bdbcd356851",
   "metadata": {},
   "source": [
    "### Merging submissions and comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d60bcd-c4ff-4318-8f11-7813e069d6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merging_related (submission,gender):\n",
    "    \n",
    "    # Lists of artists categorized by gender\n",
    "    female_list = [\"Taylor Swift\", \"SZA\", \"Miley Cyrus\", \"New Jeans\", \"Dua Lipa\", \"Olivia Rodrigo\", \"Ariana Grande\", \"Billie Eilish\", \"Rihanna\", \"Adele\"]\n",
    "    male_list = [\"Bad Bunny\", \"The Weeknd\", \"Morgan Wallen\", \"Ed Sheeran\", \"Drake\", \"Harry Styles\", \"Feid\", \"Imagine Dragons\", \"Post Malone\", \"BTS\"]\n",
    "    \n",
    "    # Identify the artist from the 'title' column in the submission dataset\n",
    "    submission['artist'] = submission['title'].apply(find_artist)\n",
    "    \n",
    "    # Create a temporary ID from the 'title' column in the submission dataset\n",
    "    submission['permalink'] = submission['permalink'].astype(str)\n",
    "    submission['temp_id'] = submission['permalink'].str.split('/').str[4]\n",
    "    submission_ids = submission[\"temp_id\"].unique()\n",
    "    \n",
    "    comment_file_path = \"/home/haters/Downloads/loaded_data/Related_Decompressed_Data/comments\"\n",
    "    file_name_comments = os.listdir(comment_file_path)\n",
    "    \n",
    "    all_data = pd.DataFrame() # Initialize an empty DataFrame\n",
    "    \n",
    "    for file_name in file_name_comments:\n",
    "        if file_name.endswith('.csv'): # Process only CSV files\n",
    "            file_path = os.path.join(comment_file_path, file_name)\n",
    "            temp_df = pd.read_csv(file_path)\n",
    "            temp_df[\"link_id\"] = temp_df[\"link_id\"].astype(str)\n",
    "            temp_df[\"temp_id\"] = temp_df[\"link_id\"].str[3:] # Exact temporary ID from 'linked_id'\n",
    "            temp_df = temp_df[temp_df['temp_id'].isin(submission_ids)] # Filter comments by submission IDs\n",
    "            all_data = pd.concat([all_data, temp_df]) # Concatenate relevant comments\n",
    "            \n",
    "    # Merge submissions data with comments based on 'temp_id' to associate each comment with an artist        \n",
    "    sub_data = submission[[\"temp_id\", \"artist\"]]\n",
    "    merged_dataset = pd.merge(all_data, sub_data, on='temp_id', how='inner')\n",
    "\n",
    "    # Filter the merge dataset based on gender preference, excluding artists not matching the gender\n",
    "    if gender == 'male':\n",
    "        merged_dataset = merged_dataset[~merged_dataset['artist'].isin(female_list)]\n",
    "    else:\n",
    "        merged_dataset = merged_dataset[~merged_dataset['artist'].isin(male_list)]\n",
    "        \n",
    "    return merged_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9471e78-0d0d-4f00-9734-48cb4b5e3c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "related_submission_female = merging_related(related_submission_female_clean,'female')\n",
    "related_submission_male = merging_related(related_submission_male_clean,'male')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f05f45f-906c-46d2-94b2-cf8c9ebc2997",
   "metadata": {},
   "source": [
    "### Convert the dataframe into csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fd7cc8-338b-4bec-952a-0711998d330e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the new submisisons with the artist\n",
    "\n",
    "# Convert the data into csv file\n",
    "related_submission_male.to_csv('related_submission_male.csv', index=False)\n",
    "related_submission_female.to_csv('related_submission_female.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00374bd5-6fed-48ba-b38d-8d2788469a18",
   "metadata": {},
   "source": [
    "## Checking the final result data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ab2478-98f8-414d-a910-737735eb8923",
   "metadata": {},
   "source": [
    "#### 1. Filtering data using the singer's name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa13a49-e07b-48f0-8a15-e5cb77a96a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data and combine them in Python / Adds a new column singer_name \n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, data_name):\n",
    "        \n",
    "        self.data_name = data_name\n",
    "        self.singer_map = {\n",
    "            'female': {\n",
    "                'taylor': 'Taylor Swift', 'swift': 'Taylor Swift', 'sza': 'SZA', 'miley': 'Miley Cyrus',\n",
    "                'newjeans': 'New Jeans', 'maknaes': 'New Jeans', 'kimminji': 'New Jeans',\n",
    "                'hanni': 'New Jeans', 'mojihye': 'New Jeans', 'dua': 'Dua Lipa',\n",
    "                'olivia': 'Olivia Rodrigo', 'ariana': 'Ariana Grande', 'billie': 'Billie Eilish',\n",
    "                'rihanna': 'Rihanna', 'adele': 'Adele'\n",
    "            },\n",
    "            'male': {\n",
    "                'bunny': 'Bad Bunny', 'weeknd': 'The Weeknd', 'morgan': 'Morgan Wallen',\n",
    "                'sheeran': 'Ed Sheeran', 'drake': 'Drake', 'harry': 'Harry Styles', 'larry': 'Harry Styles',\n",
    "                'feid': 'Feid', 'dragon': 'Imagine Dragons', 'malone': 'Post Malone', 'bts': 'BTS', \n",
    "                'justin': 'Justin Bieber'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    def load_and_combine(self, data_type='comments'):\n",
    "        # Load and combine all CSV files from a specified directory into a single DataFrame.\n",
    "        \n",
    "        file_path = f\"/home/haters/Downloads/loaded_data/{self.data_name}/{data_type}\"\n",
    "        \n",
    "        files = [os.path.join(file_path, file) for file in os.listdir(file_path) if file.endswith('.csv')]\n",
    "        dataframes = []\n",
    "        \n",
    "        for file in files: \n",
    "            df = pd.read_csv(file)\n",
    "            dataframes.append(df)\n",
    "    \n",
    "        # Generate a list of full paths to all CSV files in the directory\n",
    "        files = [os.path.join(file_path, file) for file in os.listdir(file_path) if file.endswith('.csv')]        \n",
    "        \n",
    "        # Concatenate all DataFrames in the list into a single DataFrame\n",
    "        combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "        ## Data Cleaning\n",
    "        # Remove duplicate rows\n",
    "        combined_df.drop_duplicates(inplace=True)\n",
    "        # Handle missing values\n",
    "        combined_df.dropna(inplace=True)\n",
    "    \n",
    "        if data_type == 'comments':\n",
    "            combined_df = combined_df[(combined_df['body'] != '[deleted]') & (combined_df['body'] != '[removed]')]\n",
    "        elif data_type == 'submissions':\n",
    "            combined_df = combined_df[(combined_df['selftext'] != '[deleted]') & (combined_df['selftext'] != '[removed]')]\n",
    "    \n",
    "        return combined_df\n",
    "\n",
    "\n",
    "    \n",
    "    def add_singer_name(self, df, gender='female'): \n",
    "        # Adds a new column 'artist' to the DataFrame based on 'subreddit' column content.\n",
    "        # Assigns singer's name based on keywords found in the 'subreddit' column.\n",
    "    \n",
    "        if gender in self.singer_map and 'subreddit' in df.columns:\n",
    "            # Apply the mapping using a more robust function that checks string presence\n",
    "            df['artist'] = df['subreddit'].apply(\n",
    "                lambda x: next((name for key, name in self.singer_map[gender].items() if isinstance(x, str) and key in x.lower()), None)\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Invalid gender or 'subreddit' column missing\")\n",
    "    \n",
    "        return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c42a60e-3d50-40c9-b307-235451702811",
   "metadata": {},
   "source": [
    "**Load Dataset for male**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0aa471-1726-4027-89c8-b0af7bdda894",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load comments data\n",
    "\n",
    "loader = DataLoader('Male_Decompressed_Data')\n",
    "combined_male_comments = loader.load_and_combine('comments')\n",
    "combined_male_comments = loader.add_singer_name(combined_male_comments, 'male')\n",
    "\n",
    "print(f'rows: {combined_male_comments.shape[0]}')\n",
    "combined_male_comments.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b2599b-005e-4aa3-9847-a61f9b41c6ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Load submissions data\n",
    "\n",
    "loader = DataLoader('Male_Decompressed_Data')\n",
    "combined_male_submissions = loader.load_and_combine('submissions')\n",
    "combined_male_submissions = loader.add_singer_name(combined_male_submissions, 'female')\n",
    "combined_male_submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9097c8de-253e-4b22-9fd2-86f0038b126f",
   "metadata": {},
   "source": [
    "**Load Dataset for female**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3659b5a-0797-49d0-8c31-c95b6d5ac9fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Load comments data\n",
    "\n",
    "loader = DataLoader('Female_Decompressed_Data')\n",
    "combined_female_comments = loader.load_and_combine('comments')\n",
    "combined_female_comments = loader.add_singer_name(combined_female_comments, 'female')\n",
    "combined_female_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17671eb1-c448-465a-a799-4dfeaa6cd1a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Load submissions data\n",
    "\n",
    "loader = DataLoader('Female_Decompressed_Data')\n",
    "combined_female_submissions = loader.load_and_combine('submissions')\n",
    "combined_female_submissions = loader.add_singer_name(combined_female_submissions, 'female')\n",
    "combined_female_submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0460175-9ffe-4fbf-910d-027c3fc168b2",
   "metadata": {},
   "source": [
    "### Merging files into a single file for each gender: female and male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c515d9ec-62fb-433b-87b1-f3e500ea6fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_datasets(csv_path, df, data_type, gender):\n",
    "    # Loads a CSV file into a DataFrame, combines it with an existing DataFrame, and saves the combined DataFrame to a new CSV file.\n",
    "\n",
    "    output_path = f\"/home/haters/Downloads/loaded_data/combined_{gender}_{data_type}.csv\"\n",
    "    \n",
    "    # Load the CSV file into a DataFrame\n",
    "    new_df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Clean the data\n",
    "    new_df.drop_duplicates(inplace=True)\n",
    "    new_df.dropna(inplace=True)\n",
    "\n",
    "    if data_type == 'comments':\n",
    "        new_df = new_df[(new_df['body'] != '[deleted]') & (new_df['body'] != '[removed]')]\n",
    "        \n",
    "        count_specific_text = combined_male_comments['body'].str.contains(txt, na=False).sum()\n",
    "        \n",
    "    elif data_type == 'submissions':\n",
    "        new_df = new_df[(new_df['selftext'] != '[deleted]') & (new_df['selftext'] != '[removed]')]\n",
    "        \n",
    "    # Combine the existing DataFrame with the new DataFrame\n",
    "    combined_df = pd.concat([df, new_df], ignore_index=True)\n",
    "     \n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bb125c-9821-4763-bdd7-4b65f19e59b8",
   "metadata": {},
   "source": [
    "#### Submissions data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93955f85-2428-4893-b56b-51fcf8efa1ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combine the data into a single file(male)\n",
    "path_male_com = '/home/haters/Downloads/temp_data/related_submission_male.csv'\n",
    "male_submissions_df = combine_datasets(path_male_com, combined_male_submissions, 'submissions', 'male')\n",
    "\n",
    "# Check for missing data\n",
    "missing_artists = male_submissions_df[male_submissions_df['artist'].isna()]\n",
    "print(f'missing rows(male submissions): {missing_artists.head()}')\n",
    "\n",
    "# Check the combined data\n",
    "print(f'total rows(male submissions): {male_submissions_df.shape[0]}')\n",
    "male_submissions_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7461f483-2802-45d3-ad1a-5e32f57eb010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the data into a single file(female)\n",
    "path_female_com = '/home/haters/Downloads/temp_data/related_submission_female.csv'\n",
    "female_submissions_df = combine_datasets(path_female_com, combined_female_submissions, 'submissions', 'female')\n",
    "\n",
    "# Check for missing data\n",
    "missing_artists = female_submissions_df[female_submissions_df['artist'].isna()]\n",
    "print(f'missing rows(female submissions): {missing_artists.head()}')\n",
    "\n",
    "# Check the combined data\n",
    "print(f'total rows(female submissions): {female_submissions_df.shape[0]}')\n",
    "female_submissions_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2ee603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the datatframe into csv file\n",
    "male_submissions_df.to_csv(\"../loaded_data/male_submissions_outcome_final.csv\")\n",
    "female_submissions_df.to_csv(\"../loaded_data/female_submissions_outcome_final.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415918ee-476f-4075-8ddd-c4a897617f79",
   "metadata": {},
   "source": [
    "#### Comments Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950b1c70-e6c8-4901-bb41-740967c02fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the data into a single file(male)\n",
    "path_male_sub = '/home/haters/Downloads/temp_data/combine_male_related_subreddit.csv'\n",
    "male_comments_df = combine_datasets(path_male_sub, combined_male_comments, 'comments', 'male')\n",
    "\n",
    "# Check for missing data\n",
    "missing_artists = male_comments_df[male_comments_df['artist'].isna()]\n",
    "print(f'missing rows(male comments): {missing_artists.head()}')\n",
    "\n",
    "# Check the combined data\n",
    "print(f'total rows(male comments): {male_comments_df.shape[0]}')\n",
    "male_comments_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9015954a-34cf-4c9e-a8ac-70fbca722347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the data into a single file(female)\n",
    "path_female_sub= '/home/haters/Downloads/temp_data/combine_female_related_subreddit.csv'\n",
    "female_comments_df = combine_datasets(path_female_sub, combined_female_comments, 'comments', 'female')\n",
    "\n",
    "# Check for missing data\n",
    "missing_artists = female_comments_df[female_comments_df['artist'].isna()]\n",
    "print(f'missing rows(fefmale comments): {missing_artists.head()}')\n",
    "\n",
    "# Check the combined data\n",
    "print(f'total rows(female comments): {female_comments_df.shape[0]}')\n",
    "female_comments_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342b9f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the datatframe into csv file\n",
    "male_comments_df.to_csv(\"../loaded_data/male_comments_outcome_final.csv\")\n",
    "female_comments_df.to_csv(\"../loaded_data/female_comments_outcome_final.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a089af09-dfb4-4cac-a31a-6f7c4675b14e",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8324c1b5-ee91-4b41-a180-aa39e89cdc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22756b78-3a97-4b30-8dac-711fff2f2bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee92dea-552b-4936-a3fc-e402c68eecae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Text Preprocessor class\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        # Initialize the lemmatizer, stop words list, and tokenizer\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.tokenizer = WordPunctTokenizer()\n",
    "        \n",
    "        # List of singer names and their aliases to be removed from the text\n",
    "        self.singer_names = [\n",
    "            \"Bad Bunny\", \"El Conejo Malo\", \"The Weeknd\", \"Abel\", \"Abel Tesfaye\",\n",
    "            \"Morgan Wallen\", \"Ed Sheeran\", \"Ginger Jesus\", \"Ed\", \"Drake\", \n",
    "            \"Drizzy\", \"Champagne Papi\", \"Aubrey\", \"Harry Styles\", \"Feid\", \n",
    "            \"Imagine Dragons\", \"Dan Reynolds\", \"Ben McKee\", \"Daniel Wayne Sermon\", \n",
    "            \"Daniel Platz Platzman\", \"Post Malone\", \"Posty\", \"BTS\", \"Bangtan\", \n",
    "            \"Bangtan Sonyeondan\", \"Tannies\", \"RM\", \"Jin\", \"Suga\", \"J-Hope\", \n",
    "            \"Jimin\", \"V\", \"Jungkook\", \"harry\", \"justin bieber\", \"bieber\", \"justin\", \"namjoon\", \n",
    "            \"Taylor Swift\" , \"T-Swift\" , \"TayTay\" , \"Taylor\" , \"Miss Americana\", \"SZA\", \"Solana Imani Row\", \n",
    "            \"Miley Cyrus\", \"Miley\", \"Hannah Montana\", \"New Jeans\", \"Minji\", \"Hanni\", \"Danielle New Jeans\", \"Haerin\", \"Hyein\", \n",
    "            \"Dua Lipa\", \"Dua\", \"Dula Peep\", \"Olivia Rodrigo\", \"Liv\", \"Ariana Grande\", \"Ari\", \"Ariana\", \"Ms Grande\", \n",
    "            \"Billie Eilish\", \"Billie\", \"Rihanna\", \"RiRi\", \"Badgalriri\", \"Adele\", \"swiftie\", \"Oliia\", \"newjean\", \n",
    "            \"rodrigo\", \"rodrigos\"\n",
    "        ]\n",
    "        # Convert all singer names to lowercase\n",
    "        self.singer_names = [name.lower() for name in self.singer_names]\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        text = text.lower() # Convert text into lowercase\n",
    "        text = self.remove_singer_names(text) # Remove mentions of singer names\n",
    "        text = emoji.demojize(text) # Convert emojis to text descriptions\n",
    "        text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation and special characters\n",
    "        text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "        tokens = self.tokenizer.tokenize(text) # Tokenize the text into words\n",
    "        tokens = [word for word in tokens if word not in self.stop_words and word.isalnum()] # Remove stop words and non-alphanumeric tokens\n",
    "        tokens = [self.lemmatize_token(word) for word in tokens] # Lemmatize each token to its root form\n",
    "        \n",
    "        # Return the processed tokens as a single string\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    def remove_singer_names(self, text):\n",
    "        # Remove occurrences of singer names with an empty string\n",
    "        for name in self.singer_names:\n",
    "            text = text.replace(name, '')\n",
    "        \n",
    "        # Additional specific text replacements\n",
    "        text = text.replace('austin', 'album')\n",
    "        text = text.replace('wts', 'want to sell')\n",
    "        text = text.replace('merch', 'merchandise')\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def lemmatize_token(self, token):\n",
    "        # Determine the part of POS tag for the token\n",
    "        tag = self.get_wordnet_pos(nltk.pos_tag([token])[0][1])\n",
    "        \n",
    "        # Lemmatize the token based on its POS tag\n",
    "        return self.lemmatizer.lemmatize(token, pos=tag) if tag else token\n",
    "\n",
    "    @staticmethod\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        # Converts treebank tag to wordnet tag\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f5a8dad8ea6e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "male_submissions = pd.read_csv(\"../loaded_data/male_submissions_outcome_final.csv\")\n",
    "female_submissions = pd.read_csv(\"../loaded_data/female_submissions_outcome_final.csv\")\n",
    "\n",
    "# Combine 'title' and 'selftext' into a new column 'combined_text'\n",
    "male_submissions['combined_text'] = male_submissions['title'] + \" \" + male_submissions['selftext']\n",
    "female_submissions['combined_text'] = female_submissions['title'] + \" \" + female_submissions['selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b47e105a731b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hyperlinks(text):\n",
    "    return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags = re.MULTILINE)\n",
    "\n",
    "male_submissions[\"combined_text\"] = male_submissions[\"combined_text\"].apply(remove_hyperlinks)\n",
    "female_submissions[\"combined_text\"] = female_submissions[\"combined_text\"].apply(remove_hyperlinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558bde47-1818-41e8-83a1-a3d41b859e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the TextPreprocessor class\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "def preprocess(df, data_type):\n",
    "    \n",
    "    if data_type == 'comments': # For comments, the text to preprocess is in the 'body' column\n",
    "        text = 'body'\n",
    "    elif data_type == 'submissions': # For submissions, the text to preprocess is in the 'combined_text' column\n",
    "        text = 'combined_text'\n",
    "        \n",
    "    # Find the index of the text column where the new preprocessed column will be inserted\n",
    "    col_index = df.columns.get_loc(text) + 1\n",
    "    # Apply the text preprocessing\n",
    "    preprocessed_text = df[text].apply(preprocessor.preprocess_text)\n",
    "    # Insert the preprocessed text as a new column right after the original text column\n",
    "    df.insert(col_index, 'preprocessed_txt', preprocessed_text)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb859c7-436d-470c-9c73-5aa304011668",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_male_submissions = preprocess(male_submissions, 'submissions')\n",
    "combined_female_submissions = preprocess(female_submissions, 'submissions')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
