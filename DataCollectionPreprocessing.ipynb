{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b2b2ef2-bd4f-452b-a9dc-28ba2d30b71c",
   "metadata": {},
   "source": [
    "##### Required to use command line to run the code"
   ]
  },
  {
   "cell_type": "code",
   "id": "7e47df39-5b4e-4b3a-b754-d43b53075bf4",
   "metadata": {},
   "source": [
    "# the dataset's sources coming from this torrents\n",
    "# https://academictorrents.com/details/56aa49f9653ba545f48df2e33679f014d2829c10\n",
    "\n",
    "# decompress the zst data\n",
    "# zstd -d --memory=2048MB RC_2010-04.zst (normally, we will replace the \"RC_2010-04\" into * for decompressing every zst file within the folder)\n",
    "\n",
    "# use this code in command line for spliting the decompressed data into several small file then only load into python (need to apply to teenager and hiphophead decompressed data due to memoy problem while loading into python code)\n",
    "# split -b 1048576000 teenagers_comments data_part_"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f27aeeb4-285b-43c1-813f-622283b8a7b3",
   "metadata": {},
   "source": [
    "# Loading the decompressed data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45603c44-7119-4b04-93d8-6a866d1c9eee",
   "metadata": {},
   "source": [
    "##### Function for loading the data (almost the clean code) into partition datasets and filtering them with important variables and timeframe"
   ]
  },
  {
   "cell_type": "code",
   "id": "6a7dfdf3-3a6d-4470-8158-ab631cf38ce9",
   "metadata": {},
   "source": [
    "# loading the decompressed data (comments dataset) into python code\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "28bbe163-3d93-4b54-8635-6e98f588f15c",
   "metadata": {},
   "source": [
    "def data_collection_comments(decompressed_data):\n",
    "    base_path = \"/home/haters/Downloads/temp_data\"\n",
    "    dir_path = os.path.join(base_path, decompressed_data, \"Comments\")\n",
    "    output_dir = os.path.join(base_path, \"loaded_data\", decompressed_data, \"comments\")\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    file_name_comments = os.listdir(dir_path)\n",
    "    comments_columns = [\"body\", \"subreddit\", \"link_id\", \"retrieved_on\"]\n",
    "    start_date = pd.Timestamp(\"2023-01-01\")\n",
    "    end_date = pd.Timestamp(\"2023-12-31\")\n",
    "\n",
    "    chunksize = 1000000  # Define a suitable chunk size\n",
    "    files_per_iteration = len(file_name_comments) // 5\n",
    "\n",
    "    for iteration in range(1, 6):\n",
    "        data_dicts_comments = []\n",
    "        start_index = files_per_iteration * (iteration - 1)\n",
    "        end_index = start_index + files_per_iteration if iteration < 5 else len(file_name_comments)\n",
    "        \n",
    "        for file_name in file_name_comments[start_index:end_index]:\n",
    "            if 'Teenagers' in file_name or 'Music' in file_name or 'Hiphopheads' in file_name:\n",
    "                print(f\"Skipping file: {file_name}\")\n",
    "                continue\n",
    "                \n",
    "            file_path = os.path.join(dir_path, file_name)\n",
    "            \n",
    "            with open(file_path, 'r') as file:\n",
    "                # data_dicts_comments = []\n",
    "                chunk_lines = []\n",
    "    \n",
    "                for line_number, line in enumerate(file, 1):\n",
    "                    chunk_lines.append(line)\n",
    "                    if line_number % chunksize == 0:\n",
    "                        data_dicts_comments.extend(process_chunk(chunk_lines, comments_columns))\n",
    "                        # save_data(data_dicts_comments, decompressed_data, output_dir, comments_columns, start_date, end_date, iteration)\n",
    "                        chunk_lines = []  # Reset for next chunk\n",
    "    \n",
    "                if chunk_lines:  # Process any remaining lines\n",
    "                    data_dicts_comments.extend(process_chunk(chunk_lines, comments_columns))\n",
    "                    # save_data(data_dicts_comments, decompressed_data, output_dir, comments_columns, start_date, end_date, iteration)\n",
    "                print(f'completed file name::: {file_name}')\n",
    "        save_data(data_dicts_comments, decompressed_data, output_dir, comments_columns, start_date, end_date, iteration, file_name)\n",
    "\n",
    "def process_chunk(lines, columns):\n",
    "    data_dicts = []\n",
    "    for line in lines:\n",
    "        try:\n",
    "            # Load the line as a JSON object\n",
    "            data_dict = json.loads(line)\n",
    "            \n",
    "            filtered_dict = {col: data_dict.get(col) for col in columns}  # Filter dict based on columns\n",
    "            data_dicts.append(filtered_dict)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error parsing line: {line}\")\n",
    "    return data_dicts\n",
    "\n",
    "def save_data(data_dicts, decompressed_data, output_dir, columns, start_date, end_date, iteration, file_name):\n",
    "    loaded_dataset = pd.DataFrame(data_dicts)\n",
    "    loaded_dataset = loaded_dataset[columns]\n",
    "    loaded_dataset['retrieved_on'] = pd.to_datetime(loaded_dataset['retrieved_on'], unit='s')\n",
    "    filtered_chunk = loaded_dataset[(loaded_dataset['retrieved_on'] >= start_date) & (loaded_dataset['retrieved_on'] <= end_date)]\n",
    "    csv_path = os.path.join(output_dir, f\"{decompressed_data}_test_{iteration}.csv\")\n",
    "    filtered_chunk.to_csv(csv_path, index=False)\n",
    "    \n",
    "    # Debugging output to monitor rows\n",
    "    print(filtered_chunk.info())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d271e6d5-6a45-4a20-90a4-70d59980f92c",
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def data_collection_submissions(decompressed_data):\n",
    "    base_path = \"/home/haters/Downloads/temp_data\"\n",
    "    dir_path = os.path.join(base_path, decompressed_data, \"Submissions\")\n",
    "    output_dir = os.path.join(base_path, \"loaded_data\", decompressed_data, \"submissions\")\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    file_name_submissions = os.listdir(dir_path)\n",
    "    submissions_columns = [\"selftext\", \"url\", \"title\", \"subreddit\", \"name\", \"permalink\", \"created_utc\"]\n",
    "    start_date = pd.Timestamp(\"2023-01-01\")\n",
    "    end_date = pd.Timestamp(\"2023-12-31\")\n",
    "\n",
    "    chunksize = 1000000  # Define a suitable chunk size\n",
    "    files_per_iteration = len(file_name_submissions) // 5\n",
    "\n",
    "    for iteration in range(1, 6):\n",
    "        data_dicts_submissions = []\n",
    "        start_index = files_per_iteration * (iteration - 1)\n",
    "        end_index = start_index + files_per_iteration if iteration < 5 else len(file_name_submissions)\n",
    "\n",
    "        for file_name in file_name_submissions[start_index:end_index]:\n",
    "            if 'Teenagers' in file_name or 'Music' in file_name or 'Hiphopheads' in file_name:\n",
    "                print(f\"Skipping file: {file_name}\")\n",
    "                continue\n",
    "\n",
    "            file_path = os.path.join(dir_path, file_name)\n",
    "            with open(file_path, 'r') as file:\n",
    "                chunk_lines = []\n",
    "\n",
    "                for line_number, line in enumerate(file, 1):\n",
    "                    chunk_lines.append(line)\n",
    "                    if line_number % chunksize == 0:\n",
    "                        data_dicts_submissions.extend(process_chunk(chunk_lines, submissions_columns))\n",
    "                        save_data(data_dicts_submissions, decompressed_data, output_dir, submissions_columns, start_date, end_date, iteration, file_name)\n",
    "                        data_dicts_submissions = []  # Reset for next chunk\n",
    "\n",
    "                if chunk_lines:  # Process any remaining lines\n",
    "                    data_dicts_submissions.extend(process_chunk(chunk_lines, submissions_columns))\n",
    "                    save_data(data_dicts_submissions, decompressed_data, output_dir, submissions_columns, start_date, end_date, iteration, file_name)\n",
    "\n",
    "def process_chunk(lines, columns):\n",
    "    data_dicts = []\n",
    "    for line in lines:\n",
    "        try:\n",
    "            # Load the line as a JSON object\n",
    "            data_dict = json.loads(line)\n",
    "            \n",
    "            filtered_dict = {col: data_dict.get(col) for col in columns}  # Filter dict based on columns\n",
    "            data_dicts.append(filtered_dict)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error parsing line: {line}\")\n",
    "    return data_dicts\n",
    "\n",
    "def save_data(data_dicts, decompressed_data, output_dir, columns, start_date, end_date, iteration, file_name):\n",
    "    try:\n",
    "        loaded_dataset = pd.DataFrame(data_dicts)\n",
    "        loaded_dataset = loaded_dataset[columns]\n",
    "        loaded_dataset['created_utc'] = pd.to_datetime(loaded_dataset['created_utc'], unit='s')\n",
    "        filtered_chunk = loaded_dataset[(loaded_dataset['created_utc'] >= start_date) & (loaded_dataset['created_utc'] <= end_date)]\n",
    "        csv_path = os.path.join(output_dir, f\"{decompressed_data}_submissions_{iteration}.csv\")\n",
    "        filtered_chunk.to_csv(csv_path, index=False)\n",
    "        #\"/home/haters/Downloads/temp_data/loaded_data/Male_Decompressed_Data/submissions/Male_Decompressed_Data_submission_sample_x\"\n",
    "\n",
    "        # for checking the data\n",
    "        csv_path = os.path.join(output_dir, f\"{decompressed_data}_submissions_sample_{iteration}.csv\")\n",
    "        # Convert the data into csv\n",
    "        filtered_chunk.to_csv(csv_path, index=False)\n",
    "    \n",
    "        # Debugging output to monitor rows\n",
    "        print(filtered_chunk.info())\n",
    "    except Exception as e: \n",
    "        print(f\"Failed to process {file_name}: {str(e)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "315a94e9-9bb6-4b9c-af20-d32f626b171c",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# load those dataset from  into the python code\n",
    "data_collection_comments(\"Male_Decompressed_Data\")\n",
    "data_collection_comments(\"Related_Decompressed_Data\")\n",
    "data_collection_comments(\"Female_Decompressed_Data\")\n",
    "data_collection_submissions(\"Male_Decompressed_Data\")\n",
    "data_collection_submissions(\"Female_Decompressed_Data\")\n",
    "data_collection_submissions(\"Related_Decompressed_Data\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "159c4315-cd05-421a-ab31-32bb83e26cd3",
   "metadata": {},
   "source": [
    "## Merging the CSV datasets into a single file\n",
    "Due to the memory issues, we split the loaded files into 4 or 5 parts and then mreged them into a single file"
   ]
  },
  {
   "cell_type": "code",
   "id": "223e9b3f-db0d-489d-ad22-a8eafe5c271d",
   "metadata": {},
   "source": [
    "def merge_datasets(decompressed_data, data_type=\"comments\"):\n",
    "    \n",
    "    base_path = \"/home/haters/Downloads\"\n",
    "    dir_path = os.path.join(base_path, \"loaded_data\", decompressed_data, data_type)\n",
    "    \n",
    "    files = [os.path.join(dir_path, f) for f in os.listdir(dir_path) if f.endswith('.csv')]\n",
    "\n",
    "    dataframes = []\n",
    "    \n",
    "    # Read each file and append the DataFrame to the list\n",
    "    for file in files:\n",
    "        df = pd.read_csv(file)\n",
    "        dataframes.append(df)\n",
    "\n",
    "    # Save gthe merged DataFrame to a new csv file\n",
    "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "    csv_path = os.path.join(dir_path, f\"combined_{decompressed_data}.csv\")\n",
    "    \n",
    "    print(f'csv path::: {csv_path}')\n",
    "    #merged_df.to_csv(output_dir, index=False)\n",
    "    return merged_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d3884e06-bedd-4be8-afec-c7caeb285dc6",
   "metadata": {},
   "source": [
    "merge_co_female = merge_datasets('Female_Decompressed_Data', 'comments')\n",
    "merge_co_male = merge_datasets('Male_Decompressed_Data', 'comments')\n",
    "merge_co_related = merge_datasets('Related_Decompressed_Data', 'comments')\n",
    "merge_sub_female = merge_datasets('Female_Decompressed_Data', 'submissions')\n",
    "merge_sub_male = merge_datasets('Male_Decompressed_Data', 'submissions')\n",
    "merge_sub_related = merge_datasets('Related_Decompressed_Data', 'submissions')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cd92d8b8-31b8-4a5c-8452-e6c6e2102f93",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be2c59a-1630-4e66-9c37-a7dfa03014e7",
   "metadata": {},
   "source": [
    "### Filtering data using the singer's name"
   ]
  },
  {
   "cell_type": "code",
   "id": "1b93c18e-2bd6-43e3-8037-d3b1c85c9246",
   "metadata": {},
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Dictionary with artistname and their nicknames\n",
    "artist_keywords = {\n",
    "    (\"Taylor Swift\", \"T-Swift\", \"TayTay\", \"Taylor\", \"Miss Americana\"): \"Taylor Swift\",(\"SZA\", \"Solana Imani Row\"): \"SZA\",\n",
    "    (\"Miley Cyrus\", \"Miley\", \"Hannah Montana\"): \"Miley Cyrus\",\n",
    "    (\"New Jeans\", \"Minji\", \"Hanni\", \"Danielle New Jeans\", \"Haerin\", \"Hyein\", \"maknaes\"): \"New Jeans\",\n",
    "    (\"Dua Lipa\", \"Dua\", \"Dula Peep\"): \"Dua Lipa\",\n",
    "    (\"Olivia Rodrigo\", \"Liv\"): \"Olivia Rodrigo\",\n",
    "    (\"Ariana Grande\", \"Ari\", \"Ariana\", \"Ms Grande\"): \"Ariana Grande\",\n",
    "    (\"Billie Eilish\", \"Billie\"): \"Billie Eilish\",\n",
    "    (\"Rihanna\", \"RiRi\", \"Badgalriri\"): \"Rihanna\",\n",
    "    (\"Adele\",): \"Adele\",\n",
    "    (\"Bad Bunny\", \"El Conejo Malo\"): \"Bad Bunny\",\n",
    "    (\"The Weeknd\", \"Abel\", \"Abel Tesfaye\"): \"The Weeknd\",\n",
    "    (\"Morgan Wallen\",): \"Morgan Wallen\",\n",
    "    (\"Ed Sheeran\", \"Ginger Jesus\", \"Ed\"): \"Ed Sheeran\",\n",
    "    (\"Drake\", \"Drizzy\", \"Champagne Papi\", \"Aubrey\"): \"Drake\",\n",
    "    (\"Harry Styles\",): \"Harry Styles\",\n",
    "    (\"Feid\",): \"Feid\",\n",
    "    (\"Imagine Dragons\", \"Dan Reynolds\", \"Ben McKee\", \"Daniel Wayne Sermon\", \"Daniel Platzman\"): \"Imagine Dragons\",\n",
    "    (\"Post Malone\", \"Posty\"): \"Post Malone\",\n",
    "    (\"BTS\", \"Bangtan\", \"Bangtan Sonyeondan\", \"Tannies\", \"RM\", \"Jin\", \"Suga\", \"J-Hope\", \"Jimin\", \"V\", \"Jungkook\"): \"BTS\"\n",
    "}\n",
    "\n",
    "# Function for checking each title against the dictionary of artist names\n",
    "def find_artist(title):\n",
    "    for keywords, artist_name in artist_keywords.items():\n",
    "        for keyword in keywords:\n",
    "            # Verwende Regex, um nur ganze Wörter zu matchen\n",
    "            if re.search(r'\\b' + re.escape(keyword) + r'\\b', title, re.IGNORECASE):\n",
    "                return artist_name\n",
    "    return None"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b78b14dd-ef97-4320-9b6c-0c8a133b8202",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# preprocess for female's related subreddits submission\n",
    "related_submission_female = pd.read_csv('/home/haters/Downloads/temp_data/Related_Submissions_Decompressed_Data_female.csv')\n",
    "related_submission_female_clean = related_submission_female.drop_duplicates(subset='permalink', keep='first')\n",
    "# preprocess for male's related subreddits submission\n",
    "related_submission_male = pd.read_csv('/home/haters/Downloads/temp_data/Related_Submissions_Decompressed_Data_male.csv')\n",
    "related_submission_male_clean = related_submission_male.drop_duplicates(subset='permalink', keep='first')\n",
    "\n",
    "print(related_submission_female_clean.head(10))\n",
    "print(related_submission_male_clean.head(10))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "15e79f4c-b521-47a6-9481-3bdbcd356851",
   "metadata": {},
   "source": [
    "### Merging submissions and comments"
   ]
  },
  {
   "cell_type": "code",
   "id": "76d60bcd-c4ff-4318-8f11-7813e069d6ac",
   "metadata": {},
   "source": [
    "def merging_related (submission,gender):\n",
    "    \n",
    "    female_list = [\"Taylor Swift\", \"SZA\", \"Miley Cyrus\", \"New Jeans\", \"Dua Lipa\", \"Olivia Rodrigo\", \"Ariana Grande\", \"Billie Eilish\", \"Rihanna\", \"Adele\"]\n",
    "    male_list = [\"Bad Bunny\", \"The Weeknd\", \"Morgan Wallen\", \"Ed Sheeran\", \"Drake\", \"Harry Styles\", \"Feid\", \"Imagine Dragons\", \"Post Malone\", \"BTS\"]\n",
    "    \n",
    "    submission['artist'] = submission['title'].apply(find_artist)\n",
    "    submission['permalink'] = submission['permalink'].astype(str)\n",
    "    submission['temp_id'] = submission['permalink'].str.split('/').str[4]\n",
    "    submission_ids = submission[\"temp_id\"].unique()\n",
    "    \n",
    "    comment_file_path = \"/home/haters/Downloads/loaded_data/Related_Decompressed_Data/comments\"\n",
    "    file_name_comments = os.listdir(comment_file_path)\n",
    "    all_data = pd.DataFrame()\n",
    "    for file_name in file_name_comments:\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(comment_file_path, file_name)\n",
    "            temp_df = pd.read_csv(file_path)\n",
    "            temp_df[\"link_id\"] = temp_df[\"link_id\"].astype(str)\n",
    "            temp_df[\"temp_id\"] = temp_df[\"link_id\"].str[3:]\n",
    "            temp_df = temp_df[temp_df['temp_id'].isin(submission_ids)]\n",
    "            all_data = pd.concat([all_data, temp_df])\n",
    "    sub_data = submission[[\"temp_id\", \"artist\"]]\n",
    "    merged_dataset = pd.merge(all_data, sub_data, on='temp_id', how='inner')\n",
    "\n",
    "    if gender == 'male':\n",
    "        merged_dataset = merged_dataset[~merged_dataset['artist'].isin(female_list)]\n",
    "    else:\n",
    "        merged_dataset = merged_dataset[~merged_dataset['artist'].isin(male_list)]\n",
    "    return merged_dataset"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b9471e78-0d0d-4f00-9734-48cb4b5e3c75",
   "metadata": {},
   "source": [
    "related_submission_female = merging_related(related_submission_female_clean,'female')\n",
    "related_submission_male = merging_related(related_submission_male_clean,'male')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8f05f45f-906c-46d2-94b2-cf8c9ebc2997",
   "metadata": {},
   "source": [
    "### Convert the dataframe into csv file"
   ]
  },
  {
   "cell_type": "code",
   "id": "a0fd7cc8-338b-4bec-952a-0711998d330e",
   "metadata": {},
   "source": [
    "#Save the new submisisons with the artist\n",
    "\n",
    "# Convert the data into csv file\n",
    "# related_submission_male.to_csv('related_submission_male.csv', index=False)\n",
    "# related_submission_female.to_csv('related_submission_female.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "00374bd5-6fed-48ba-b38d-8d2788469a18",
   "metadata": {},
   "source": [
    "## Checking the final result data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ab2478-98f8-414d-a910-737735eb8923",
   "metadata": {},
   "source": [
    "#### 1. Filtering data using the singer's name"
   ]
  },
  {
   "cell_type": "code",
   "id": "daa13a49-e07b-48f0-8a15-e5cb77a96a2d",
   "metadata": {},
   "source": [
    "## Load data and combine them in Python / Adds a new column singer_name \n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, data_name):\n",
    "        \n",
    "        self.data_name = data_name\n",
    "        self.singer_map = {\n",
    "            'female': {\n",
    "                'taylor': 'Taylor Swift', 'swift': 'Taylor Swift', 'sza': 'SZA', 'miley': 'Miley Cyrus',\n",
    "                'newjeans': 'New Jeans', 'maknaes': 'New Jeans', 'kimminji': 'New Jeans',\n",
    "                'hanni': 'New Jeans', 'mojihye': 'New Jeans', 'dua': 'Dua Lipa',\n",
    "                'olivia': 'Olivia Rodrigo', 'ariana': 'Ariana Grande', 'billie': 'Billie Eilish',\n",
    "                'rihanna': 'Rihanna', 'adele': 'Adele'\n",
    "            },\n",
    "            'male': {\n",
    "                'bunny': 'Bad Bunny', 'weeknd': 'The Weeknd', 'morgan': 'Morgan Wallen',\n",
    "                'sheeran': 'Ed Sheeran', 'drake': 'Drake', 'harry': 'Harry Styles', 'larry': 'Harry Styles',\n",
    "                'feid': 'Feid', 'dragon': 'Imagine Dragons', 'malone': 'Post Malone', 'bts': 'BTS', \n",
    "                'justin': 'Justin Bieber'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    def load_and_combine(self, data_type='comments'):\n",
    "        \"\"\"\n",
    "        Load and combine all CSV files from a specified directory into a single DataFrame.\n",
    "    \n",
    "        \"\"\"\n",
    "        # # for male data files\n",
    "        # file_path = f\"/home/haters/Downloads/loaded_data/{self.data_name}/{data_type}/loaded_file_25Apr\"\n",
    "        file_path = f\"/home/haters/Downloads/loaded_data/{self.data_name}/{data_type}\"\n",
    "        \n",
    "        #print(f'file path:: {file_path}')\n",
    "        files = [os.path.join(file_path, file) for file in os.listdir(file_path) if file.endswith('.csv')]\n",
    "        dataframes = []\n",
    "        \n",
    "        for file in files: \n",
    "            df = pd.read_csv(file)\n",
    "            dataframes.append(df)\n",
    "    \n",
    "        # Generate a list of full paths to all CSV files in the directory\n",
    "        files = [os.path.join(file_path, file) for file in os.listdir(file_path) if file.endswith('.csv')]\n",
    "        # #an empty list to hold the DataFrame objects loaded from each CSV file\n",
    "        # dataframes = []\n",
    "        \n",
    "        # Concatenate all DataFrames in the list into a single DataFrame\n",
    "        combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "        # Data Cleaning\n",
    "        # Remove duplicate rows\n",
    "        combined_df.drop_duplicates(inplace=True)\n",
    "        # Handle missing values\n",
    "        combined_df.dropna(inplace=True)\n",
    "    \n",
    "        if data_type == 'comments':\n",
    "            combined_df = combined_df[(combined_df['body'] != '[deleted]') & (combined_df['body'] != '[removed]')]\n",
    "        elif data_type == 'submissions':\n",
    "            combined_df = combined_df[(combined_df['selftext'] != '[deleted]') & (combined_df['selftext'] != '[removed]')]\n",
    "    \n",
    "        return combined_df\n",
    "\n",
    "\n",
    "    \n",
    "    def add_singer_name(self, df, gender='female'):\n",
    "        \"\"\"\n",
    "        Adds a new column 'artist' to the DataFrame based on 'subreddit' column content.\n",
    "        Assigns singer's name based on keywords found in the 'subreddit' column.\n",
    "        \"\"\"\n",
    "    \n",
    "        if gender in self.singer_map and 'subreddit' in df.columns:\n",
    "            # Apply the mapping using a more robust function that checks string presence\n",
    "            df['artist'] = df['subreddit'].apply(\n",
    "                lambda x: next((name for key, name in self.singer_map[gender].items() if isinstance(x, str) and key in x.lower()), None)\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Invalid gender or 'subreddit' column missing\")\n",
    "    \n",
    "        return df\n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3c42a60e-3d50-40c9-b307-235451702811",
   "metadata": {},
   "source": [
    "**Load Dataset for male**"
   ]
  },
  {
   "cell_type": "code",
   "id": "cd0aa471-1726-4027-89c8-b0af7bdda894",
   "metadata": {},
   "source": [
    "## Load comments data\n",
    "\n",
    "loader = DataLoader('Male_Decompressed_Data')\n",
    "combined_male_comments = loader.load_and_combine('comments')\n",
    "combined_male_comments = loader.add_singer_name(combined_male_comments, 'male')\n",
    "\n",
    "print(f'rows: {combined_male_comments.shape[0]}')\n",
    "combined_male_comments.head(10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "74b2599b-005e-4aa3-9847-a61f9b41c6ca",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Load submissions data\n",
    "loader = DataLoader('Male_Decompressed_Data')\n",
    "combined_male_submissions = loader.load_and_combine('submissions')\n",
    "combined_male_submissions = loader.add_singer_name(combined_male_submissions, 'female')\n",
    "combined_male_submissions"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9097c8de-253e-4b22-9fd2-86f0038b126f",
   "metadata": {},
   "source": [
    "**Load Dataset for female**"
   ]
  },
  {
   "cell_type": "code",
   "id": "f3659b5a-0797-49d0-8c31-c95b6d5ac9fa",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Load comments data\n",
    "\n",
    "loader = DataLoader('Female_Decompressed_Data')\n",
    "combined_female_comments = loader.load_and_combine('comments')\n",
    "combined_female_comments = loader.add_singer_name(combined_female_comments, 'female')\n",
    "combined_female_comments"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "17671eb1-c448-465a-a799-4dfeaa6cd1a0",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Load submissions data\n",
    "loader = DataLoader('Female_Decompressed_Data')\n",
    "combined_female_submissions = loader.load_and_combine('submissions')\n",
    "combined_female_submissions = loader.add_singer_name(combined_female_submissions, 'female')\n",
    "combined_female_submissions"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d0460175-9ffe-4fbf-910d-027c3fc168b2",
   "metadata": {},
   "source": [
    "### Merging files into a single file for each gender: female and male"
   ]
  },
  {
   "cell_type": "code",
   "id": "c515d9ec-62fb-433b-87b1-f3e500ea6fcd",
   "metadata": {},
   "source": [
    "def combine_datasets(csv_path, df, data_type, gender):\n",
    "    \"\"\"\n",
    "    Loads a CSV file into a DataFrame, combines it with an existing DataFrame,\n",
    "    and saves the combined DataFrame to a new CSV file.\n",
    "    \"\"\"\n",
    "    output_path = f\"/home/haters/Downloads/loaded_data/combined_{gender}_{data_type}.csv\"\n",
    "    # Load the CSV file into a DataFrame\n",
    "    new_df = pd.read_csv(csv_path)\n",
    "\n",
    "    new_df.drop_duplicates(inplace=True)\n",
    "        # Handle missing values\n",
    "    new_df.dropna(inplace=True)\n",
    "\n",
    "    if data_type == 'comments':\n",
    "        new_df = new_df[(new_df['body'] != '[deleted]') & (new_df['body'] != '[removed]')]\n",
    "        \n",
    "        txt = \"^^I'm ^^a ^^bot.\"\n",
    "        count_specific_text = combined_male_comments['body'].str.contains(txt, na=False).sum()\n",
    "        \n",
    "    elif data_type == 'submissions':\n",
    "        new_df = new_df[(new_df['selftext'] != '[deleted]') & (new_df['selftext'] != '[removed]')]\n",
    "        \n",
    "    # Combine the existing DataFrame with the new DataFrame\n",
    "    combined_df = pd.concat([df, new_df], ignore_index=True)\n",
    "    # del combined_df['temp_id']\n",
    "    \n",
    "    # Save the combined DataFrame to a CSV file\n",
    "    # combined_df.to_csv(output_path, index=False)    \n",
    "    \n",
    "    return combined_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "96bb125c-9821-4763-bdd7-4b65f19e59b8",
   "metadata": {},
   "source": [
    "#### Submissions data"
   ]
  },
  {
   "cell_type": "code",
   "id": "93955f85-2428-4893-b56b-51fcf8efa1ea",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Combine the data into a single file(male)\n",
    "\n",
    "male_submissions_df = combine_datasets(path_male, combined_male_submissions, 'submissions', 'male')\n",
    "print(male_submissions_df.shape[0])\n",
    "male_submissions_df.head(10)\n",
    "\n",
    "# # Check whether the data has been filtered correctly or not\n",
    "# tempt_df = male_submissions_df[male_submissions_df['artist'].isin(female_list)]\n",
    "# tempt_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7461f483-2802-45d3-ad1a-5e32f57eb010",
   "metadata": {},
   "source": [
    "# Combine the data into a single file(female)\n",
    "\n",
    "path_female = '/home/haters/Downloads/temp_data/related_submission_female.csv'\n",
    "female_submissions_df = pd.read_csv(path_female)\n",
    "\n",
    "# print(female_submissions_df.shape[0])\n",
    "# female_submissions_df.head(10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c8ec0726-2eb5-4f5d-a08a-413e0bb855f7",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Combine the data into a single file\n",
    "print(combined_female_submissions.shape[0])\n",
    "\n",
    "female_submissions_df = combine_datasets(path_female, combined_female_submissions, 'submissions', 'female')\n",
    "missing_artists = female_submissions_df[female_submissions_df['artist'].isna()]\n",
    "missing_artists.head() # no issues\n",
    "\n",
    "print(female_submissions_df.shape[0])\n",
    "female_submissions_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3a5d1265-6d0c-4b09-8036-04c3a64fc45c",
   "metadata": {},
   "source": [
    "#### Comments Data### check the final file of male submission\n",
    "\n",
    "## Male Dataset\n",
    "path_male = '/home/haters/Downloads/related_submission_male.csv'\n",
    "# 'Downloads/related_submission_male.csv'\n",
    "\n",
    "#male_submissions_df = combine_datasets(file_path, combined_male_submissions, 'submissions', 'male')\n",
    "male_submissions_df = pd.read_csv(path_male)\n",
    "print(male_submissions_df.shape[0])\n",
    "female_singers = set(singer_map['female'].values())\n",
    "tempt_df = male_submissions_df[male_submissions_df['artist'].isin(female_singers)]\n",
    "#print(tempt_df)\n",
    "print(tempt_df.shape[0])\n",
    "print(tempt_df['artist'].unique())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4ef10549-42b8-42da-b2c6-dcd42d206f32",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### check the final file of male submission\n",
    "\n",
    "## Male Dataset\n",
    "path_male = '/home/haters/Downloads/related_submission_male.csv'\n",
    "# 'Downloads/related_submission_male.csv'\n",
    "\n",
    "#male_submissions_df = combine_datasets(file_path, combined_male_submissions, 'submissions', 'male')\n",
    "male_submissions_df = pd.read_csv(path_male)\n",
    "print(male_submissions_df.shape[0])\n",
    "female_singers = set(singer_map['female'].values())\n",
    "tempt_df = male_submissions_df[male_submissions_df['artist'].isin(female_singers)]\n",
    "#print(tempt_df)\n",
    "print(tempt_df.shape[0])\n",
    "print(tempt_df['artist'].unique())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "47a0fbcc-26c0-4b26-97fa-0099e7b0c177",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "file_path = '/home/haters/Downloads/temp_data/related_submission_male.csv'\n",
    "# 'Downloads/related_submission_male.csv'\n",
    "\n",
    "male_submissions_df = combine_datasets(file_path, combined_male_submissions, 'submissions', 'male')\n",
    "male_submissions_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "077fd250-c5c4-4ac6-988f-573d19bcb062",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "file_path = '/home/haters/Downloads/temp_data/related_submission_female.csv'\n",
    "# Downloads/combine_female_related_subreddit.csv\n",
    "# 'Downloads/related_submission_male.csv'\n",
    "\n",
    "# female_submissions_df = combine_datasets(file_path, combined_female_submissions, 'submissions', 'female')\n",
    "# print(female_submissions_df.shape[0])\n",
    "\n",
    "# 34742\n",
    "\n",
    "\n",
    "path_female = '/home/haters/Downloads/loaded_data/combined_female_submissions.csv'\n",
    "\n",
    "# 'Downloads/related_submission_male.csv'\n",
    "\n",
    "#male_submissions_df = combine_datasets(file_path, combined_male_submissions, 'submissions', 'male')\n",
    "female_submissions_df = pd.read_csv(path_female)\n",
    "print(female_submissions_df.shape[0])\n",
    "female_singers = set(singer_map['male'].values())\n",
    "tempt_df = female_submissions_df[female_submissions_df['artist'].isin(male_singers)]\n",
    "#print(tempt_df)\n",
    "print(tempt_df.shape[0])\n",
    "print(tempt_df['artist'].unique())\n",
    "\n",
    "missing_artists = female_submissions_df[female_submissions_df['artist'].isna()]\n",
    "print(missing_artists.head())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "415918ee-476f-4075-8ddd-c4a897617f79",
   "metadata": {},
   "source": [
    "#### Comments Data"
   ]
  },
  {
   "cell_type": "code",
   "id": "950b1c70-e6c8-4901-bb41-740967c02fb7",
   "metadata": {},
   "source": [
    "file_path = '/home/haters/Downloads/combine_male_related_subreddit.csv'\n",
    "# 'Downloads/related_submission_male.csv'\n",
    "print(combined_male_comments.shape[0])\n",
    "male_comments_df = combine_datasets(file_path, combined_male_comments, 'comments', 'male')\n",
    "male_comments_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9015954a-34cf-4c9e-a8ac-70fbca722347",
   "metadata": {},
   "source": [
    "file_path = '/home/haters/Downloads/combine_female_related_subreddit.csv'\n",
    "# 'Downloads/related_submission_male.csv'\n",
    "print(combined_female_comments.shape[0])\n",
    "\n",
    "female_comments_df = combine_datasets(file_path, combined_female_comments, 'comments', 'female')\n",
    "female_comments_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3d70d895-c9d2-4141-9618-3245bd4681e7",
   "metadata": {},
   "source": [
    "import pandas as pd \n",
    "\n",
    "file_path = \"/home/haters/Downloads/loaded_data/Combined_data_29Apr/Sexism_Detection/combined_female_comments.csv\"\n",
    "df = pd.read_csv(file_path, low_memory=True)\n",
    "subreddit_uniq_by_artist = df.groupby('artist')['subreddit'].nunique()\n",
    "\n",
    "print(subreddit_uniq_by_artist)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "436a2924-443f-4bfa-b518-54ea585dfeb9",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "id": "c8f064fb-f19c-412b-b43f-600f1bf2e32c",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_separate_distributions(male_comments_path, female_comments_path, male_submissions_path, female_submissions_path, figsize=(20, 12)):\n",
    "    \"\"\"\n",
    "    Plots separate distributions of comments and submissions from male and female datasets.\n",
    "    \"\"\"\n",
    "    # Load the datasets\n",
    "    male_comments = pd.read_csv(male_comments_path, low_memory=True)\n",
    "    female_comments = pd.read_csv(female_comments_path, low_memory=True)\n",
    "    male_submissions = pd.read_csv(male_submissions_path, low_memory=True)\n",
    "    female_submissions = pd.read_csv(female_submissions_path, low_memory=True)\n",
    "\n",
    "    # Def for proceDefine a helper to process data\n",
    "    def process_data(df, gender, dtype):\n",
    "        artist_dist = df.groupby('artist').size().reset_index(name='count')\n",
    "        subreddit_dist = df.groupby('artist')['subreddit'].nunique().reset_index(name='unique_subreddits')\n",
    "        artist_dist['gender'] = gender\n",
    "        artist_dist['type'] = dtype\n",
    "        subreddit_dist['gender'] = gender\n",
    "        subreddit_dist['type'] = dtype\n",
    "        return artist_dist, subreddit_dist\n",
    "\n",
    "    # Process each dataset\n",
    "    male_comments_artist, male_comments_subreddit = process_data(male_comments, 'Male', 'Comments')\n",
    "    female_comments_artist, female_comments_subreddit = process_data(female_comments, 'Female', 'Comments')\n",
    "    male_submissions_artist, male_submissions_subreddit = process_data(male_submissions, 'Male', 'Submissions')\n",
    "    female_submissions_artist, female_submissions_subreddit = process_data(female_submissions, 'Female', 'Submissions')\n",
    "\n",
    "    # Set up a figure with four subplots\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=figsize)\n",
    "\n",
    "    # Plot Comments Distribution\n",
    "    sns.barplot(x='artist', y='count', hue='gender', data=pd.concat([male_comments_artist, female_comments_artist]), ax=ax1)\n",
    "    ax1.set_title('Comments Distribution by Artist')\n",
    "    ax1.set_ylabel('Count')\n",
    "    ax1.set_xlabel('Artist')\n",
    "    ax1.legend(title='Gender')\n",
    "\n",
    "    # Plot Submissions Distribution\n",
    "    sns.barplot(x='artist', y='count', hue='gender', data=pd.concat([male_submissions_artist, female_submissions_artist]), ax=ax2)\n",
    "    ax2.set_title('Submissions Distribution by Artist')\n",
    "    ax2.set_ylabel('Count')\n",
    "    ax2.set_xlabel('Artist')\n",
    "    ax2.legend(title='Gender')\n",
    "\n",
    "    # Plot Subreddit Distribution for Comments\n",
    "    sns.barplot(x='artist', y='unique_subreddits', hue='gender', data=pd.concat([male_comments_subreddit, female_comments_subreddit]), ax=ax3)\n",
    "    ax3.set_title('Number of Subreddits per Artist in Comments')\n",
    "    ax3.set_ylabel('Number of Unique Subreddits')\n",
    "    ax3.set_xlabel('Artist')\n",
    "    ax3.legend(title='Gender')\n",
    "\n",
    "    # Plot Subreddit Distribution for Submissions\n",
    "    sns.barplot(x='artist', y='unique_subreddits', hue='gender', data=pd.concat([male_submissions_subreddit, female_submissions_subreddit]), ax=ax4)\n",
    "    ax4.set_title('Number of Subreddits per Artist in Submissions')\n",
    "    ax4.set_ylabel('Number of Unique Subreddits')\n",
    "    ax4.set_xlabel('Artist')\n",
    "    ax4.legend(title='Gender')\n",
    "\n",
    "    # Improve layout and readability\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
    "    plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45)\n",
    "    plt.setp(ax3.xaxis.get_majorticklabels(), rotation=45)\n",
    "    plt.setp(ax4.xaxis.get_majorticklabels(), rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "72a8052f-57b7-4f11-bbad-1e95b7b624e5",
   "metadata": {},
   "source": [
    "female_comments_path = \"/home/haters/Downloads/loaded_data/Combined_data_29Apr/Sexism_Detection/combined_female_comments.csv\"\n",
    "male_comments_path = \"/home/haters/Downloads/loaded_data/Combined_data_29Apr/Sexism_Detection/combined_male_comments.csv\"\n",
    "male_submissions_path = \"/home/haters/Downloads/loaded_data/Combined_data_29Apr/Sexism_Detection/combined_male_submissions.csv\"\n",
    "female_submissions_path = \"/home/haters/Downloads/loaded_data/Combined_data_29Apr/Sexism_Detection/combined_female_submissions.csv\"\n",
    "# plot_artist_and_subreddit_distribution('comments', male_path, female_path, figsize=(20, 10))\n",
    "\n",
    "plot_separate_distributions(male_comments_path, female_comments_path, male_submissions_path, female_submissions_path, figsize=(20, 12))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "90ac619e-bf96-4e2a-877d-c74a8acf9a6d",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def count_comments(filename):\n",
    "    \"\"\"Read the CSV file and count the number of rows.\"\"\"\n",
    "    data = pd.read_csv(filename, header=None)\n",
    "    return len(data)\n",
    "\n",
    "# Paths to the files\n",
    "path_male = '/home/haters/Downloads/loaded_data/Combined_data_29Apr/combined_male_comments.csv'\n",
    "path_female = \"/home/haters/Downloads/loaded_data/Combined_data_29Apr/combined_female_comments.csv\"\n",
    "\n",
    "# Count the comments in each file\n",
    "count_male = count_comments(path_male)\n",
    "count_female = count_comments(path_female)\n",
    "\n",
    "# Data for visualization\n",
    "genders = ['Male', 'Female']\n",
    "comments = [count_male, count_female]\n",
    "\n",
    "# Create a bar chart\n",
    "fig, ax = plt.subplots(figsize=(9, 4))\n",
    "bars = ax.bar(genders, comments, color=['lightblue', 'orange'])\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Number of Comments')\n",
    "plt.title('Number of Comments by Gender')\n",
    "\n",
    "# Adjust the Y-axis ticks\n",
    "plt.yticks(range(0, max(comments) + 1, max(comments) // 10))\n",
    "\n",
    "# Adjust the layout to avoid any overlap\n",
    "plt.subplots_adjust(top=1.9)\n",
    "\n",
    "# Label each bar with the number of comments\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    ax.annotate(f'{yval}',\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, yval),\n",
    "                xytext=(0, 3),  # 3 points vertical offset\n",
    "                textcoords=\"offset points\",\n",
    "                ha='center', va='bottom')\n",
    "plt.savefig(\"genderVis.png\")\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f43d9e78-7b0d-4cfe-9e7b-a41023808931",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def count_rows(filename):\n",
    "    \"\"\"Read the CSV file and count the number of rows.\"\"\"\n",
    "    data = pd.read_csv(filename, low_memory=False)\n",
    "    return len(data)\n",
    "\n",
    "def plot_gender_distribution(comments_male_path, comments_female_path, submissions_male_path, submissions_female_path):\n",
    "    \"\"\"Plot the distribution of comments and submissions for male and female datasets.\"\"\"\n",
    "    # Count the rows in each file\n",
    "    count_male_comments = count_rows(comments_male_path)\n",
    "    count_female_comments = count_rows(comments_female_path)\n",
    "    count_male_submissions = count_rows(submissions_male_path)\n",
    "    count_female_submissions = count_rows(submissions_female_path)\n",
    "\n",
    "    # Data for visualization\n",
    "    genders = ['Male', 'Female']\n",
    "    comments = [count_male_comments, count_female_comments]\n",
    "    submissions = [count_male_submissions, count_female_submissions]\n",
    "    colors = ['lightblue', 'orange']  # Consistent color coding for male and female\n",
    "\n",
    "    # Set up a figure with two subplots arranged vertically\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 10), sharex=True)\n",
    "\n",
    "    # Plot Comments Distribution\n",
    "    ax1.bar(genders, comments, color=colors)\n",
    "    ax1.set_title('Number of Comments by Gender')\n",
    "    ax1.set_ylabel('Number of Comments')\n",
    "    for i, value in enumerate(comments):\n",
    "        ax1.text(i, value + 3, str(value), ha='center', va='bottom')\n",
    "\n",
    "    # Plot Submissions Distribution\n",
    "    ax2.bar(genders, submissions, color=colors)\n",
    "    ax2.set_title('Number of Submissions by Gender')\n",
    "    ax2.set_ylabel('Number of Submissions')\n",
    "    ax2.set_xlabel('Gender')\n",
    "    for i, value in enumerate(submissions):\n",
    "        ax2.text(i, value + 3, str(value), ha='center', va='bottom')\n",
    "\n",
    "    plt.xticks(rotation=0)  # Set rotation for x ticks if needed\n",
    "    plt.tight_layout()  # Adjust layout to fit everything\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_gender_distribution(\n",
    "    '/home/haters/Downloads/loaded_data/Combined_data_29Apr/combined_male_comments.csv',\n",
    "    '/home/haters/Downloads/loaded_data/Combined_data_29Apr/combined_female_comments.csv',\n",
    "    '/home/haters/Downloads/loaded_data/Combined_data_29Apr/combined_male_submissions.csv',\n",
    "    '/home/haters/Downloads/loaded_data/Combined_data_29Apr/combined_female_submissions.csv'\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a089af09-dfb4-4cac-a31a-6f7c4675b14e",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "id": "8324c1b5-ee91-4b41-a180-aa39e89cdc11",
   "metadata": {},
   "source": [
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import WordNetLemmatizer"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "22756b78-3a97-4b30-8dac-711fff2f2bb8",
   "metadata": {},
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dee92dea-552b-4936-a3fc-e402c68eecae",
   "metadata": {},
   "source": [
    "# Define Text Preprocessor class\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.tokenizer = WordPunctTokenizer()\n",
    "        self.singer_names = [\n",
    "            \"Bad Bunny\", \"El Conejo Malo\", \"The Weeknd\", \"Abel\", \"Abel Tesfaye\",\n",
    "            \"Morgan Wallen\", \"Ed Sheeran\", \"Ginger Jesus\", \"Ed\", \"Drake\", \n",
    "            \"Drizzy\", \"Champagne Papi\", \"Aubrey\", \"Harry Styles\", \"Feid\", \n",
    "            \"Imagine Dragons\", \"Dan Reynolds\", \"Ben McKee\", \"Daniel Wayne Sermon\", \n",
    "            \"Daniel Platz Platzman\", \"Post Malone\", \"Posty\", \"BTS\", \"Bangtan\", \n",
    "            \"Bangtan Sonyeondan\", \"Tannies\", \"RM\", \"Jin\", \"Suga\", \"J-Hope\", \n",
    "            \"Jimin\", \"V\", \"Jungkook\", \"harry\", \"justin bieber\", \"bieber\", \"justin\", \"namjoon\", \n",
    "            \"Taylor Swift\" , \"T-Swift\" , \"TayTay\" , \"Taylor\" , \"Miss Americana\", \"SZA\", \"Solana Imani Row\", \n",
    "            \"Miley Cyrus\", \"Miley\", \"Hannah Montana\", \"New Jeans\", \"Minji\", \"Hanni\", \"Danielle New Jeans\", \"Haerin\", \"Hyein\", \n",
    "            \"Dua Lipa\", \"Dua\", \"Dula Peep\", \"Olivia Rodrigo\", \"Liv\", \"Ariana Grande\", \"Ari\", \"Ariana\", \"Ms Grande\", \n",
    "            \"Billie Eilish\", \"Billie\", \"Rihanna\", \"RiRi\", \"Badgalriri\", \"Adele\", \"swiftie\", \"Oliia\", \"newjean\", \n",
    "            \"rodrigo\", \"rodrigos\"\n",
    "        ]\n",
    "        self.singer_names = [name.lower() for name in self.singer_names]\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = self.remove_singer_names(text)\n",
    "        text = emoji.demojize(text)\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        tokens = [word for word in tokens if word not in self.stop_words and word.isalnum()]\n",
    "        tokens = [self.lemmatize_token(word) for word in tokens]\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    def remove_singer_names(self, text):\n",
    "        for name in self.singer_names:\n",
    "            text = text.replace(name, '')\n",
    "        \n",
    "        text = text.replace('austin', 'album')\n",
    "        text = text.replace('wts', 'want to sell')\n",
    "        text = text.replace('merch', 'merchandise')\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def lemmatize_token(self, token):\n",
    "        tag = self.get_wordnet_pos(nltk.pos_tag([token])[0][1])\n",
    "        return self.lemmatizer.lemmatize(token, pos=tag) if tag else token\n",
    "\n",
    "    @staticmethod\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        # Converts treebank tag to wordnet tag\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return None"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "male_submissions = pd.read_csv(\"/home/haters/Downloads/Toxicity_Detection/output_perspective/output_score/male_submissions_outcome_final.csv\")\n",
    "female_submissions = pd.read_csv(\"/home/haters/Downloads/Toxicity_Detection/output_perspective/output_score/female_submissions_outcome_final.csv\")\n",
    "\n",
    "# Combine 'title' and 'selftext' into a new column 'combined_text'\n",
    "male_submissions['combined_text'] = male_submissions['title'] + \" \" + male_submissions['selftext']\n",
    "female_submissions['combined_text'] = female_submissions['title'] + \" \" + female_submissions['selftext']"
   ],
   "id": "73f5a8dad8ea6e29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def remove_hyperlinks(text):\n",
    "    return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags = re.MULTILINE)\n",
    "\n",
    "male_submissions[\"combined_text\"] = male_submissions[\"combined_text\"].apply(remove_hyperlinks)\n",
    "female_submissions[\"combined_text\"] = female_submissions[\"combined_text\"].apply(remove_hyperlinks)"
   ],
   "id": "49b47e105a731b47",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "558bde47-1818-41e8-83a1-a3d41b859e35",
   "metadata": {},
   "source": [
    "def preprocess(df, data_type):\n",
    "    if data_type == 'comments':\n",
    "        text = 'body'\n",
    "    elif data_type == 'submissions':\n",
    "        text = 'combined_text'\n",
    "    col_index = df.columns.get_loc(text) + 1\n",
    "    preprocessed_text = df[text].apply(preprocessor.preprocess_text)\n",
    "    df.insert(col_index, 'preprocessed_txt', preprocessed_text)\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cbb859c7-436d-470c-9c73-5aa304011668",
   "metadata": {},
   "source": [
    "preprocessor = TextPreprocessor()\n",
    "combined_male_submissions = preprocess(male_submissions, 'submissions')\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "combined_female_submissions = preprocess(female_submissions, 'submissions')"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
