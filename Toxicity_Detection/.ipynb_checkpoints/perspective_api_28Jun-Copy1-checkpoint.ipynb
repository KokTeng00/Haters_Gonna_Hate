{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebf96794-7a70-41a0-be65-97e05fca1088",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9284/4010494547.py:9: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  female_com_df = pd.read_csv(female_comments_path)\n",
      "/tmp/ipykernel_9284/4010494547.py:10: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  male_com_df = pd.read_csv(male_comments_path)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths \n",
    "female_comments_path = \"/home/haters/Downloads/loaded_data/Combined_data_29Apr/combined_female_comments.csv\"\n",
    "male_comments_path = \"/home/haters/Downloads/loaded_data/Combined_data_29Apr/combined_male_comments.csv\"\n",
    "female_submissions_path = \"/home/haters/Downloads/loaded_data/Combined_data_29Apr/combined_female_submissions.csv\"\n",
    "male_submissions_path = \"/home/haters/Downloads/loaded_data/Combined_data_29Apr/combined_male_submissions.csv\"\n",
    "\n",
    "female_com_df = pd.read_csv(female_comments_path)\n",
    "male_com_df = pd.read_csv(male_comments_path)\n",
    "female_sub_df = pd.read_csv(female_submissions_path)\n",
    "male_sub_df = pd.read_csv(male_submissions_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1aacf56-9d6a-4601-be83-375c15c5d30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'temp_id' in female_com_df.columns:\n",
    "    female_com_df = female_com_df.drop(columns=['temp_id'])\n",
    "\n",
    "if 'temp_id' in male_com_df.columns:\n",
    "    male_com_df = male_com_df.drop(columns=['temp_id'])\n",
    "\n",
    "if 'temp_id' in female_sub_df.columns:\n",
    "    female_sub_df = female_sub_df.drop(columns=['temp_id'])\n",
    "\n",
    "if 'temp_id' in male_sub_df.columns:\n",
    "    male_sub_df = male_sub_df.drop(columns=['temp_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aae4c7f-a5b9-42e4-874e-f71e48316385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "458946\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>link_id</th>\n",
       "      <th>retrieved_on</th>\n",
       "      <th>artist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This song is fucking magic live, when the crow...</td>\n",
       "      <td>BadBunnyPR</td>\n",
       "      <td>t3_za73lr</td>\n",
       "      <td>2023-01-08 06:20:57</td>\n",
       "      <td>Bad Bunny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WOW Top 0.05%!! My gf was 0.1% and I thought T...</td>\n",
       "      <td>BadBunnyPR</td>\n",
       "      <td>t3_z9wmhw</td>\n",
       "      <td>2023-01-08 06:19:27</td>\n",
       "      <td>Bad Bunny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dude same omg  \\nI cant find the right mix any...</td>\n",
       "      <td>BadBunnyPR</td>\n",
       "      <td>t3_xm3rmh</td>\n",
       "      <td>2023-01-08 06:18:20</td>\n",
       "      <td>Bad Bunny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I’m proud to say that I’m top 0.01 percent</td>\n",
       "      <td>BadBunnyPR</td>\n",
       "      <td>t3_z9wmhw</td>\n",
       "      <td>2023-01-08 06:17:58</td>\n",
       "      <td>Bad Bunny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I had that in 2019</td>\n",
       "      <td>BadBunnyPR</td>\n",
       "      <td>t3_zad5e3</td>\n",
       "      <td>2023-01-08 06:16:46</td>\n",
       "      <td>Bad Bunny</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body   subreddit    link_id  \\\n",
       "0  This song is fucking magic live, when the crow...  BadBunnyPR  t3_za73lr   \n",
       "1  WOW Top 0.05%!! My gf was 0.1% and I thought T...  BadBunnyPR  t3_z9wmhw   \n",
       "2  Dude same omg  \\nI cant find the right mix any...  BadBunnyPR  t3_xm3rmh   \n",
       "3         I’m proud to say that I’m top 0.01 percent  BadBunnyPR  t3_z9wmhw   \n",
       "4                                 I had that in 2019  BadBunnyPR  t3_zad5e3   \n",
       "\n",
       "          retrieved_on     artist  \n",
       "0  2023-01-08 06:20:57  Bad Bunny  \n",
       "1  2023-01-08 06:19:27  Bad Bunny  \n",
       "2  2023-01-08 06:18:20  Bad Bunny  \n",
       "3  2023-01-08 06:17:58  Bad Bunny  \n",
       "4  2023-01-08 06:16:46  Bad Bunny  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(male_com_df.shape[0])\n",
    "male_com_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "460a9c7f-851b-406c-9d4f-fa1b278598a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Split the dataset into 3\n",
    "df_split = np.array_split(male_com_df, 3)\n",
    "\n",
    "split_dfs = {}\n",
    "for i, split in enumerate(df_split):\n",
    "     split_dfs[i] = split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce3b1aad-6562-48b1-8867-71aca287443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_com_df_1 = split_dfs[0]\n",
    "male_com_df_2 = split_dfs[1]\n",
    "male_com_df_3 = split_dfs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e508c0b0-6c06-4f77-a211-5ac3ae9cb301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152982\n",
      "152982\n",
      "152982\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>link_id</th>\n",
       "      <th>retrieved_on</th>\n",
       "      <th>artist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This song is fucking magic live, when the crow...</td>\n",
       "      <td>BadBunnyPR</td>\n",
       "      <td>t3_za73lr</td>\n",
       "      <td>2023-01-08 06:20:57</td>\n",
       "      <td>Bad Bunny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WOW Top 0.05%!! My gf was 0.1% and I thought T...</td>\n",
       "      <td>BadBunnyPR</td>\n",
       "      <td>t3_z9wmhw</td>\n",
       "      <td>2023-01-08 06:19:27</td>\n",
       "      <td>Bad Bunny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dude same omg  \\nI cant find the right mix any...</td>\n",
       "      <td>BadBunnyPR</td>\n",
       "      <td>t3_xm3rmh</td>\n",
       "      <td>2023-01-08 06:18:20</td>\n",
       "      <td>Bad Bunny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I’m proud to say that I’m top 0.01 percent</td>\n",
       "      <td>BadBunnyPR</td>\n",
       "      <td>t3_z9wmhw</td>\n",
       "      <td>2023-01-08 06:17:58</td>\n",
       "      <td>Bad Bunny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I had that in 2019</td>\n",
       "      <td>BadBunnyPR</td>\n",
       "      <td>t3_zad5e3</td>\n",
       "      <td>2023-01-08 06:16:46</td>\n",
       "      <td>Bad Bunny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152977</th>\n",
       "      <td>Well that is pretty cool though! I’d be hype a...</td>\n",
       "      <td>EdSheeran</td>\n",
       "      <td>t3_14hb70j</td>\n",
       "      <td>2023-06-24 05:30:39</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152978</th>\n",
       "      <td>I had no idea who he was and he sounded like h...</td>\n",
       "      <td>EdSheeran</td>\n",
       "      <td>t3_14hb70j</td>\n",
       "      <td>2023-06-24 05:30:46</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152979</th>\n",
       "      <td>SAMEEEE. gimme 3 hours of Ed please!!!! 🙏🏻</td>\n",
       "      <td>EdSheeran</td>\n",
       "      <td>t3_14hb70j</td>\n",
       "      <td>2023-06-24 05:31:12</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152980</th>\n",
       "      <td>Addd me to the discord</td>\n",
       "      <td>EdSheeran</td>\n",
       "      <td>t3_1493ler</td>\n",
       "      <td>2023-06-24 05:32:16</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152981</th>\n",
       "      <td>can you please add me too. my username:   小林#0400</td>\n",
       "      <td>EdSheeran</td>\n",
       "      <td>t3_1493ler</td>\n",
       "      <td>2023-06-24 05:32:17</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>152982 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     body   subreddit  \\\n",
       "0       This song is fucking magic live, when the crow...  BadBunnyPR   \n",
       "1       WOW Top 0.05%!! My gf was 0.1% and I thought T...  BadBunnyPR   \n",
       "2       Dude same omg  \\nI cant find the right mix any...  BadBunnyPR   \n",
       "3              I’m proud to say that I’m top 0.01 percent  BadBunnyPR   \n",
       "4                                      I had that in 2019  BadBunnyPR   \n",
       "...                                                   ...         ...   \n",
       "152977  Well that is pretty cool though! I’d be hype a...   EdSheeran   \n",
       "152978  I had no idea who he was and he sounded like h...   EdSheeran   \n",
       "152979         SAMEEEE. gimme 3 hours of Ed please!!!! 🙏🏻   EdSheeran   \n",
       "152980                             Addd me to the discord   EdSheeran   \n",
       "152981  can you please add me too. my username:   小林#0400   EdSheeran   \n",
       "\n",
       "           link_id         retrieved_on      artist  \n",
       "0        t3_za73lr  2023-01-08 06:20:57   Bad Bunny  \n",
       "1        t3_z9wmhw  2023-01-08 06:19:27   Bad Bunny  \n",
       "2        t3_xm3rmh  2023-01-08 06:18:20   Bad Bunny  \n",
       "3        t3_z9wmhw  2023-01-08 06:17:58   Bad Bunny  \n",
       "4        t3_zad5e3  2023-01-08 06:16:46   Bad Bunny  \n",
       "...            ...                  ...         ...  \n",
       "152977  t3_14hb70j  2023-06-24 05:30:39  Ed Sheeran  \n",
       "152978  t3_14hb70j  2023-06-24 05:30:46  Ed Sheeran  \n",
       "152979  t3_14hb70j  2023-06-24 05:31:12  Ed Sheeran  \n",
       "152980  t3_1493ler  2023-06-24 05:32:16  Ed Sheeran  \n",
       "152981  t3_1493ler  2023-06-24 05:32:17  Ed Sheeran  \n",
       "\n",
       "[152982 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(male_com_df_1.shape[0])\n",
    "print(male_com_df_2.shape[0])\n",
    "print(male_com_df_3.shape[0])\n",
    "male_com_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d2eb329-0c4f-4d70-b2c8-96152c10f39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from googleapiclient import discovery\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import emoji\n",
    "from tqdm import tqdm\n",
    "\n",
    "API_KEY_1 = 'AIzaSyA_uZndSn69dCshlHBt01IZRmmL6GV00eM'\n",
    "API_KEY_2 = 'AIzaSyA_uZndSn69dCshlHBt01IZRmmL6GV00eM'\n",
    "API_KEY_3 = 'AIzaSyA_uZndSn69dCshlHBt01IZRmmL6GV00eM'\n",
    "\n",
    "client_1 = discovery.build(\n",
    "    \"commentanalyzer\",\n",
    "    \"v1alpha1\",\n",
    "    developerKey=API_KEY_1,\n",
    "    discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "    static_discovery=False,\n",
    ")\n",
    "\n",
    "client_2 = discovery.build(\n",
    "    \"commentanalyzer\",\n",
    "    \"v1alpha1\",\n",
    "    developerKey=API_KEY_2,\n",
    "    discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "    static_discovery=False,\n",
    ")\n",
    "\n",
    "client_3 = discovery.build(\n",
    "    \"commentanalyzer\",\n",
    "    \"v1alpha1\",\n",
    "    developerKey=API_KEY_3,\n",
    "    discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "    static_discovery=False,\n",
    ")\n",
    "\n",
    "# Basic text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove special characters\n",
    "    text = emoji.replace_emoji(text, replace='')  # Remove emojis\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d81d7c-5f07-47b3-b268-31237e41135e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze text using Perspective API with timeout handling(the result is much better) \n",
    "def get_toxicity_score(text, sleep_time=1, retry_limit=3):\n",
    "    preprocessed_text = preprocess_text(text)\n",
    "    analyze_request = {\n",
    "        'comment': {'text': preprocessed_text},\n",
    "        'requestedAttributes': {'TOXICITY': {}}\n",
    "    }\n",
    "    retries = 0\n",
    "    while retries < retry_limit:\n",
    "        try:\n",
    "            response = client.comments().analyze(body=analyze_request).execute()\n",
    "            score = response['attributeScores']['TOXICITY']['summaryScore']['value']\n",
    "            return score\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing text: {e}. Retrying ({retries + 1}/{retry_limit})...\")\n",
    "            retries += 1\n",
    "            time.sleep(sleep_time)\n",
    "    print(f\"Skipping text after {retry_limit} retries: {preprocessed_text[:30]}...\")  # Log the problematic text\n",
    "    return None\n",
    "\n",
    "# Function to process DataFrame with rate limiting and timeout handling\n",
    "def process_dataframe(df):\n",
    "    scores = []\n",
    "    for i, text in enumerate(tqdm(df['body'], desc=\"Processing rows\")):\n",
    "        score = get_toxicity_score(text)\n",
    "        scores.append(score)\n",
    "        if (i + 1) % 50 == 0:  # Adjust the batch size as needed\n",
    "            time.sleep(60)  # Sleep for 60 seconds after every 50 requests\n",
    "    df['toxicity_score'] = scores\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe586d22-3540-4f6d-8ed6-bc2bbe0d4989",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:   0%|                    | 20/152982 [00:03<7:33:42,  5.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error analyzing text: <HttpError 400 when requesting https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze?key=AIzaSyA_uZndSn69dCshlHBt01IZRmmL6GV00eM&alt=json returned \"Comment must be non-empty.\". Details: \"[{'@type': 'type.googleapis.com/google.commentanalyzer.v1alpha1.Error', 'errorType': 'COMMENT_EMPTY'}]\">. Retrying (1/3)...\n",
      "Error analyzing text: <HttpError 400 when requesting https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze?key=AIzaSyA_uZndSn69dCshlHBt01IZRmmL6GV00eM&alt=json returned \"Comment must be non-empty.\". Details: \"[{'@type': 'type.googleapis.com/google.commentanalyzer.v1alpha1.Error', 'errorType': 'COMMENT_EMPTY'}]\">. Retrying (2/3)...\n",
      "Error analyzing text: <HttpError 400 when requesting https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze?key=AIzaSyA_uZndSn69dCshlHBt01IZRmmL6GV00eM&alt=json returned \"Comment must be non-empty.\". Details: \"[{'@type': 'type.googleapis.com/google.commentanalyzer.v1alpha1.Error', 'errorType': 'COMMENT_EMPTY'}]\">. Retrying (3/3)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:   0%|                   | 22/152982 [00:06<33:00:44,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping text after 3 retries: ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:   0%|                   | 28/152982 [00:07<10:11:57,  4.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error analyzing text: <HttpError 400 when requesting https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze?key=AIzaSyA_uZndSn69dCshlHBt01IZRmmL6GV00eM&alt=json returned \"Attribute TOXICITY does not support request languages: ms\". Details: \"[{'@type': 'type.googleapis.com/google.commentanalyzer.v1alpha1.Error', 'errorType': 'LANGUAGE_NOT_SUPPORTED_BY_ATTRIBUTE', 'languageNotSupportedByAttributeError': {'detectedLanguages': ['ms'], 'attribute': 'TOXICITY'}}]\">. Retrying (1/3)...\n",
      "Error analyzing text: <HttpError 400 when requesting https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze?key=AIzaSyA_uZndSn69dCshlHBt01IZRmmL6GV00eM&alt=json returned \"Attribute TOXICITY does not support request languages: ms\". Details: \"[{'@type': 'type.googleapis.com/google.commentanalyzer.v1alpha1.Error', 'errorType': 'LANGUAGE_NOT_SUPPORTED_BY_ATTRIBUTE', 'languageNotSupportedByAttributeError': {'detectedLanguages': ['ms'], 'attribute': 'TOXICITY'}}]\">. Retrying (2/3)...\n",
      "Error analyzing text: <HttpError 400 when requesting https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze?key=AIzaSyA_uZndSn69dCshlHBt01IZRmmL6GV00eM&alt=json returned \"Attribute TOXICITY does not support request languages: ms\". Details: \"[{'@type': 'type.googleapis.com/google.commentanalyzer.v1alpha1.Error', 'errorType': 'LANGUAGE_NOT_SUPPORTED_BY_ATTRIBUTE', 'languageNotSupportedByAttributeError': {'detectedLanguages': ['ms'], 'attribute': 'TOXICITY'}}]\">. Retrying (3/3)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:   0%|                   | 30/152982 [00:11<34:35:33,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping text after 3 retries: hahaaa...\n",
      "Error analyzing text: <HttpError 400 when requesting https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze?key=AIzaSyA_uZndSn69dCshlHBt01IZRmmL6GV00eM&alt=json returned \"Comment must be non-empty.\". Details: \"[{'@type': 'type.googleapis.com/google.commentanalyzer.v1alpha1.Error', 'errorType': 'COMMENT_EMPTY'}]\">. Retrying (1/3)...\n",
      "Error analyzing text: <HttpError 400 when requesting https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze?key=AIzaSyA_uZndSn69dCshlHBt01IZRmmL6GV00eM&alt=json returned \"Comment must be non-empty.\". Details: \"[{'@type': 'type.googleapis.com/google.commentanalyzer.v1alpha1.Error', 'errorType': 'COMMENT_EMPTY'}]\">. Retrying (2/3)...\n",
      "Error analyzing text: <HttpError 400 when requesting https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze?key=AIzaSyA_uZndSn69dCshlHBt01IZRmmL6GV00eM&alt=json returned \"Comment must be non-empty.\". Details: \"[{'@type': 'type.googleapis.com/google.commentanalyzer.v1alpha1.Error', 'errorType': 'COMMENT_EMPTY'}]\">. Retrying (3/3)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:   0%|                   | 31/152982 [00:14<63:28:28,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping text after 3 retries: ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:   0%|                    | 46/152982 [00:16<7:46:11,  5.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error analyzing text: <HttpError 400 when requesting https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze?key=AIzaSyA_uZndSn69dCshlHBt01IZRmmL6GV00eM&alt=json returned \"Comment must be non-empty.\". Details: \"[{'@type': 'type.googleapis.com/google.commentanalyzer.v1alpha1.Error', 'errorType': 'COMMENT_EMPTY'}]\">. Retrying (1/3)...\n",
      "Error analyzing text: <HttpError 400 when requesting https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze?key=AIzaSyA_uZndSn69dCshlHBt01IZRmmL6GV00eM&alt=json returned \"Comment must be non-empty.\". Details: \"[{'@type': 'type.googleapis.com/google.commentanalyzer.v1alpha1.Error', 'errorType': 'COMMENT_EMPTY'}]\">. Retrying (2/3)...\n",
      "Error analyzing text: <HttpError 400 when requesting https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze?key=AIzaSyA_uZndSn69dCshlHBt01IZRmmL6GV00eM&alt=json returned \"Comment must be non-empty.\". Details: \"[{'@type': 'type.googleapis.com/google.commentanalyzer.v1alpha1.Error', 'errorType': 'COMMENT_EMPTY'}]\">. Retrying (3/3)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:   0%|                   | 48/152982 [00:20<33:20:20,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping text after 3 retries: ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:   0%|                   | 49/152982 [00:20<25:15:09,  1.68it/s]"
     ]
    }
   ],
   "source": [
    "# After resampling the data\n",
    "male_com_toxicity = process_dataframe(male_com_df_1)\n",
    "\n",
    "male_com_df.to_csv('/home/haters/Downloads/Toxicity_Detection/output_perspective/output_toxic_maleCom_1.csv', index=False)\n",
    "male_com_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6c61cb-95de-418b-b037-3db38de9f747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b4270d-dff7-4399-93e0-85aa701781eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
