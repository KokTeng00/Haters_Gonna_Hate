{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c0bba52-3f91-4ea5-90e6-162b2da9d5ab",
   "metadata": {},
   "source": [
    "# Coherence score"
   ]
  },
  {
   "cell_type": "code",
   "id": "ce638568-fd2c-4d25-b821-6550c55bfc06",
   "metadata": {},
   "source": [
    "# import needed libraries and modules for text preprocessing and topic modeling by using LDA algorithm (Latent Dirichlet Allocation)\n",
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "import emoji\n",
    "import random\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import CoherenceModel\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# download necessary nltk resources for text preprocessing\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# set random seed for reproducibility of results across different runs of the code\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Text Preprocessing",
   "id": "9cadf788ccc820ed"
  },
  {
   "cell_type": "code",
   "id": "fb583b73-9023-4df2-87a2-a9a894d7016a",
   "metadata": {},
   "source": [
    "# define Text Preprocessor class\n",
    "class TextPreprocessor:\n",
    "    # initialize the TextPreprocessor class with necessary attributes and methods for text preprocessing tasks\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.tokenizer = WordPunctTokenizer()\n",
    "        # define a list of singer names to remove from text data for better topic modeling results (case-insensitive)\n",
    "        self.singer_names = [\n",
    "            \"Bad Bunny\", \"El Conejo Malo\", \"The Weeknd\", \"Abel\", \"Abel Tesfaye\",\n",
    "            \"Morgan Wallen\", \"Ed Sheeran\", \"Ginger Jesus\", \"Ed\", \"Drake\", \n",
    "            \"Drizzy\", \"Champagne Papi\", \"Aubrey\", \"Harry Styles\", \"Feid\", \n",
    "            \"Imagine Dragons\", \"Dan Reynolds\", \"Ben McKee\", \"Daniel Wayne Sermon\", \n",
    "            \"Daniel Platz Platzman\", \"Post Malone\", \"Posty\", \"BTS\", \"Bangtan\", \n",
    "            \"Bangtan Sonyeondan\", \"Tannies\", \"RM\", \"Jin\", \"Suga\", \"J-Hope\", \n",
    "            \"Jimin\", \"V\", \"Jungkook\", \"harry\", \"justin bieber\", \"bieber\", \"justin\", \"namjoon\", \n",
    "            \"Taylor Swift\" , \"T-Swift\" , \"TayTay\" , \"Taylor\" , \"Miss Americana\", \"SZA\", \"Solana Imani Row\", \n",
    "            \"Miley Cyrus\", \"Miley\", \"Hannah Montana\", \"New Jeans\", \"Minji\", \"Hanni\", \"Danielle New Jeans\", \"Haerin\", \"Hyein\", \n",
    "            \"Dua Lipa\", \"Dua\", \"Dula Peep\", \"Olivia Rodrigo\", \"Liv\", \"Ariana Grande\", \"Ari\", \"Ariana\", \"Ms Grande\", \n",
    "            \"Billie Eilish\", \"Billie\", \"Rihanna\", \"RiRi\", \"Badgalriri\", \"Adele\", \"swiftie\", \"Oliia\", \"newjean\", \n",
    "            \"rodrigo\", \"rodrigos\"\n",
    "        ]\n",
    "        # convert singer names to lowercase for case-insensitive matching during text preprocessing tasks\n",
    "        self.singer_names = [name.lower() for name in self.singer_names]\n",
    "\n",
    "    # preprocess text data by removing singer names, emojis, special characters, numbers, and lemmatizing tokens for topic modeling tasks\n",
    "    def preprocess_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = self.remove_singer_names(text)\n",
    "        text = emoji.demojize(text)\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        tokens = [word for word in tokens if word not in self.stop_words and word.isalnum()]\n",
    "        tokens = [self.lemmatize_token(word) for word in tokens]\n",
    "        # return preprocessed text data as a single string with tokens separated by whitespace\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    # remove singer names from text data for better topic modeling results and to avoid bias in topic assignments\n",
    "    def remove_singer_names(self, text):\n",
    "        for name in self.singer_names:\n",
    "            text = text.replace(name, '')\n",
    "        # replace common abbreviations and acronyms with full words for better topic modeling results\n",
    "        text = text.replace('austin', 'album')\n",
    "        text = text.replace('wts', 'want to sell')\n",
    "        text = text.replace('merch', 'merchandise')\n",
    "        # return text data with singer names removed for further preprocessing\n",
    "        return text\n",
    "\n",
    "    # lemmatize tokens in text data to reduce inflectional forms and sometimes derivationally related forms of words to a common base form\n",
    "    def lemmatize_token(self, token):\n",
    "        # get the part of speech tag for each token to lemmatize based on the WordNet lexical database and NLTK library functions\n",
    "        tag = self.get_wordnet_pos(nltk.pos_tag([token])[0][1])\n",
    "        # return lemmatized token based on the part of speech tag if available, otherwise return the original token\n",
    "        return self.lemmatizer.lemmatize(token, pos=tag) if tag else token\n",
    "\n",
    "    # get the WordNet part of speech tag for each token to lemmatize based on the Penn Treebank tag set and WordNet lexical database in NLTK\n",
    "    @staticmethod\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        # converts treebank tag to wordnet tag\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return None"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# define a function to remove hyperlinks from text data for better topic modeling results\n",
    "def remove_hyperlinks(text):\n",
    "    return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags = re.MULTILINE)"
   ],
   "id": "f81f84578c7da84"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# define a function to preprocess text data for topic modeling tasks using the TextPreprocessor class and methods\n",
    "def preprocess(df, data_type):\n",
    "    # initialize the TextPreprocessor class for text preprocessing tasks in the dataset based on the data type (comments or submissions)\n",
    "    if data_type == 'comments':\n",
    "        text = 'body'\n",
    "    elif data_type == 'submissions':\n",
    "        text = 'combined_text'\n",
    "    col_index = df.columns.get_loc(text) + 1\n",
    "    # preprocess text data in the dataset using the TextPreprocessor class and methods\n",
    "    preprocessed_text = df[text].apply(preprocessor.preprocess_text)\n",
    "    df.insert(col_index, 'preprocessed_txt', preprocessed_text)\n",
    "    return df"
   ],
   "id": "9673f186cfa4c1fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# import female and male submission dataset files for text analysis and topic modeling tasks\n",
    "male_submissions = pd.read_csv(\"/home/haters/Downloads/Toxicity_Detection/output_perspective/output_score/male_submissions_outcome_final.csv\")\n",
    "female_submissions = pd.read_csv(\"/home/haters/Downloads/Toxicity_Detection/output_perspective/output_score/female_submissions_outcome_final.csv\")\n",
    "\n",
    "# combine title and selftext columns into a single column for text analysis and topic modeling tasks in the dataset\n",
    "male_submissions['combined_text'] = male_submissions['title'] + \" \" + male_submissions['selftext']\n",
    "female_submissions['combined_text'] = female_submissions['title'] + \" \" + female_submissions['selftext']\n",
    "\n",
    "# remove hyperlinks from text data in the combined_text column for better topic modeling results\n",
    "male_submissions[\"combined_text\"] = male_submissions[\"combined_text\"].apply(remove_hyperlinks)\n",
    "female_submissions[\"combined_text\"] = female_submissions[\"combined_text\"].apply(remove_hyperlinks)"
   ],
   "id": "c424e83528d2492a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# initialize the TextPreprocessor class for text preprocessing tasks in the dataset\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# preprocess the male and female submission datasets by using the preprocess function with the TextPreprocessor class and methods\n",
    "male_submissions = preprocess(male_submissions, 'submissions')\n",
    "female_submissions = preprocess(female_submissions, 'submissions')"
   ],
   "id": "bbbf21f147088238"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Define LDA Topic Modeling Function and Compute Coherence Scores for Different Numbers of Topics in the Dataset (Visualization and Interpretation too)",
   "id": "f55cb1e0d2b1d52d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def lda_topic_modeling(texts, num_topics, stop_words='english', max_features=1000):\n",
    "    # fit LDA model to the text data using the CountVectorizer for text preprocessing and feature extraction with the specified parameters for topic modeling\n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words=stop_words, max_features=max_features)\n",
    "    # transform the text data into a document-term matrix for topic modeling using the CountVectorizer for text preprocessing and feature extraction\n",
    "    data_vectorized = vectorizer.fit_transform(texts)\n",
    "    # fit LDA model to the text data using the document-term matrix for topic modeling with the specified parameters for topic modeling and text analysis\n",
    "    lda_model = LatentDirichletAllocation(n_components=num_topics, max_iter=10, learning_method='online', random_state=100)\n",
    "    # extract topics from the LDA model using the document-term matrix for topic modeling and text analysis\n",
    "    lda_output = lda_model.fit_transform(data_vectorized)\n",
    "    # return the LDA model, LDA output, and CountVectorizer for text preprocessing and feature extraction\n",
    "    return lda_model, lda_output, vectorizer\n",
    "\n",
    "def compute_coherence_values(texts, vectorizer, start=2, limit=15, step=1):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        # fit LDA model to the text data and extract topics using the CountVectorizer for text preprocessing and feature extraction\n",
    "        lda_model, lda_output, vectorizer = lda_topic_modeling(texts, num_topics)\n",
    "        # append lda model to list for each number of topics\n",
    "        model_list.append(lda_model)\n",
    "        # extract topics and convert to gensim format for coherence score computation\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        topics = [[feature_names[i] for i in topic.argsort()[:-11:-1]] for topic in lda_model.components_]\n",
    "        # compute coherence score \n",
    "        dictionary = gensim.corpora.Dictionary([text.split() for text in texts])\n",
    "        # create a gensim corpus for the text data and dictionary for the topics in the LDA model for coherence score computation using the c_v metric (UMass)\n",
    "        coherence_model_lda = CoherenceModel(topics=topics, texts=[text.split() for text in texts], dictionary=dictionary, coherence='c_v')\n",
    "        # append coherence score to list for each number of topics \n",
    "        coherence_values.append(coherence_model_lda.get_coherence())\n",
    "    return model_list, coherence_values"
   ],
   "id": "419806d85d3877cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# fill missing values in preprocessed_txt column with empty strings and convert to string data type for text analysis and topic modeling tasks\n",
    "male_submissions['preprocessed_txt'] = male_submissions['preprocessed_txt'].fillna('').astype(str)\n",
    "female_submissions['preprocessed_txt'] = female_submissions['preprocessed_txt'].fillna('').astype(str)"
   ],
   "id": "32f6eab480b093cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# compute coherence scores for male/female submissions\n",
    "model_list_male, coherence_values_male = compute_coherence_values(male_submissions['preprocessed_txt'].tolist(), CountVectorizer(stop_words='english'))\n",
    "model_list_female, coherence_values_female = compute_coherence_values(female_submissions['preprocessed_txt'].tolist(), CountVectorizer(stop_words='english'))"
   ],
   "id": "94ec6e700a43f801"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# plot coherence scores for different numbers of topics to find the optimal number of topics for topic modeling tasks in the dataset \n",
    "x = range(2, 15, 1)\n",
    "plt.figure(figsize=(10, 5))\n",
    "# plot coherence scores for male and female submissions to compare the optimal number of topics for topic modeling tasks\n",
    "plt.plot(x, coherence_values_male, label=\"Male Submissions\", marker='o')\n",
    "plt.plot(x, coherence_values_female, label=\"Female Submissions\", marker='o')\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Coherence Score\")\n",
    "plt.legend()\n",
    "plt.title(\"Coherence Scores for Different Numbers of Topics\")\n",
    "plt.show()"
   ],
   "id": "6f92438fd071362d"
  },
  {
   "cell_type": "code",
   "id": "95e1dfaa-4506-4d82-b74b-f33bb42638c4",
   "metadata": {},
   "source": [
    "# find the optimal number of topics\n",
    "optimal_num_topics_male = x[coherence_values_male.index(max(coherence_values_male))]\n",
    "optimal_num_topics_female = x[coherence_values_female.index(max(coherence_values_female))]\n",
    "\n",
    "# print the optimal number of topics\n",
    "print(f\"Optimal number of topics for male submissions: {optimal_num_topics_male, max(coherence_values_male)}\")\n",
    "print(f\"Optimal number of topics for female submissions: {optimal_num_topics_female, max(coherence_values_female) }\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ecbb1569-0a20-4ae8-b20d-22c6ccf4442a",
   "metadata": {},
   "source": [
    "# define a function to plot the top words for each topic in the LDA model for topic modeling tasks in the dataset\n",
    "def plot_top_words(lda_model, feature_names, n_top_words, title):\n",
    "    # plot the top words for each topic in the LDA model for topic modeling tasks in the dataset using matplotlib and seaborn libraries for data visualization\n",
    "    num_topics = lda_model.n_components\n",
    "    fig, axes = plt.subplots(1, num_topics, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    # iterate over each topic in the LDA model and plot the top words for each topic using a horizontal bar plot with the specified number of top words to display\n",
    "    for topic_idx, topic in enumerate(lda_model.components_):\n",
    "        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f'Topic {topic_idx + 1}', fontdict={'fontsize': 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "        for i in 'top right left'.split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "    fig.suptitle(title, fontsize=40)\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "41a895e8-7d5b-4551-8a80-9e7bccfe2e6a",
   "metadata": {},
   "source": [
    "# fit LDA model\n",
    "lda_model_male, lda_output_male, vectorizer_male = lda_topic_modeling(male_submissions['preprocessed_txt'].tolist(), optimal_num_topics_male)\n",
    "lda_model_female, lda_output_female, vectorizer_female = lda_topic_modeling(female_submissions['preprocessed_txt'].tolist(), optimal_num_topics_female)\n",
    "\n",
    "# display topics\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx + 1}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "# display topics with top 10 words\n",
    "no_top_words = 10\n",
    "print(\"\\nTopics in Male Submissions:\")\n",
    "display_topics(lda_model_male, vectorizer_male.get_feature_names_out(), no_top_words)\n",
    "\n",
    "print(\"\\nTopics in Female Submissions:\")\n",
    "display_topics(lda_model_female, vectorizer_female.get_feature_names_out(), no_top_words)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8cdaffa5-f256-4860-b493-08bc0de0cd72",
   "metadata": {},
   "source": [
    "n_top_words = 10\n",
    "plot_top_words(lda_model_male, vectorizer_male.get_feature_names_out(), n_top_words, f'Topics in Male Submissions ({optimal_num_topics_male} Topics)')\n",
    "plot_top_words(lda_model_female, vectorizer_female.get_feature_names_out(), n_top_words, f'Topics in Female Submissions ({optimal_num_topics_female} Topics)')"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
